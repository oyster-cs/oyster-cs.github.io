<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>自动文摘（十三） | PaperWeekly</title>
  
  
  <meta name="description" content="引
 天下武功，唯快不破 

今天分享的paper是Incorporating Copying Mechanism in Sequence-to-Sequence Learning，作者来自香港大学和华为诺亚方舟实验室。
本文的模型通过借鉴人类在处理难理解的文字时采用的死记硬背的方法，提出了COPY">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="自动文摘（十三）"/>
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-18T09:56:53.000Z"><a href="/2016/05/18/自动文摘（十三）/">2016-05-18</a></time>
      
      
  
    <h1 class="title">自动文摘（十三）</h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 天下武功，唯快不破 </strong></p>
</blockquote>
<p>今天分享的paper是<b>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</b>，作者来自香港大学和华为诺亚方舟实验室。</p>
<p><code>本文的模型通过借鉴人类在处理难理解的文字时采用的死记硬背的方法，提出了COPYNET。将拷贝模式融入到了Seq2Seq模型中，将传统的生成模式和拷贝模式混合起来构建了新的模型，非常好地解决了OOV问题。解决问题的思路与之前的一篇有关Pointer的文章十分类似。decoder部分不断地变复杂，考虑的因素越来越多，模型的效果也越来越好。如果结合上一篇Minimum Risk Training的训练方法，相信在评价指标上会更进一步。</code></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>Seq2Seq技术占据了nlp多个研究任务的评测榜首，包括最早提出该技术的机器翻译，句法分析，文本摘要，对话系统。Seq2Seq本质上是一个encoder-decoder的模型，encoder部分将输入的序列变换成某一种表示，然后decoder将这种表示变换成输出序列。在Seq2Seq的基础上，首次增加注意力机制来做机器翻译的自动对齐。注意力机制在很大程度上提升了Seq2Seq的性能。</p>
<p>本文研究了人类语言交流的另一个机制，“拷贝机制”（<code>copy mechanism</code>），定位到输入序列中的某个片段，然后将该片段拷贝到输出序列中。比如：</p>
<img src="/2016/05/18/自动文摘（十三）/fig1.png" width="400" height="400">
<p>但是注意力机制严重依赖于语义的表示，在系统需要获取到命名实体或者日期时难以准确地表示。相对之下，拷贝机制更加接近于人类处理语言问题的方式。本文提出了COPYNET系统，不仅具备传统Seq2Seq生成词的能力，而且可以从输入序列中拷贝合适的片段到输出序列中。在合成数据和真实数据中均取得了不错的结果。</p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>文章的这部分简单介绍了一下Seq2Seq+Attention Mechanism技术，前面的博客分享了很多这部分的内容，这里就不再赘述了。</p>
<h1 id="COPYNET"><a href="#COPYNET" class="headerlink" title="COPYNET"></a>COPYNET</h1><p>从神经学角度来讲，拷贝机制和人类的死记硬背类似，较少地理解到了意思但保留了字面的完整。从模型的角度来讲，拷贝机制相比于soft注意力模型更加死板，所以更难整合到神经网络模型中。</p>
<h2 id="模型综述"><a href="#模型综述" class="headerlink" title="模型综述"></a>模型综述</h2><p>COPYNET依然是一个encoder-decoder模型，如图1所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig2.png" width="600" height="600">
<p>encoder采用了一个双向RNN模型，输出一个隐藏层表示的矩阵M作为decoder的输入。decoder部分与传统的Seq2Seq不同之处在于以下三部分：</p>
<ul>
<li><b>预测</b>：在生成词时存在两种模式，一种是生成模式，一种是拷贝模式，生成模型是一个结合两种模式的概率模型。</li>
<li><b>状态更新</b>：用t-1时刻的预测出的词来更新t时刻的状态，COPYNET不仅仅词向量，而且使用M矩阵中特定位置的hidden state。</li>
<li><b>读取M</b>：COPYNET也会选择性地读取M矩阵，来获取混合了内容和位置的信息。</li>
</ul>
<h2 id="拷贝模式和生成模式"><a href="#拷贝模式和生成模式" class="headerlink" title="拷贝模式和生成模式"></a>拷贝模式和生成模式</h2><p>首先，构造了两个词汇表，一个是高频词词汇表，另一个是只在输入序列中出现过一次的词，这部分的词用来支持COPYNET，用UNK表示超纲词（OOV），最终输入序列的词汇表是三者的并集。</p>
<p>给定了decoder当前状态和M矩阵，生成目标单词的概率模型如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig3.png" width="300" height="300">
<p>模型中g表示生成模式，c表示拷贝模式。两种模式的概率由下式给定：</p>
<img src="/2016/05/18/自动文摘（十三）/fig4.png" width="300" height="300">
<p>共四种可能情况，下图会更形象一些：</p>
<img src="/2016/05/18/自动文摘（十三）/fig5.png" width="300" height="300">
<p>其中生成模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig6.png" width="300" height="300">
<p>拷贝模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig7.png" width="300" height="300">
<h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><p>decoder状态更新的公式是</p>
<img src="/2016/05/18/自动文摘（十三）/fig8.png" width="300" height="300">
<p>不同的是这里的t-1时刻的y由下式表示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig9.png" width="300" height="300">
<p>后面的部分是M矩阵中与t时刻y相关的状态权重之和，如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig10.png" width="300" height="300">
<h2 id="M矩阵"><a href="#M矩阵" class="headerlink" title="M矩阵"></a>M矩阵</h2><p>M矩阵中既包含了内容（语义）信息，又包含了位置信息。COPYNET在attentive read时由内容（语义）信息和语言模型来驱动，即生成模式；在拷贝模式时，由位置信息来控制。</p>
<p>位置信息的更新方式如下图所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="300" height="300">
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>一共分为三个实验：</p>
<ul>
<li>简单规则构造的合成数据。</li>
<li>文本摘要相关的真实数据。</li>
<li>简单对话系统的数据。</li>
</ul>
<p>这里只看文本摘要实验。</p>
<h2 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h2><p>数据采用LCSTS中文短文本摘要数据集，分为两个level来测试：word-level和char-level，并且以LCSTS的baseline作为对比，结果如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="400" height="400">
<p>本文的模型远远优于baseline，而且word-level的结果比char-level更好，这与当时LCSTS paper中的结论不同，一个可能的原因是，数据集中包含了大量的命名实体名词（entity name），LCSTS paper中的方法并不能很好地处理大量的UNK单词，因此baseline中的char-level效果比word-level更好，而本文的模型的优势在于处理OOV问题，所以word-level结果更好一些。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1603.06393v2.pdf" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning Training</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p><code>PaperWeekly</code>，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/nlp/">nlp</a>, <a href="/tags/seq2seq/">seq2seq</a>, <a href="/tags/自动文摘/">自动文摘</a>, <a href="/tags/paper/">paper</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">留言</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://rsarxiv.github.io/2016/05/18/自动文摘（十三）/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>110</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>128</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/招聘/">招聘</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>