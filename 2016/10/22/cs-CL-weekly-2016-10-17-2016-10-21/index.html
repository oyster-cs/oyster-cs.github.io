<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>cs.CL weekly 2016.10.17-2016.10.21 | PaperWeekly</title>
  
  
  <meta name="description" content="一周值得读Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification【情感分析】RNN处理文本这样的序列数据有天然优势，但对于长文本效果却不尽人意。本文针对这个问题，提出了一种新的">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="cs.CL weekly 2016.10.17-2016.10.21"/>
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-10-22T05:11:20.000Z"><a href="/2016/10/22/cs-CL-weekly-2016-10-17-2016-10-21/">2016-10-22</a></time>
      
      
  
    <h1 class="title">cs.CL weekly 2016.10.17-2016.10.21</h1>
  

    </header>
    <div class="entry">
      
        <h1 id="一周值得读"><a href="#一周值得读" class="headerlink" title="一周值得读"></a>一周值得读</h1><h2 id="Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification"><a href="#Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification" class="headerlink" title="Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification"></a><a href="https://arxiv.org/pdf/1610.04989v1.pdf" target="_blank" rel="external">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</a></h2><p>【情感分析】RNN处理文本这样的序列数据有天然优势，但对于长文本效果却不尽人意。本文针对这个问题，提出了一种新的LSTM结构Cached LSTM。通过cache机制，来捕捉整体语义信息，将memory分成几组，对应不同的forget门，在文档集情感分析任务中取得了不错的结果。其实，RNN处理长文本信息都面临这个问题，chatbot中对context信息的处理也可以考虑借鉴这个思路。本文是FudanvNLP的工作。</p>
<h2 id="Lexicon-Integrated-CNN-Models-with-Attention-for-Sentiment-Analysis"><a href="#Lexicon-Integrated-CNN-Models-with-Attention-for-Sentiment-Analysis" class="headerlink" title="Lexicon Integrated CNN Models with Attention for Sentiment Analysis"></a><a href="https://arxiv.org/pdf/1610.06272v1.pdf" target="_blank" rel="external">Lexicon Integrated CNN Models with Attention for Sentiment Analysis</a></h2><p>【情感分析】本文研究的内容是情感分析，论文的亮点在于提出了一种新的CNN+attention模型。本文适合在情感分析模型上有所突破的童鞋来读，从事相关工作的工程师或数据科学家也适合粗读一下。</p>
<h2 id="A-Language-independent-and-Compositional-Model-for-Personality-Trait-Recognition-from-Short-Texts"><a href="#A-Language-independent-and-Compositional-Model-for-Personality-Trait-Recognition-from-Short-Texts" class="headerlink" title="A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts"></a><a href="https://arxiv.org/pdf/1610.04345v1.pdf" target="_blank" rel="external">A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts</a></h2><p>【用户画像】本文研究的问题是从短文本中学习用户画像，提出了一种深度学习模型，模型上从学术上讲没有太多亮点，适合工业界从事相关工作的童鞋阅读。</p>
<h2 id="Neural-Machine-Translation-Advised-by-Statistical-Machine-Translation"><a href="#Neural-Machine-Translation-Advised-by-Statistical-Machine-Translation" class="headerlink" title="Neural Machine Translation Advised by Statistical Machine Translation"></a><a href="https://arxiv.org/pdf/1610.05150v1.pdf" target="_blank" rel="external">Neural Machine Translation Advised by Statistical Machine Translation</a></h2><p>【机器翻译】NMT翻译流利但有时翻译不准，SMT翻译准确但不够流利，两者各有优劣。本文结合了两种方法的优点，提出了在NMT解码阶段，用SMT来做辅助，通过一种门机制来选择用SMT还是NMT生成。</p>
<h2 id="Interactive-Attention-for-Neural-Machine-Translation"><a href="#Interactive-Attention-for-Neural-Machine-Translation" class="headerlink" title="Interactive Attention for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1610.05011v1.pdf" target="_blank" rel="external">Interactive Attention for Neural Machine Translation</a></h2><p>【机器翻译】【注意力模型】注意力模型证明了其强大威力，本文提出了一种新的注意力模型，INTERACTIVE ATTENTION，在encoder和decoder之间通过读和写操作进行交互，实验中对比了其他注意力模型，结果不错。</p>
<h2 id="A-General-Framework-for-Content-enhanced-Network-Representation-Learning"><a href="#A-General-Framework-for-Content-enhanced-Network-Representation-Learning" class="headerlink" title="A General Framework for Content-enhanced Network Representation Learning"></a><a href="https://arxiv.org/pdf/1610.02906v3.pdf" target="_blank" rel="external">A General Framework for Content-enhanced Network Representation Learning</a></h2><p>【社交网络】本文研究的是社交网络中各个节点的表示问题，亮点在于考虑了node（比如：用户）的相关文本信息，从文本中挖掘出node的一些特性（比如：性别、职业、爱好等），对node的刻画更精准。利用富文本信息来刻画社交网络中的各个node，在广告、推荐系统等应用方面都会带来很大的价值，这也正是nlp的价值所在，从杂乱无章的非结构文本中挖掘出大量的有用信息，本文适合研究社交网络价值、推荐系统的童鞋深入阅读。本文工作来自哈工大刘挺老师组。</p>
<h2 id="Reasoning-with-Memory-Augmented-Neural-Networks-for-Language-Comprehension"><a href="#Reasoning-with-Memory-Augmented-Neural-Networks-for-Language-Comprehension" class="headerlink" title="Reasoning with Memory Augmented Neural Networks for Language Comprehension"></a><a href="https://arxiv.org/pdf/1610.06454v1.pdf" target="_blank" rel="external">Reasoning with Memory Augmented Neural Networks for Language Comprehension</a></h2><p>【机器阅读】本文提出了一种做假设检验的神经网络方法（Neural Semantic Encoders），并且应用在机器阅读任务上，取得了不错的效果，涉及的数据集是CBT和WDW。</p>
<h1 id="一周资源"><a href="#一周资源" class="headerlink" title="一周资源"></a>一周资源</h1><h2 id="STC短文本对话数据"><a href="#STC短文本对话数据" class="headerlink" title="STC短文本对话数据"></a><a href="http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm" target="_blank" rel="external">STC短文本对话数据</a></h2><p>【中文对话数据】对话很热，但依然很难！华为诺亚方舟实验室在NTCIR-13组织的关于短文本对话(Short-Text Conversation)的比赛已经开始注册了，让我们一起从大数据中探求人类对话的本质！比赛有两个任务，一个是基于检索给出response，一个是直接生成response。数据来自微博，输入是微博内容，输出是评论内容。13年华为的文章提到的短文本对话数据集可能就是指该数据集，一直愁对话数据的各位可以看过来，你们等的数据来了！</p>
<h2 id="DataHub"><a href="#DataHub" class="headerlink" title="DataHub"></a><a href="https://datahub.io/dataset" target="_blank" rel="external">DataHub</a></h2><p>一个收集各种数据集的网站。</p>
<h2 id="t-SNE可视化工具的python和torch封装"><a href="#t-SNE可视化工具的python和torch封装" class="headerlink" title="t-SNE可视化工具的python和torch封装"></a><a href="https://github.com/DmitryUlyanov/Multicore-TSNE" target="_blank" rel="external">t-SNE可视化工具的python和torch封装</a></h2><p>DmitryUlyanov/Multicore-TSNE: Parallel t-SNE implementation with Python and Torch wrappers</p>
<h2 id="Jiwei-Li关于聊天机器人NLG问题的slide"><a href="#Jiwei-Li关于聊天机器人NLG问题的slide" class="headerlink" title="Jiwei Li关于聊天机器人NLG问题的slide"></a><a href="http://web.stanford.edu/class/cs224u/materials/cs224u-2016-li-chatbots.pdf" target="_blank" rel="external">Jiwei Li关于聊天机器人NLG问题的slide</a></h2><p>分享一个斯坦福大学Jiwei Li关于聊天机器人NLG问题的slide，Jiwei Li是一个非常高产的作者，这个slide包括了非常多精彩的内容。 </p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/nlp/">nlp</a>, <a href="/tags/PaperWeekly/">PaperWeekly</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">留言</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://rsarxiv.github.io/2016/10/22/cs-CL-weekly-2016-10-17-2016-10-21/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>110</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>128</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/招聘/">招聘</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>