<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>cs.CL weekly 2016.09.12-2016.09.16 | PaperWeekly</title>
  
  
  <meta name="description" content="本周（2016.09.12-2016.09.16）质量较高的arXiv cs.CL的paper如下：（点击标题可看原文）
Dialogue manager domain adaptation using Gaussian process reinforcement learning本文是Steve ">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="cs.CL weekly 2016.09.12-2016.09.16"/>
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-09-16T13:34:58.000Z"><a href="/2016/09/16/cs-CL-weekly-2016-09-12-2016-09-16/">2016-09-16</a></time>
      
      
  
    <h1 class="title">cs.CL weekly 2016.09.12-2016.09.16</h1>
  

    </header>
    <div class="entry">
      
        <p>本周（2016.09.12-2016.09.16）质量较高的arXiv cs.CL的paper如下：<br>（点击标题可看原文）</p>
<h1 id="Dialogue-manager-domain-adaptation-using-Gaussian-process-reinforcement-learning"><a href="#Dialogue-manager-domain-adaptation-using-Gaussian-process-reinforcement-learning" class="headerlink" title="Dialogue manager domain adaptation using Gaussian process reinforcement learning"></a><a href="http://120.52.73.75/arxiv.org/pdf/1609.02846v1.pdf" target="_blank" rel="external">Dialogue manager domain adaptation using Gaussian process reinforcement learning</a></h1><p>本文是Steve Young组的一篇大作，文中详细介绍了Gaussian process reinforcement learning框架的思路和优势，并且在多个对话领域中进行了实验并得到更好的结果。</p>
<h1 id="A-Hierarchical-Model-of-Reviews-for-Aspect-based-Sentiment-Analysis"><a href="#A-Hierarchical-Model-of-Reviews-for-Aspect-based-Sentiment-Analysis" class="headerlink" title="A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis"></a><a href="http://120.52.73.79/arxiv.org/pdf/1609.02745v1.pdf" target="_blank" rel="external">A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis</a></h1><p>本文提出用分层双向LSTM模型对网站评论数据进行观点挖掘，发表在EMNLP 2016。该作者今天在arxiv上提交了三篇同类问题不同解决方案的paper，对评论观点和情感挖掘的童鞋可作参考。</p>
<h1 id="Knowledge-as-a-Teacher-Knowledge-Guided-Structural-Attention-Networks"><a href="#Knowledge-as-a-Teacher-Knowledge-Guided-Structural-Attention-Networks" class="headerlink" title="Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks"></a><a href="http://120.52.73.79/arxiv.org/pdf/1609.03286v1.pdf" target="_blank" rel="external">Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks</a></h1><p>本文提出了用先验知识+attention network的模型，用来解决了自然语言理解存在问题：通过从少量训练数据中捕获重要子结构，来缓解测试集中的unseen data问题，同时提高理解能力。</p>
<h1 id="Wav2Letter-an-End-to-End-ConvNet-based-Speech-Recognition-System"><a href="#Wav2Letter-an-End-to-End-ConvNet-based-Speech-Recognition-System" class="headerlink" title="Wav2Letter: an End-to-End ConvNet-based Speech Recognition System"></a><a href="http://120.52.73.79/arxiv.org/pdf/1609.03193v2.pdf" target="_blank" rel="external">Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</a></h1><p>本文提出了一种语音识别的端到端模型，基于CNN和graph decoding，在不依赖因素对齐的前提下，输出letters。本文工作来自Facebook AI。</p>
<h1 id="Multimodal-Attention-for-Neural-Machine-Translation"><a href="#Multimodal-Attention-for-Neural-Machine-Translation" class="headerlink" title="Multimodal Attention for Neural Machine Translation "></a><a href="http://120.52.73.78/arxiv.org/pdf/1609.03976v1.pdf" target="_blank" rel="external">Multimodal Attention for Neural Machine Translation </a></h1><p>本文通过利用image caption的多模态、多语言数据构建了一个NMT模型，模型的输入不仅是source language，还有所描述的图像，输出是target language。通过输入更多的信息，得到了更好的效果。</p>
<h1 id="Joint-Extraction-of-Events-and-Entities-within-a-Document-Context"><a href="#Joint-Extraction-of-Events-and-Entities-within-a-Document-Context" class="headerlink" title="Joint Extraction of Events and Entities within a Document Context"></a><a href="http://120.52.73.78/arxiv.org/pdf/1609.03632v1.pdf" target="_blank" rel="external">Joint Extraction of Events and Entities within a Document Context</a></h1><p>本文针对传统信息抽取方法将event和entity分开考虑的问题，提出了在docuemnt-level context下考虑event和entity之间关系进行信息抽取的新方法，取得了非常好的结果。本文发表在NAACL2016.</p>
<h1 id="Character-Level-Language-Modeling-with-Hierarchical-Recurrent-Neural-Networks"><a href="#Character-Level-Language-Modeling-with-Hierarchical-Recurrent-Neural-Networks" class="headerlink" title="Character-Level Language Modeling with Hierarchical Recurrent Neural Networks"></a><a href="http://120.52.73.75/arxiv.org/pdf/1609.03777v1.pdf" target="_blank" rel="external">Character-Level Language Modeling with Hierarchical Recurrent Neural Networks</a></h1><p>语言模型问题上，char-level可以很好地解决OOV的问题，但效果不如word-level，本文针对该问题提出了一种分层模型，同时兼顾word-level和char-level的优势。本文发表在nips2016。</p>
<h1 id="Neural-Machine-Translation-with-Supervised-Attention"><a href="#Neural-Machine-Translation-with-Supervised-Attention" class="headerlink" title="Neural Machine Translation with Supervised Attention"></a><a href="http://120.52.73.78/arxiv.org/pdf/1609.04186v1.pdf" target="_blank" rel="external">Neural Machine Translation with Supervised Attention</a></h1><p>attention机制可以动态地对齐source和target words，但准确率不如传统方法。本文提出了用传统方法作为teacher，来“教”model学习alignment，模型称为supervised attention。本文已投稿COLING2016，在审。</p>
<h1 id="Efficient-softmax-approximation-for-GPUs"><a href="#Efficient-softmax-approximation-for-GPUs" class="headerlink" title="Efficient softmax approximation for GPUs"></a><a href="http://120.52.73.76/arxiv.org/pdf/1609.04309v1.pdf" target="_blank" rel="external">Efficient softmax approximation for GPUs</a></h1><p>本文提出了一种高效的softmax近似方法，并且可以方便地进行并行计算。本文称之为adaptive softmax，根据词分布进行聚类，极大地提高了计算效率并保证了不错的准确率。本文工作来自Facebook AI Research。</p>
<p>在自然语言生成任务中常常面临word vocabulary size太大的困境，softmax的效率非常低，本文给出了一种快速计算的方法。Tomas Mikolov之前也提到过类似的思路。</p>
<h1 id="Characterizing-the-Language-of-Online-Communities-and-its-Relation-to-Community-Reception"><a href="#Characterizing-the-Language-of-Online-Communities-and-its-Relation-to-Community-Reception" class="headerlink" title="Characterizing the Language of Online Communities and its Relation to Community Reception"></a><a href="http://120.52.73.78/arxiv.org/pdf/1609.04779v1.pdf" target="_blank" rel="external">Characterizing the Language of Online Communities and its Relation to Community Reception</a></h1><p>本文研究了在线社区语言的style和topic哪个更具代表性，这里style用复合语言模型来表示，topic用LDA来表示，通过Reddit Forum实验得到style比topic更有代表性。</p>
<h1 id="Factored-Neural-Machine-Translation"><a href="#Factored-Neural-Machine-Translation" class="headerlink" title="Factored Neural Machine Translation"></a><a href="http://120.52.73.79/arxiv.org/pdf/1609.04621v1.pdf" target="_blank" rel="external">Factored Neural Machine Translation</a></h1><p>针对机器翻译领域中两个常见的问题：1、目标语言词汇表过大；2、OOV问题；利用了单词的词形和语法分解，提出了一种新的NMT模型，并取得了满意的效果。</p>
<h1 id="Context-Aware-Nonnegative-Matrix-Factorization-Clustering"><a href="#Context-Aware-Nonnegative-Matrix-Factorization-Clustering" class="headerlink" title="Context Aware Nonnegative Matrix Factorization Clustering"></a><a href="http://120.52.73.78/arxiv.org/pdf/1609.04628v1.pdf" target="_blank" rel="external">Context Aware Nonnegative Matrix Factorization Clustering</a></h1><p>大多数paper都在研究NMF在聚类中的初始化和优化部分，而本文关注的点在于最后的聚类分配上。本文被 ICPR 2016全文收录。</p>
<p>以下内容为arXiv外的优质内容：</p>
<h1 id="SIGDIAL-2016-Accepted-Paper"><a href="#SIGDIAL-2016-Accepted-Paper" class="headerlink" title="SIGDIAL 2016 Accepted Paper"></a><a href="http://www.sigdial.org/workshops/conference17/proceedings/SIGDIAL-2016.pdf" target="_blank" rel="external">SIGDIAL 2016 Accepted Paper</a></h1><p>SIGdial是ACL下面的一个关于对话系统地特别兴趣小组，每年开一次会。今年的会议最近正在开，会议录用的所有paper都已经放出。</p>
<h1 id="CMU-SPEECH-Team-Homepage"><a href="#CMU-SPEECH-Team-Homepage" class="headerlink" title="CMU SPEECH Team Homepage"></a><a href="http://speech.sv.cmu.edu/software.html" target="_blank" rel="external">CMU SPEECH Team Homepage</a></h1><p>CMU SPEECH Team的主页，包括他们的开源软件Yoda和publication及其开源实现。</p>
<h1 id="Machine-Learning-WAYR-What-Are-You-Reading"><a href="#Machine-Learning-WAYR-What-Are-You-Reading" class="headerlink" title="Machine Learning - WAYR (What Are You Reading)"></a><a href="https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/?st=ISZ6YT6D&amp;sh=02bd0722" target="_blank" rel="external">Machine Learning - WAYR (What Are You Reading)</a></h1><p>reddit上的这个帖子很有意思，和paperweekly想做的一个事情非常像，就是可以让读类似或者同一篇paper的童鞋得到充分交流。</p>
<h1 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h1><p>PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。</p>
<p>微信公众号：PaperWeekly<br><img src="media/qrcode_for_gh_5138cebd4585_430%20-2-.jpg" alt="qrcode_for_gh_5138cebd4585_430 -2-"><br>微博账号：PaperWeekly（<a href="http://weibo.com/u/2678093863" target="_blank" rel="external">http://weibo.com/u/2678093863</a> ）每天都会分享当天arXiv cs.CL板块刷新的高质量paper<br>知乎专栏：PaperWeekly（<a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="external">https://zhuanlan.zhihu.com/paperweekly</a> ）<br>微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/nlp/">nlp</a>, <a href="/tags/PaperWeekly/">PaperWeekly</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">留言</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://rsarxiv.github.io/2016/09/16/cs-CL-weekly-2016-09-12-2016-09-16/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>110</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>128</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/招聘/">招聘</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>