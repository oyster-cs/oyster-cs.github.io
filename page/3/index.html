<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>第 3 页 | PaperWeekly</title>
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="PaperWeekly"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="PaperWeekly" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77933764-1', 'auto');
	ga('send', 'pageview');

</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">PaperWeekly</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/atom.xml">Rss</a></li>
    
      <li><a href="/about/index.html">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-12T02:59:52.000Z"><a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">2016-06-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/12/当我们在谈论deep-learning的时候，我们在谈论什么？/">当我们在谈论deep learning的时候，我们在谈论什么？</a></h1>
  

    </header>
    <div class="entry">
      
        <p>标题起的有一点装x了，昨天看到微博上刘知远老师对关于deep learning哪年火的问题的讨论，突然有一些自己的感触就写了下来。</p>
<p>从接触nlp到现在大概过去了4个月的时间，最初的动机是要用word2vec工具包来给自己写的app(rsarxiv)添加一个paper knowledge graph的功能。当时用word2vec的感受时，参数太多了，不知道这些参数到底是什么意思，所以就想着看看paper，看看源代码来试着理解下每个参数到底起什么作用，以方便我使用它。就是从这个时候开始算是接触了nlp。</p>
<p>因为觉得自动文摘是一个非常炫酷的功能，同时也是想给自己的app添加一个根据查询结果自动生成文献综述的功能，所以开始看一些自动文摘方面的paper，正好微博上找到了一个paper list，里面列出了近两年abstractive summarization相关的paper，加上自己买的一些书中介绍了很多传统的extractive的方法，经过了一个月时间的学习，自己从这些资料中学到了很多关于自动文摘的东西，为了记录下所学到的东西和理解到的东西，就写了一个系列博客——<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>。在看abstractive思路的时候，接触到了当前研究的一个热门方法，seq2seq+attention，这个组合几乎席卷了nlp的所有任务，一次又一次地刷新着排行榜。seq2seq的思路其实并不复杂，没有太多晦涩难懂的数学公式，看着模型图和下面的解释基本就能看懂思路是怎样的，attention也没有多么难，只是在seq2seq的接触上将输出与输入之间的关系考虑更加全面了，而不是简单地认为输出的每一个部分都与整个输入有关，而应该是将注意力放在相关的输入上。</p>
<p>因为看自动文摘的paper比较过瘾，就想着可以不可以做一个公众号来督促自己每天读一篇paper，写一篇博客。这样既可以养成一个读paper的好习惯，又可以提高自己的写作水平，同时也能够分享给可能感兴趣的童鞋。于是乎开始了PaperWeekly这个side project，前一天晚上睡觉前读paper，理解了其中模型的思路，第二天早上早起，开始写作，希望用尽量短的话介绍清楚该篇paper的模型思路和贡献。关于公众号如何做的问题，和另外一个账号的作者讨论过。他认为发布频率不应该太高，一天一篇太高了，而且要写长文，写干活，写高质量的东西，这样才能提供给用户最好的服务。我觉得他的话没有一点瑕疵，但我做PaperWeekly的初衷还是以自己为主，希望自己每天可以读一篇，写一篇，用最概括的话、图来讲清楚paper的贡献，当然不排除有的用户不喜欢这种方式，那么我只能说您不适合我这个公众号，我也不会因此去改变。于是，这个side project坚持了一个多月，现在有文章22篇+自动文章8篇一共30篇。当然读过的paper不止这些，30篇是写下来的。<code>今后的形式可能是这样，工作日时间较短，所以写单篇，周末的话，时间充裕，写综述，所以每周末都会将下周的5篇文章选好，尽量是相似topic的，这样方便写综述。</code>我觉得这是一个好习惯，长此以往坚持下来，一定会有很大的收获，这一点我坚信。</p>
<p>看了很多paper，也明白了很多的model，剩下的部分应该就是动手实践了。在选择框架的路上走过一些弯路，最开始用纯python写过一些简单的nnlm这样的模型，后来觉得用框架效率更高一些，于是尝试了keras，一个基于theano和tensorflow的框架，使用方法和torch差不多。如果只是解决一些常用的model结构的话，用keras非常地easy，代码量非常小，非常容易上手，比如rnn，cnn等等。但如果你想实现一个稍微复杂的model，对灵活性要求比较高的model，keras就有点捉襟见肘了，毕竟是一个框架上的框架，灵活性肯定好不了。后来就决定试一下theano，毕竟是deep learning发源地之一出的框架，github上开源了很多的程序都是用theano写的，而且自己也比较擅长python，于是就开始了theano的学习之路，其中最吸引我的是自动求导的功能，但最终导致我放弃theano的一个重要的原因是每次报错都让我特别头疼，因为根本没法找到错误的地方。挣扎了几天，通过重新调研，我选择了Torch，Torch是用lua封装的，意味着我得先学习lua，然后就开始torch，习惯了python的简单，用lua时感觉特别恶心，说不出原因的恶心，后来强忍着开始看一些demo，当初给自己的一个目标是用torch写出seq2seq+attention，然后做一些好玩的事情，于是找了HarvardNLP开源的代码<a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">seq2seq-attn</a>，这个组非常年轻，但非常nice，paper的code都会开源供大家学习。第一次看这个代码就觉得，写的好他么凌乱啊，根本没法读，现在来读的话也觉得很恶心，感觉写代码的人没有太多的规范，想到哪里写到哪里，于是就放弃了这个demo。后来跟着oxford的<a href="https://github.com/oxford-cs-ml-2015/practical6" target="_blank" rel="external">课程代码</a>开始一步一步地学习torch，这个project是实现一个char-level语言模型，里面的功能虽然不够完善，但从最基础的东西开始写起，并没有用类似于rnn，dp这样的框架来做，非常适合入门学习和打基础。跟着这个project，也看着<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">char-rnn</a>，其实char-rnn也是跟着oxford的这个程序学着写的，很多的套路和源码都一样。经过了一段时间的挣扎，从0开始写起，实现了一个seq2seq+attention的torch源码，我个人认为比seq2seq-attn那个代码更加清晰和简洁，过段时间整理下会放在Github上。torch有很多的缺点，比如需要学习lua，处理文本不方便，demo不给力。所以，在文本处理这一块，我用了python+hdf5的方式来解决，因为用python处理文本太方便了，然后将处理好的结果放入hdf5中，让torch去调用。demo这块是一个很大的关，只要自己可以动手写出一个完整的project之后，后面的事情就好办了。最近一段时间，tensorflow在网上非常火，有很多的文章和微博大号都在热推他，我觉得框架各有各好，坚持用好一个就会很了不起了，不必盲目跟风。</p>
<p>以上的部分是我从接触nlp开始，到现在的一个学习过程，下面的部分谈谈我的一些感触。</p>
<p>deep learning从哪年火起来，并不重要，重要的是它还能火多久？会不会遭遇另一个寒冬？媒体的热捧是跟风、炒作还是客观？现在如果你不聊两句deep learning都不好意思出门。我认为对一个知识的理解大概有三个level：是什么？怎么样？为什么？</p>
<p>第一个level，是什么的问题。RNN、CNN、RCNN、CRNN、FNN、DNN各种各样的NN充斥在各大媒体上，每天都有大量的文章来介绍各种NN做了什么牛逼的事情，哪些牛逼的机构提出了一种新的NN，将会给人类带来前所未有的方便等等等等。大家通过看一些博客，看一些新闻媒体都会了解到这些NN的概念和作用，以及类似于端到端、注意力模型的这样的概念。</p>
<p>第二个level，是怎么样的问题。知道概念并不难，做出来才是真好汉！如何提出一个自己的模型，然后用熟悉的框架编程实现它，跑分排名发paper。这个level应该是比较难的level，可能也是一些学生处于的level。之前记得王威廉老师发过一个讨论帖：</p>
<blockquote>
<p><strong>跟大家探讨几个开放式问题：大家认为到底计算机科学(Science)与工程(Engineering)的边界在哪里？以深度学习来说，学术界和工业界的着重点应该有什么不同与相同之处？如果本科毕业生也能玩转Theano/TensorFlow/Torch等平台的话，那么深度学习的博士优势到底何在？</strong></p>
</blockquote>
<p>如果大家可以玩转torch之类的框架，并且实现自己提出的模型，然后跑分刷榜发paper，那么做深度学习研究的博士优势又会在哪里呢？</p>
<p>另外一种流传于网络的说法是，deep learning就是比谁更会调参数，这里不得不祭出这张图：</p>
<img src="http://ww2.sinaimg.cn/mw690/ba115fdfjw1f4nwmxwkn4j20ak05x74m.jpg" width="400" height="400">
<p>非常地讽刺，我想应该不是这么简单。</p>
<p>还有一点是model这部分，可能是文章看的不够深，看到的paper都是从model这个层次来创新的，基本搞清楚整个数据流，输入和输出然后就会比较清楚了，而文章中一般也会用一张图把model的数据流讲解地非常清楚。在基本的model上添加gate，用hierarchical来做，套用seq2seq加attention，用copy mechanism等等方式来提出新model，获得更好的结果，亦或者是将人工feature添加到模型中。model是一个非常灵活的东西，记得本科时参加数学建模竞赛就是在做这样的一件事情，根据问题来提出自己模型，往往都是从已有模型中进行一些改进。</p>
<p>第三个level，是为什么的问题。我个人认为Phd应该能够回答出自己所研究领域的各种各样的为什么，回答出不同level的人提出的为什么，而不仅仅是会用一个框架，提出几个model，然后刷几篇顶会就博士毕业了。更重要的是对问题的思考和理解，尤其是深度地理解。我觉得一个Phd应该具备的能力是提出一个问题，分析一个问题，解决一个问题的能力，而不仅仅是简单重复别人的东西，或者是简单改进下别人的model。调参数是一个基本工作，属于工程型的范畴，为了达到一个更好的效果，需要具备这个基本能力，但并不等同于说deep learning就是调参数，这种说法太过荒唐可笑。框架也只是一个工具，至于你用torch，用tensorflow或者是自己手写都只是一种工具而已，也是一个基本能力，但并不是说deep learning就是用框架。这种说法同样很可笑。一个升级版的model也可以发paper，但是否你的model真的改变了研究现状，带来了革命，而或者只是在原来model的基础上添加了一些小的想法，刷了一下排行榜，这里引用下不久前ACL主席Christopher D. Manning在文章中写过一句话：</p>
<blockquote>
<p><strong>However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.</strong></p>
</blockquote>
<p>只是刷分并没有意义，研究的意义应该是对问题本身的认识。虽然deep learning看着热闹，会议非常多，隔一段时间就会出现一个会议论文集，但仔细想想有仅仅有几篇paper是在做更大更重要的事情，大量的paper还是处于model创新这个level。当然，毕业对论文的要求是一个很大的压力，所以这样的现象也并不奇怪。数量的增加必定会带来质量的下降，浮躁的气息必定会让大家都变得急功近利。</p>
<p>以上是一个初学者对deep learning的一个非常浅薄的认识，很多观点只是这一阶段的观点，可能随着学习的深入会有不同的想法。看paper也有一段时间了，心中一直有些疑问，比如：</p>
<p>1、gate函数的提出是基于怎样的一种情况，为什么要用gate而不是别的？lstm，gru等单元都采用了gate函数，通过这个函数成功地解决了rnn的长程依赖问题。</p>
<p>2、miniBatch中batch的大小为什么会影响结果的优劣，包括其他的超参数，可不可以给出一些偏理论的分析，而不只是说大家都这么用，这就是经验这样的说法。</p>
<p>3、hierarchical是不是都会比non-hierarchical更好呢？分层之后的什么导致了更好的结果？</p>
<p>4、optimization是一个数学味道非常浓的学科，那么很多的model都采用sgd，或者是adam，或者是rmsprop，区别在哪里？哪种算法适合哪种model。</p>
<p>5、deep learning到底多deep算deep，越deep越好？还是说到了一定的deep就ok了，神经网络是用来近似非线性函数的，那么可不可以计算出多深的网络可以以最低的误差来拟合函数？</p>
<p>等等等等，心中有太多的疑惑，虽然现在可以提自己的model，也可以用框架来编程实现，但仍然回答不出上面的问题，那些为什么的问题。</p>
<p>路漫漫其修远兮，吾将上下而求索。</p>
<p>最后是广告时间，如果您对PaperWeekly做的事情感兴趣，可以关注下面的公众号，或者<a href="http://rsarxiv.github.io/atom.xml">订阅</a>本博客。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-12T01:33:10.000Z"><a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">2016-06-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/">Gated-Attention Readers for Text Comprehension #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>完形填空一直是各大英语考试的常见题型，读一篇短文，填20个空。那么如果是机器来做完形填空，该如何来定义问题，提出模型呢？本周开始将会介绍一系列文本理解的模型。本文分享的题目是<a href="http://cn.arxiv.org/pdf/1606.01549v1" target="_blank" rel="external">Gated-Attention Readers for Text Comprehension</a>，最早于6月5日submit于arxiv上，作者是CMU的Graduate Research Assistant <a href="https://www.cs.cmu.edu/directory/bdhingra" target="_blank" rel="external">Bhuwan Dhingra</a>。</p>
<p>首先，介绍一下对完形填空问题的定义。问题可以表述为一个三元组(d,q,a)，这里d是指原文document，q是指完形填空的问题query（这里需要注意一点的是，与我们英语考试中的完形填空不同，更像是只用一个单词来回答的阅读理解），a是问题的答案。这个答案是来自一个固定大小的词汇表A中的一个词。即：给定一个文档-问题对(d,q)，从A中找到最合适的答案a。</p>
<p>本文精彩的部分有两个，一个是related work写的非常漂亮，另一个是提出了一种新的注意力模型GA（Gate-Attention） Reader，并且取得了领先的结果。</p>
<p>后续的文本理解系列的文章将会从related work中产生，包括以下几篇：</p>
<p>[1] <a href="http://arxiv.org/pdf/1506.03340v3.pdf" target="_blank" rel="external">Teaching machines to read and comprehend</a></p>
<p>[2] <a href="https://arxiv.org/pdf/1406.2710v1.pdf" target="_blank" rel="external">A multiplicative model for learning distributed text-based attribute representations</a></p>
<p>[3] <a href="http://www.cl.ecei.tohoku.ac.jp/publications/2016/kobayashi-dynamic-entity-naacl2016.pdf" target="_blank" rel="external">Dynamic entity representations with max-pooling improves machine reading</a></p>
<p>[4] <a href="http://arxiv.org/pdf/1603.01547v1.pdf" target="_blank" rel="external">Text understanding with the attention sum reader network</a></p>
<p>[5] <a href="http://arxiv.org/pdf/1511.02301v4.pdf" target="_blank" rel="external">The goldilocks principle: Reading children’s books with explicit memory representations</a></p>
<p>[6] <a href="http://arxiv.org/pdf/1606.02858v1.pdf" target="_blank" rel="external">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a></p>
<p>下面来介绍本文的模型，结合下图来看：</p>
<img src="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig1.png" width="600" height="600">
<p><b>step 1</b> document和query通过一个Lookup层，使得每个词都表示成一个低维向量。</p>
<p><b>step 2</b> 将document中的词向量通过一个双向GRU，将两个方向的state做拼接获得该词的新表示。同时也将query通过一个双向GRU，用两个方向上的last hidden state作为query的表示。</p>
<p><b>step 3</b> 将document中每个词的新表示与query的新表示逐元素相乘得到下一个GRU层的输入。</p>
<p><b>step 4</b> 重复step 2和3，直到通过设定的K层，在第K层时，document的每个词向量与query向量做内积，得到一个最终的向量。</p>
<p><b>step 5</b> 将该向量输入到softmax层中，做概率归一化。</p>
<p><b>step 6</b> 因为document中有重复出现的词，聚合之后得到最终的分类结果，即确定应该填哪个词。</p>
<p>模型的计算流程还是很好理解的，下面给出一些可视化的attention结果。</p>
<img src="/2016/06/12/Gated-Attention-Readers-for-Text-Comprehension-PaperWeekly/fig2.png" width="400" height="400">
<p>图中高亮的部分是针对问题时的最后一层注意力所关注的地方。</p>
<p>注意力模型是一个非常热门的研究领域，很多专家都看好其在今后各大nlp任务中的应用前景，不同版本、不同结构、不同层次的注意力模型丰富了模型，也提升了效果。注意力的本质就是说你关注的输出与你的输入中的哪个元素关系更加紧密，即输出的部分应该更加注意哪个输入细节，在做完形填空、阅读理解的时候，我们也会有这样的感受，就是题目的答案往往就在某一句话或某几句话当中，并不需要回答每个问题都从全文中找一遍答案，而是定位到关键句上。这里的定位就是注意力，剩下的问题就是研究如何更加准确地定义、建模注意力，是用普通的前馈神经网络，还是用GRU，还是用分层模型都需要针对具体问题的特点。</p>
<p>后续的几篇文章将会继续介绍文本理解，敬请关注。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-11T08:48:38.000Z"><a href="/2016/06/11/PaperWeekly文章分类导航/">2016-06-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/11/PaperWeekly文章分类导航/">PaperWeekly文章分类导航</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h1><p>[1] <a href="http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">Gated Word-Character Recurrent Language Model</a></p>
<h1 id="Word-Embeddings"><a href="#Word-Embeddings" class="headerlink" title="Word Embeddings"></a>Word Embeddings</h1><p>[1] <a href="http://rsarxiv.github.io/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">Efficient Estimation of Word Representations in Vector Space</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">GloVe: Global Vectors for Word Representation</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding</a></p>
<p>[4] <a href="http://rsarxiv.github.io/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">A Joint Model for Word Embedding and Word Morphology</a></p>
<h1 id="Sentence-Embeddings"><a href="#Sentence-Embeddings" class="headerlink" title="Sentence Embeddings"></a>Sentence Embeddings</h1><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">Learning Distributed Representations of Sentences from Unlabelled Data</a></p>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a></p>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification</a></p>
<h2 id="Semi-Supervised-Learning"><a href="#Semi-Supervised-Learning" class="headerlink" title="Semi-Supervised Learning"></a>Semi-Supervised Learning</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">Semi-supervised Sequence Learning</a></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><p>[1] <a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate</a></p>
<h2 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">Neural Network-Based Abstract Generation for Opinions and Arguments</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/05/18/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89/">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
<p>[3] <a href="http://rsarxiv.github.io/2016/05/17/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/">Neural Headline Generation with Minimum Risk Training</a></p>
<p>[4] <a href="http://rsarxiv.github.io/2016/05/11/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B9%9D%EF%BC%89/">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[5] <a href="http://rsarxiv.github.io/2016/05/10/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AB%EF%BC%89/">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></p>
<p>[6] <a href="http://rsarxiv.github.io/2016/05/07/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%B8%83%EF%BC%89/">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
<p>[7] <a href="http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[8] <a href="http://rsarxiv.github.io/2016/04/30/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E5%85%AD%EF%BC%89/">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</a></p>
<p>[9] <a href="http://rsarxiv.github.io/2016/04/24/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%EF%BC%88%E4%BA%94%EF%BC%89/">Generating News Headlines with Recurrent Neural Networks</a></p>
<h2 id="Text-Entailment"><a href="#Text-Entailment" class="headerlink" title="Text Entailment"></a>Text Entailment</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a></p>
<h2 id="Dialogue-Generation"><a href="#Dialogue-Generation" class="headerlink" title="Dialogue Generation"></a>Dialogue Generation</h2><p>[1] <a href="http://rsarxiv.github.io/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</a></p>
<p>[2] <a href="http://rsarxiv.github.io/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">A Neural Conversational Model</a></p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-10T04:34:27.000Z"><a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">2016-06-10</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/">Neural Network-Based Abstract Generation for Opinions and Arguments #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将会分享的是一篇工程性比较强的paper，如果您想做一个实实在在的意见摘要系统（比如：淘宝商品评论摘要、电影评论摘要）的话，可以仔细研读下本文的解决方案。本文的题目是<a href="http://arxiv.org/pdf/1606.02785v1.pdf" target="_blank" rel="external">Neural Network-Based Abstract Generation for Opinions and Arguments</a>，于6月9日submit于arxiv上。作者是来自美国东北大学的<a href="http://www.ccs.neu.edu/home/luwang/" target="_blank" rel="external">Lu Wang</a>助教。</p>
<p>关于<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>，之前写过一系列的文章，包含了自动文摘的方方面面以及近期的一些相关paper的详细描述。本文的自动文摘问题是一个多评论摘要问题，用的是abstractive方法，而非简单的extractive方法，就是说从多个评论中总结出观点。</p>
<p>本文模型的主题框架仍是seq2seq+attention，最主要的不同之处是输入包括多个文本序列，而是之前介绍的单文本序列。这里，seq2seq+attention的思路不再赘述，主要讲一下不同的地方。</p>
<p>为了套用seq2seq，本文将多文本拼接成单文本，中间用特殊的标记SEG隔开。但是如果只是简单的套用seq2seq的话，会存在以下两个问题：</p>
<p>1、seq2seq对序列的顺序非常敏感，多个文本排列的顺序对结果的影响比较大。</p>
<p>2、多篇评论包括的词会比较多，会导致在计算attention的时候花费更大的时间代价。</p>
<p>本文用了子采样(sub-sampling)的方法来解决上面的问题，首先给原始输入中的每个评论定义importance score，然后归一化，最后从原始输入中进行多项分布采样，获得K个候选sample作为seq2seq的输入数据，进行训练。本文针对importance score建立了一个回归模型，使用了一些人工feature作为输入进行回归打分。这些feature如下表所示：</p>
<img src="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig1.png" width="300" height="300">
<p>包括了词的数量，命名实体的数量，tf-idf平均数和最大数等8个feature作为输入。通过学习这个回归模型，来计算给定评论的分数。</p>
<p>最后给大家展示一个结果图：</p>
<img src="/2016/06/10/Neural-Network-Based-Abstract-Generation-for-Opinions-and-Arguments-PaperWeekly/fig2.png" width="600" height="600">
<p>本文在模型上创新的点并不突出，最不同以往的地方便是用了人工feature来给每个评论打分，给原始输入中的评论进行排序，然后多项分布采样，子采样的过程是一个降维的过程，保留了原始数据中最重要的部分，去掉了冗余的信息。可以说本文是将人工features添加到abstractive来提升纯粹的seq2seq模型性能，针对了多文档摘要问题的特点，给出了一个实用性较强的思路。如果从模型角度来说，新的东西没有太多，而且可改进的地方有很多，比如打分模型，可以用sentence representation的思路来做，完全可以避免用人工feature这种比较low的思路，做成一个data-driven的打分模型；再比如，不用打分，而是采用CNN从多个评论中提取出最有用的feature作为输入。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-09T06:34:03.000Z"><a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">2016-06-09</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/">A Joint Model for Word Embedding and Word Morphology #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>大家端午节快乐！本文将分享一篇关于词向量模型最新研究的文章，文章于6月8号提交到arxiv上，题目是<a href="http://cn.arxiv.org/pdf/1606.02601v1.pdf" target="_blank" rel="external">A Joint Model for Word Embedding and Word Morphology</a>，作者是来自剑桥大学的博士生<a href="https://www.cl.cam.ac.uk/~kc391/" target="_blank" rel="external">Kris Cao</a>。</p>
<p>本文最大的贡献在于第一次将词形联合词向量一同进行训练，从某种程度上解决了未登录词（OOV）的词向量表示问题，同时也得到了一个效果不错的词形分析器。</p>
<p>介绍本文模型之前先简单介绍下本文中采用的词向量训练方法，skip-gram with negative sampling（SGNS）。这个方法是word2vec中的一种方法，大概的思路是可参见下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig3.png" width="600" height="600">
<p>通过用dog这个词来预测其上下文，比如cute、fluffy、barked、loudly，为了更快地收敛，增加负样本，即图中的bicycle和Episcopal这两个与dog无关的词。skip-gram的思路就是通过word来预测上下文context，而negative sampling则是根据当前词构造出一些与之无关的词，作为负样本加速收敛。</p>
<p>接下来介绍本文的模型Char2Vec，将字符作为最小的单元进行研究，因为对于字符这个层次来说，并不会出现OOV词的情况。具体看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig1.png" width="600" height="600">
<p>在每个单词的首和尾分别添加符号^和$作为标记，将词看作是一个字符序列。在这个序列上用一个正向LSTM和一个反向LSTM得到两组hidden state，每个位置上的字符都对应着两个hidden state，将其拼接起来，然后用一个单层前馈神经网络进行处理，得到该位置上的hidden state，记为h(i)。有了每个字符的表示，接下来用attention机制来构造出词的表示，即学习一个权重系数，来表明这个词的语义与哪个h(i)关系更大，一般来说词干所在的h(i)权重会大一些，词前缀或者后缀并不能表示语义，所以权重会小很多。见下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig2.png" width="600" height="600">
<p>图中的单词malice、hatred、greed会与序列中的词干spit、spite，前缀^、后缀ful关系就会紧密一些，而与其他错误的字符串关系不大。</p>
<p>通过attention model我们得到了词向量f(w)。剩下的过程就是用skip-gram with negative sampling来训练词向量了。先前的工作都是用lstm处理字符序列来表示整个单词向量，本文并没有这样做，而是将直接使用attention model来获取每个h(i)中的信息，包括一个正向的lstm和反向的lstm，正向的lstm包含了词干和词前缀，反向的lstm包含了词后缀。当我们处理未知的词时，可以将这个词分解为已知的部分和未知的部分，这个模型就可以通过已知的部分来预测整个词的词向量，因此解决了OOV的问题。</p>
<p>实验中测试了该模型的词形分析的能力，尤其是在单词词形很丰富（包括词干、前缀、后缀）的情况下，效果优于一些成熟的分词器。看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig4.png" width="300" height="300">
<p>在词向量效果测试中，本文模型在semantic测试中表现很差，但在syntactic测试中表现非常好。看下图：</p>
<img src="/2016/06/09/A-Joint-Model-for-Word-Embedding-and-Word-Morphology-PaperWeekly/fig5.png" width="300" height="300">
<p>看得出来本文模型的优势非常明显，优势在于解决了大量处于长尾尾端的合成词的词向量表示问题，通过用未知词的已知部分（词干）来预测该词的词向量，从而解决了word2vec等一系列前人工作中未解决的问题，在英语语境中效果可能没那么好，如果换作是德语或者土耳其语这种词形非常丰富的语言会有更好的效果。在整个任务评测中，可以更好地解决syntactic相似问题，因为引入了词形这个feature可以很好地解决syntactic任务；而在semantic任务中却表现非常差，原因是char-level的词向量模型在捕捉语义上效果本身就不如word-level的模型。可以说，本文在传统词向量模型的基础上考虑加入feature来提升性能，是一种非常积极的尝试，虽然并没有在方方面面上都得到改善，但毕竟是一个探索性的、且非常有意义的工作，值得学习。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-08T01:00:50.000Z"><a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">2016-06-08</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/">Gated Word-Character Recurrent Language Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将会分享一篇最新的paper，2016年6月6日submit到arxiv上，paper的题目是<a href="http://cn.arxiv.org/pdf/1606.01700v1" target="_blank" rel="external">Gated Word-Character Recurrent Language Model</a>，作者是来自纽约大学的硕士生<a href="http://cds.nyu.edu/people/yasumasa-miyamoto/" target="_blank" rel="external">Yasumasa Miyamoto</a>。</p>
<p>语言模型或者说一切自然语言生成的问题都面临着一个严峻的挑战就是未登录词（OOV），一般的语言模型处理方法都是将前N个高频词当做词表，后面的低频词都用unk来代替，而且所有的低频词都用同一个词向量来表示。本文的最大贡献在于提出了一种混合char-level和word-level的语言模型，通过一种gate机制来选择是用char-level来表示一个词向量，还是直接用word-level来表示一个词向量。char-level模型的优势在于解决低频词的表达，很多之前分享过的模型都是用char来作为基本单元。</p>
<p>本文的模型并不复杂，思路也非常清晰，如下图：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig1.png" width="600" height="600">
<p>模型分为两个部分：</p>
<p>1、词向量。模型中的词向量由两部分综合而成。第一部分是传统的词向量，每一个词都用一个低维实向量来表示，第二部分是将每个词认为是一个char-level的序列，用一个双向LSTM来表示这个词。两部分词向量由一个门函数来决定使用哪个，如下式：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig2.png" width="300" height="300">
<p>门函数我们见过太多了，尤其是在LSTM和GRU中，各种各样的门函数来控制信息的流动，本文模型中采用了一种非常简单的机制来决定采用哪种词向量，高频词的话，一定是采用传统的word-level方式，直接从lookup table中读取；低频词的话，用char-level的方式获得一个更好的表示。这里需要注意的一点是，门函数的值，也就是说每个单词用哪种词向量是与上下文无关的，只要是同一个单词，就会采用相同的选择方式。</p>
<p>2、语言模型。这个部分就非常简单了，就是一个典型的RNNLM，这里的隐藏单元采用LSTM。</p>
<p>实验部分选了三个baseline，（1）仅仅用word-level，（2）仅仅用char-level，（3）将两种词向量拼接。在三个数据集上进行了测试，本文模型比起baseline具有明显的优势。</p>
<p>最后简单讨论了门函数值与词出现的频率之间的关系，如下图：</p>
<img src="/2016/06/08/Gated-Word-Character-Recurrent-Language-Model-PaperWeekly/fig3.png" width="600" height="600">
<p>本文采用了一中混合模型，然后用gate mechanism从多个模型中进行选择。这种思路有一种似曾相识的感觉，好比是参加kaggle比赛，通常一个分类器并不能得到最好的结果，混合使用多个分类器往往会得到更好的结果。本文的感觉有一点类似，用了char-level的优势来弥补word-level的劣势，从而取得更好的效果。也是一种很好的启发。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-06T23:11:34.000Z"><a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">2016-06-07</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/">Semi-supervised Sequence Learning #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>之前分享过几篇有监督的sentence表示方法，比如<a href="http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification</a>、<a href="http://rsarxiv.github.io/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification</a>，也分享过很多几篇无监督的sentence表示方法，比如<a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents</a>、<a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors</a>。本篇将分享是一篇半监督的sentence表示方法，该方法比Paragraph Vectors更容易做微调，与Skip-Thought相比，目标函数并没有它那么困难，因为Skip-Thought是用来预测相邻句子的。本文的题目是<a href="http://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="external">Semi-supervised Sequence Learning</a>，作者是来自Google的<a href="http://homepages.inf.ed.ac.uk/s0681274/About_Me.html" target="_blank" rel="external">Andrew M. Dai</a>博士。</p>
<p>纯粹的有监督学习是通过神经网络来表示一个句子，然后通过分类任务数据集去学习网络参数；而纯粹的无监督学习是通过上文预测下文来学习句子表示，利用得到的表示进行分类任务。本文的方法将无监督学习之后的表示作为有监督训练模型的初始值，所以称为半监督。本文的有监督模型采用LSTM，无监督模型共两种，一种是自编码器，一种是循环神经网络语言模型。</p>
<p>第一种模型称为Sequence AutoEncoder LSTM(SA-LSTM)，模型架构图如下：</p>
<img src="/2016/06/07/Semi-supervised-Sequence-Learning-PaperWeekly/fig1.png" width="400" height="400">
<p>这幅图大家看着都眼熟，和<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>中的seq2seq架构图很相似，只不过target和input一样，即用input来预测input自己。将自编码器学习到的表示作为LSTM的初始值，进行有监督训练。一般来说用LSTM中的最后一个hidden state作为输出，但本文也尝试用了每个hidden state权重递增的线性组合作为输出。这两种思路都是将无监督和有监督分开训练，本文也提供了一种联合训练的思路作为对比，称为joint learning。</p>
<p>第二种模型称为Language Model LSTM(LM-LSTM)，将上图中的encoder部分去掉就是LM模型。语言模型介绍过很多了，比如<a href="http://rsarxiv.github.io/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model</a>和<a href="http://rsarxiv.github.io/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models</a>，详细的可以看之前的分享，这里不再赘述了。</p>
<p>模型部分就是这些，后面作者在情感分析、文本分类、目标分类等多组任务中进行了对比实验，均取得了不错的结果。</p>
<p>本文的创新点在于结合了无监督和有监督学习两种思路的优点来解决一个传统问题，虽然说无监督是一种趋势所在，但有监督针对具体的问题会有更好的效果。这种融合各类模型优点的模型会更受欢迎，也是一种不错的思路。</p>
<p>这里进行几点说明：</p>
<p>1、为什么不对实验结果进行详细的介绍？</p>
<p>因为我个人更加关注的是解决问题的思路，也就是模型部分；另一方面，paper中的实验结果只能在某些程度上说明问题，对比结果中的数据可能是作者精心挑选的最好数据，并不一定可以复现，所以我不会太纠结于到底哪个模型比哪个模型高几个百分点。而文中的模型思路会带给我更多的启发，所以更加有意义一些。</p>
<p>2、为什么内容总是这么短？</p>
<p>因为我对PaperWeekly的定位是每天一篇或者几天一篇的paper短文介绍和理解，并不是详细地剖析它，我希望内容尽可能短，大家可以用5-10分钟来明白一篇文章的贡献和创新点在哪里，更多的是为了带给大家更多的思考或者说是启发。另外一个方面，短的文章我写起来也会很快，基本上都是前一晚睡觉前来读，六点半早起来写，不影响一天的正常工作生活，却一天一天地在积累着。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-05T23:09:52.000Z"><a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">2016-06-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/">A Hierarchical Neural Autoencoder for Paragraphs and Documents #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将会分享一篇用自动编码器(AutoEncoder)来做文档表示的文章，本文的结果会给自然语言生成、自动文摘等任务提供更多的帮助。本文作者是来自斯坦福大学的博士生<a href="http://web.stanford.edu/~jiweil/" target="_blank" rel="external">Jiwei Li</a>，简单看了下其简历，本科居然是北大生物系的，是一个跨界选手。本文的题目是<a href="http://arxiv.org/pdf/1506.01057.pdf" target="_blank" rel="external">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>，于2015年6月放在arxiv上。</p>
<p>自动编码器之前接触的并不多，所以读了下Yoshua Bengio的deep learning一书补了一下知识。其实挺简单的，就是通过构造一个网络来学习x-&gt;x，最简单的原型就是h=f(x)，x=g(h)。如果输入和输出的x都是完全一样的话，那么就没什么意义了。一般来说，后一个x会与前一个x有一些“误差”或者说“噪声”。而且自动编码器关注的是中间层h，即对输入的表示。如果h的维度小于x的维度，学习这个表示其实就是一个降维的过程。自动编码器有很多种类型，这里就不一一赘述了。</p>
<p>本文的贡献在于用分层LSTM模型来做自动编码器。模型分为三个，为递进关系。</p>
<p>1、标准的LSTM，没有分层。模型结构看起来和最简单的seq2seq没有区别，只是说这里输入和输出一样。看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig1.png" width="600" height="600">
<p>2、分层LSTM。这里分层的思想是用句子中的所有单词意思来表示这个句子，用文档中的所有句子意思来表示这个文档，一层接一层。看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig2.png" width="600" height="600">
<p>在word这一层，用一个标准的LSTM作为encoder，每一句中的最后一个word的hidden state作为该句的state，在sentence这一层，文档中所有的句子构成一个序列，用一个标准的LSTM作为encoder，得到整个文档的表示。decoder部分同样是一个分层结构，初始state就是刚刚生成的文档表示向量，然后先decoder出sentence这一层的表示，然后再进入该sentence对其内部的word进行decoder。</p>
<p>3、分层LSTM+Attention，这里的Attention机制和之前分享的是一样的，并且只在sentence这一层用了attention，参看下图：</p>
<img src="/2016/06/06/A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents-PaperWeekly/fig3.png" width="600" height="600">
<p>在decoder部分中生成句子表示时，会重点注意输入中与该句子相关的句子，也就是输入中与之相同的句子。这里注意力的权重与<a href="http://rsarxiv.github.io/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate </a>中的计算方法一样。</p>
<p>在实验中验证了本文模型的有效性，并且经过对比验证了第三种模型的效果最好，其次是第二种，最差的第一种，也与预期的相符。</p>
<p>昨天分享的也是一个分层模型，相比于单层的模型效果更好一些，这是否可以引起一些思考？本文也提到后面可以将本文的这种思想应用到自动文摘、对话系统、问答系统上。虽然seq2seq+attention已经在这几大领域中取得了不错的成绩，但如果改成分层模型呢，是不是可以取得更好的成绩？是不是可以将本文的input和output换作自动文摘中的input和target，然后用同样的方法来解决呢？我想应该是可以的。</p>
<p>另外，因为我个人比较关注自动文摘技术，自动文摘中abstractive类的方法一般都会涉及到Paraphrase（转述，换句话说），本文的自动编码器模型正好很适合做Paraphrase，输入一句话或者一段话，得到一个带有“误差”的语句通顺的版本。一种最简单的思路，用传统的方法提取出文中最重要的几句话（extractive式的方法），用Paraphrase处理一下得到文本摘要。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-05T02:57:48.000Z"><a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">2016-06-05</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>昨天介绍了一篇工程性比较强的paper，关于对话生成（bot）任务的，今天继续分享一篇bot方面的paper，6月2日刚刚submit在arxiv上。昨天的文章用了一种最最简单的端到端模型来生成对话，取得了不错的结果，而本文用了一种更加复杂的模型来解决这个问题，取得了更好的结果。文章的题目是<a href="http://cn.arxiv.org/pdf/1606.00776v1" target="_blank" rel="external">Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation</a>，作者是来自蒙特利尔大学的博士生<a href="https://mila.umontreal.ca/en/person/iulian-vlad-serban/" target="_blank" rel="external">Iulian Vlad Serban</a>。</p>
<p>本文最大的贡献在于提出了一种多尺度循环神经网络（Multiresolution RNN,MrRNN），这里的多尺度是指描述文本序列的方式有多种尺度，不仅仅是传统的用一个又一个word来表示序列，这种表示称为自然语言表示，还包括了一种所谓的high-level信息来表示文本序列，这种表示称为粗糙序列表示。本文的模型受启发于分层循环端到端模型（Hierarchical Recurrent Encoder-Decoder，HERD），该模型应用于搜索领域，将用户的search session划分为两个层次的序列，一个是query的序列，一个是每个query中词的序列。</p>
<p>本文模型中一个非常重要的部分是数据的预处理，将训练数据中的所谓high-level信息提取出来构造第二种序列来表示整个文本，这里用了两种思路。</p>
<p>1、提取文本中的名词。用词性标注工具提取出文本中的名词，去掉停用词和重复的词，并且保持原始的词序，还添加了句子的时态。通过这个过程构造了一种表示原始文本的序列。</p>
<p>2、提取文本中的动词和命名实体。用词性标注工具提取文本中的动词，并标记为activity，然后通过一些其他工具从所有训练数据中构造了一个命名实体的词典，帮助提取原句中的命名实体。因为数据集是ubuntu对话数据集，会涉及到大量的linux命令，所以还构造了一个linux命令词典，以标记原句中的命令。同样地也添加了句子的时态。通过这个处理过程，构造了另外一种表示原始文本的序列。</p>
<p>两种处理方法将原句用一种关键词的形式表示出来，尤其是第二种方法针对Ubuntu数据集的特点，包含了非常多的特征进来。这样的表示本文称为coarse sequence representation，包含了high-level的信息，比起单纯的word by word sequence具有更加丰富的意义。</p>
<p>接下来，看一下本文模型的架构图：</p>
<img src="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig1.png" width="600" height="600">
<p>模型中包括了两个层次，或者说是两种尺度，一种用了很多的词来表示一个句子，另外一种用了经过处理的包含了更加重要的信息的词来表示一个句子。下层生成的预测值将会作为上层docoder在做预测时的context的一部分，这部分context包含了重要的、high-level的信息，再加上上层自己encoder的输出也作为context，可以说这个模型的context包含了非常丰富的内容。理解上面的图，只要仔细看好箭头的指向，也就明白了各个部分的输入输出是哪些。每个time step的数据流过程如下：</p>
<p>下层：coarse encoder -&gt; coarse context -&gt; coarse decdoer -&gt; coarse predciton encoder</p>
<p>上层：natural language encoder -&gt; <b>(natural language context + coarse prediction encoder)</b> -&gt; natural language decoder -&gt; natural language prediction</p>
<p>不管是用自动评价指标还是人工评价，结果都表明了本文的模型效果比baseline要高出很多个百分点，远远好于其他模型。下面展示一个结果，是ubuntu数据集上的测试效果：</p>
<img src="/2016/06/05/Multiresolution-Recurrent-Neural-Networks-An-Application-to-Dialogue-Response-Generation-PaperWeekly/fig2.png" width="600" height="600">
<p>可以看的出本文模型生成的结果效果比其他模型好很多。</p>
<p>本文模型并不是一个纯粹的数据驱动的模型，在初始的阶段需要做一些非常重要的数据预处理，正是这个预处理得到的序列表示给本文的好结果带来了保证。我想，这种处理问题的思路可以推广到解决其他问题上，虽然本文模型很难直接应用到其他问题上，但我相信经过一些不大的变化之后，可以很好地解决其他问题，比如我一直关注的自动文摘问题，还有机器翻译、自动问答等等各种涉及到自然语言生成问题的任务上。这篇文章的结果也支持了我之前的一个观点，就是在解决问题上不可能存在银弹，不同的问题虽然可以经过一些假设变成相同的数学问题，但真正在应用中，不同的问题就是具有不同的特点，如果只是想用一种简单粗暴的data driven模型来解决问题的话，相信效果会不如结合着一些该问题feature的模型。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-04T02:57:31.000Z"><a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">2016-06-04</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/">A Neural Conversational Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面介绍过几篇seq2seq在机器翻译、文本蕴藏、自动文摘领域的应用，模型上每篇稍有不同，但基本的思想是接近的。本文继续分享一篇seq2seq在对话生成任务上的应用，是一篇工业界的论文，因此并没有什么理论创新。之所以选这一篇，是因为对话生成是一个非常热门的研究领域和应用领域，也可能是一个非常热门的创业领域，另外一个原因是为了充实seq2seq在各个领域中的应用这一主题。论文题目是<a href="http://cn.arxiv.org/pdf/1506.05869.pdf" target="_blank" rel="external">A Neural Conversational Model</a>，作者是来自Google Brain，毕业于UC Berkeley的<a href="http://www1.icsi.berkeley.edu/~vinyals/" target="_blank" rel="external">Oriol Vinyals</a>博士，论文最早于2015年7月放在arxiv上。</p>
<p>模型部分不用多说，是最简单的seq2seq，架构图如下：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig1.png" width="400" height="400">
<p>本篇主要想分享的东西是结果以及一些思考。文中采用了两个数据集，IT Helpdesk Troubleshooting dataset和OpenSubtitles dataset，前者是一个关于IT类的FAQ数据集，后者是一个电影剧本的数据集。</p>
<p>我们可以看一下训练后的模型生成的对话结果，这里只关注第二个数据集的结果：</p>
<p>常识类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig2.png" width="400" height="400">
<p>哲学类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig3.png" width="400" height="400">
<p>道德类问题：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig4.png" width="400" height="400">
<p>为了对比，作者添加了一组<a href="www.cleverbot.com">cleverbot</a>(cleverbot是一个在线聊天机器人)的对比结果，如下：</p>
<img src="/2016/06/04/A-Neural-Conversational-Model-PaperWeekly/fig5.png" width="400" height="400">
<p>从对比结果中可以看得出，本文模型生成的结果比网上流行的在线聊天机器人要看起来更加“智能”一些，之前在知乎上回答过一个问题 <a href="https://www.zhihu.com/question/46558198/answer/102722213?group_id=720215813641998336" target="_blank" rel="external">三代聊天机器人在技术上的区别在哪里？</a>，我想cleverbot更接近于第二代，采用了对话检索，即对话是从一个很庞大的数据库中匹配检索来的，而本文的模型属于第三代，更加智能，给定输入生成输出，并不需要借助于人工特征。</p>
<p>但bot这个领域确实还面临一些问题，就像文中作者所说，如何客观地评价生成的效果非常重要，尤其是对于一些没有标准答案的问题来说，根本无法衡量哪个结果更加好。其实不仅仅bot，在自动文摘、机器翻译等各种nlp任务中，评价都是一个很难的问题，自动评价只是从某种意义上解决了各个模型相互比较的一种需求，但在实际应用当中用户的评价更加重要，虽然有时并不是那么客观。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-02T23:06:55.000Z"><a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">2016-06-03</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面几篇文章分享的都是seq2seq和attention model在机器翻译领域中的应用，在自动文摘系列文章中也分享了六七篇在<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>领域中的应用。本文将分享的这篇文章研究了seq2seq+attention在<a href="https://en.wikipedia.org/wiki/Textual_entailment" target="_blank" rel="external">textual entailment</a>领域的应用。本文题目是<a href="http://arxiv.org/pdf/1509.06664.pdf" target="_blank" rel="external">REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</a>，作者是来自英国伦敦大学学院的<a href="http://rockt.github.io/" target="_blank" rel="external">Tim Rocktaschel</a>博士（后面两个作者来自Google Deepmind），文章于2015年9月放在arxiv上，被ICLR 2016录用。</p>
<p>首先，介绍一下文本蕴藏（textual entailment）是一个怎样的任务，简单点说就是用来判断两个文本序列之间的是否存在推断关系，是一个分类问题（具体可参见Wikipedia）。两个文本序列分别称为premise和hypothesis。</p>
<p>本文最大的贡献在于：</p>
<p>1、将end2end的思想应用到了文本蕴藏领域，取得了不错的效果。</p>
<p>2、提出了一种seq2seq模型、两种attention模型和一种trick模型。</p>
<p>本篇关注的重点在于四种模型的构建，来看模型架构图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig1.png" width="600" height="600">
<p>图中A是我们最熟悉的简单seq2seq模型，本文称为conditional encoding模型；B是本文提出的Attention模型，context与之前分享的都不一样；C是我们之前介绍过的attention模型，本文称为word-by-word attention模型。</p>
<p>1、首先介绍A模型。将premise和hypothesis认为是source和target，即用encoder来处理premise，用decoder来处理hypothesis，只不过这里的decoder并不是一个语言模型，而只是一个和encoder一样的LSTM，decoder的输入是encoder的最后一个hidden state，对应图中的c5和h5。最后decoder的输出是一个联合表示premise和hypothesis的向量，用于最终的分类。</p>
<p>2、介绍B模型。该任务和机器翻译不同，并不一定需要做所谓的soft alignment，而是只需要表示好两个句子之间的关系即可，因此这个模型的想法是将hypothesis的句子表示与premise建立注意力机制，而不是将hypothesis的每个单词都与premise做alignment。从上图中标记B的地方也可以看出，attention仅仅依赖于hypothesis的last hidden state。结果可以参看下图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig2.png" width="600" height="600">
<p>从图中可以看出hypothesis与premise中哪些词相关性更强。</p>
<p>3、介绍C模型。这个模型与我们之前一直分享的attention模型一致，模型对hypothesis和premise每个单词做了alignment，所以这里称为word-by-word attention，从模型架构图中也可以看出，hypothesis中的每个词都与premise中对应的词进行了alignment。这里并不是生成单词，而是建立起两个文本序列之间的关系。结果可以参看下图：</p>
<img src="/2016/06/03/REASONING-ABOUT-ENTAILMENT-WITH-NEURAL-ATTENTION-PaperWeekly/fig3.png" width="600" height="600">
<p>图中表示的是alignment矩阵，更暗的地方表示这两个词更加相关。</p>
<p>4、最后一种模型称为two-way模型，其实是一个trick，借鉴了BiRNN的思想，使用两个相同参数的LSTM，第一个LSTM从一个方向上对基于hypothesis的premise进行表示，而第二个LSTM从相反的方向上对基于premise的hypothesis进行表示，最终得到两个句子表示，拼接起来作为分类的输入。（过程与BiRNN类似，从两个方向上对hypothesis-premise进行了表示，可与前面的模型组合使用，从结果上来看并没有什么明显的作用）</p>
<p>最后的实验结果表明，采用模型C，即word-by-word attention模型效果最好，其次是B模型，最差的是A模型。结果与预期基本符合，但加了two-way的效果并没有更好，反而更差。作者分析说用了相同的参数来做two-way可能会给训练给来更多的噪声影响，所以效果并不好。</p>
<p>整体上来说，seq2seq+attention的组合给很多研究领域带来了春天，给了研究者们更多的启发，attention的形式有多种，可能针对不同的问题，不同的attention会带来不同的效果，也不好说哪一种一定更加适合某一个特定的任务，所以需要去不断地探索。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-02T04:30:30.000Z"><a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">2016-06-02</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/">Neural Machine Translation by Jointly Learning to Align and Translate #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>前面的两篇文章简单介绍了seq2seq在机器翻译领域的尝试，效果令人满意。上一篇也介绍到这一类问题可以归纳为求解P(output|context)的问题，不同的地方在于context的构建思路不同，上两篇中的seq2seq将context定义为encoder的last hidden state，即认为rnn将整个input部分的信息都保存在了last hidden state中。而事实上，rnn是一个有偏的模型，越靠后的单词在last state中占据的“比例”越高，所以这样的context并不是一个非常好的办法，本文将分享的文章来解决这个问题。题目是<a href="http://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a>，作者是来自德国雅各布大学的<a href="http://minds.jacobs-university.de/dima" target="_blank" rel="external">Dzmitry Bahdanau</a>，现在是Yoshua Bengio组的一个博士生，文章于2015年4月放在arxiv上。</p>
<p>本篇不再讨论seq2seq，如果您想了解seq2seq，可以去看<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>和<a href="http://rsarxiv.github.io/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a>两篇博客。本篇只讨论不同的地方。</p>
<p>本文用encoder所有hidden state的加权平均来表示context，权重表示decoder中各state与encoder各state的相关性，简单的seq2seq认为decoder中每一个state都与input的全部信息（用last state表示）有关，而本文则认为只与相关的state有关系，即在decoder部分中，模型只将注意力放在了相关的部分，对其他部分注意很少，这一点与人类的行为很像，当人看到一段话或者一幅图的时候，往往会将注意力放在一个很小的局部，而不是全部。具体看下图：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig1.png" width="400" height="400">
<p>decoder中预测每个输出的条件概率变为：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig2.png" width="300" height="300">
<p>这里每个time step的state变为：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig3.png" width="300" height="300">
<p>这里，context vector由下式计算：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig4.png" width="300" height="300">
<p>权重用了一个最简单的mlp来计算，</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig6.png" width="300" height="300">
<p>然后做归一化：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig5.png" width="300" height="300">
<p>这里的权重反应了decoder中的state s(i-1)和encoder中的state h(j)之间的相关性。本文在为了得到相对来说无偏的state，在encoder部分采用了BiRNN。</p>
<p>在机器翻译领域中，attention model可以理解为source和target words的soft alignment，像下图一样：</p>
<img src="/2016/06/02/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate-PaperWeekly/fig7.png" width="500" height="500">
<p>上图是英语翻译成法语的一个结果。越亮的地方表示source和target中的words相关性越强（或者说对齐地越准），图中的每一个点的亮度就是前面计算出的权重。</p>
<p>本文最大的贡献在于提出了attention model，为今后研究对话生成，问答系统，自动文摘等任务打下了坚实的基础。context的定义也成为了一个非常有意思的研究点，rnn是一种思路，cnn同样也是一种思路，简单的word embedding也可以算是一种思路，交叉起来rnn+cnn也可以作为一种思路，将word替换成char可以作为一种思路，思路其实非常多，不同的组合有不同的模型，都可以去探索。</p>
<p>另外，不知道是不是Yoshua Bengio组的习惯，本文也在附录附上了详细的模型推导过程。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-01T00:15:17.000Z"><a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">2016-06-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将分享的文章相比于昨天那篇<a href="http://rsarxiv.github.io/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks</a>更早地使用了seq2seq的框架来解决机器翻译的问题，可能上一篇来自于Google，工程性更强一些，学术性有一些不足。本文来自于学术机构，学术范更浓一些。本文的题目是<a href="http://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder–Decoder for Statistical<br>  Machine Translation</a>，作者是来自蒙特利尔大学的<a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a>博士（现在在纽约大学任教），2014年6月登在arxiv上。</p>
<p>本文最大的两个贡献是：</p>
<p>1、提出了一种类似于LSTM的GRU结构作为RNN的hidden unit，并且具有比LSTM更少的参数，更不容易过拟合。</p>
<p>2、较早地（据说2013年就有人提出用seq2seq思路来解决问题）将seq2seq应用在了机器翻译领域，并且取得了不错的效果。</p>
<p>自然语言生成（NLG）领域中有很多任务，比如机器翻译，<a href="http://rsarxiv.github.io/tags/%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98/">自动文摘</a>，自动问答，对话生成等，都是根据一个上下文来生成一个文本序列，这里分类两个过程，encoder部分将输入序列表示成一个context，decoder部分在context的条件下生成一个输出序列，联合训练两个部分得到最优的模型。这里的context就像是一个memory，试着保存了encoder部分的所有信息（但往往用较低的维度表示整个输入序列一定会造成信息损失）。本文的思路就是如此，具体可参看下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig1.png" width="600" height="600">
<p>本文模型将encoder部分的最后一个hidden state作为context输入给decoder，decoder中的每一个时间t的hidden state s(t)都与context,s(t-1),y(t-1)有关系，而每一个时间t的输出y(t)都与context,s(t),y(t-1)有关。当然，这种模型是非常灵活的，你的context可以有很多种选择，比如可以选encoder中所有hidden state组成的矩阵来作为context，可以用BiRNN计算出两个last hidden state进行拼接作为context；而s(t)和y(t)根据RNN结构不同，也可以将context作为s(0)依次向后传递，而不是每次都依赖于context。</p>
<p>说完了模型部分，来说说本文最大的贡献是提出了GRU，一种更轻量级的hidden unit，效果还不输LSTM，函数结构如下图：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig2.png" width="400" height="400">
<p>GRU有两个门函数，reset gate和update gate，公式如下：</p>
<p>reset gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig3.png" width="300" height="300">
<p>update gate：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig4.png" width="300" height="300">
<p>reset gate接近于0的时候，当前hidden state会忽略前面的hidden state，在当前输入处reset。reset gate控制了哪些信息可以通过，而update gate控制着多少信息可以通过，与LSTM中的cell扮演着相似的角色。计算出每一步的reset和update gate，即可计算出当前的hidden state，如下：</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig5.png" width="300" height="300">
<p>这里，</p>
<img src="/2016/06/01/Learning-Phrase-Representations-using-RNN-Encoder–Decoder-for-Statistical-Machine-Translation-PaperWeekly/fig6.png" width="300" height="300">
<p>实验部分，作者利用本文模型得到了满意的结果，不再赘述。</p>
<p>另外，本文在附录部分给出了一个比较详尽的encoder-decoder公式推导过程，大家可以参看原文。</p>
<p>从context预测output，是一件很神奇的事情。而context又是千变万化的，当下正流行的模型attention model正是在context上做了文章，得到了更好的结果。相信，对context的变化和应用会带来更多好玩的模型。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-31T00:22:43.000Z"><a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">2016-05-31</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/">Sequence to Sequence Learning with Neural Networks #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>seq2seq+各种形式的attention近期横扫了nlp的很多任务，本篇将分享的文章是比较早（可能不是最早）提出用seq2seq来解决机器翻译任务的，并且取得了不错的效果。本文的题目是<a href="http://cn.arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a>，作者是来自Google的<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a>博士（现在OpenAI）。可以说这篇文章较早地探索了seq2seq在nlp任务中的应用，后续的研究者在其基础上进行了更广泛的应用，比如自动文本摘要，对话机器人，问答系统等等。</p>
<p>这里看一张很经典的图，如下：</p>
<img src="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig1.png" width="600" height="600">
<p>图的左半边是encoder，右半边是decoder，两边都采用lstm模型，decoder本质上是一个rnn语言模型，不同的是在生成词的时候依赖于encoder的最后一个hidden state，可以用下式来表示：</p>
<img src="/2016/05/31/Sequence-to-Sequence-Learning-with-Neural-Networks-PaperWeekly/fig2.png" width="300" height="300">
<p>模型非常简单，就是最普通的多层lstm，实际实现的时候有几点不同：</p>
<ul>
<li><p>用了两种不同的lstm，一种是处理输入序列，一种是处理输出序列。</p>
</li>
<li><p>更深的lstm会比浅的lstm效果更好，所以本文选择了四层。</p>
</li>
<li><p>将输入的序列翻转之后作为输入效果更好一些。</p>
</li>
</ul>
<p>这里在decoder部分中应用了beam search来提升效果，beam search大概的思路是每次生成词是取使得整个概率最高的前k个词作为候选，这里显然beam size越大，效果越好，但是beam size越大会造成计算的代价也增大，所以存在一个trade off。</p>
<p>最后通过机器翻译的数据集验证了了seq2seq模型的有效性。</p>
<p>这里需要讨论的一点是，为什么将输入倒序效果比正序好？文中并没有说，只是说这是一个trick。但后面读了关于attention的文章之后，发现soft attention或者说alignment对于seq2seq这类问题有着很大的提升，我们都知道rnn是一个有偏模型，顺序越靠后的单词在最终占据的信息量越大，那么如果是正序的话，最后一个词对应的state作为decoder的输入来预测第一个词，显然在alignment上来看，这两个词并不是对齐的，反过来，如果用倒序的话，之前的一个词成了最后一个词，在last state中占据了主导，用这个词来预测decoder的第一个词，从某种意义上来说实现了alignment，所以效果会好一些。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-30T14:11:49.000Z"><a href="/2016/05/30/大牛主页/">2016-05-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/30/大牛主页/">大牛主页</a></h1>
  

    </header>
    <div class="entry">
      
        <p>本篇将汇总PaperWeekly翻译过的作者的主页（持续更新中）：</p>
<p>1、Konstantin Lopyrev  <b><a href="https://github.com/klopyrev" target="_blank" rel="external">Github</a></b></p>
<p>2、<a href="http://people.seas.harvard.edu/~srush/" target="_blank" rel="external">Alexander M. Rush</a> </p>
<p>3、<a href="https://research.facebook.com/sumit-chopra" target="_blank" rel="external">Sumit Chopra</a></p>
<p>4、<a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-nallapati" target="_blank" rel="external">Ramesh Nallapati</a></p>
<p>5、<a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-zhou" target="_blank" rel="external">Bowen Zhou</a></p>
<p>6、<a href="http://nlp.stanford.edu/~jpennin/" target="_blank" rel="external">Jeffrey Pennington</a></p>
<p>7、<a href="https://research.facebook.com/tomas-mikolov" target="_blank" rel="external">Tomas Mikolov</a></p>
<p>8、<a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" target="_blank" rel="external">Yoshua Bengio</a></p>
<p>9、<a href="http://www.people.fas.harvard.edu/~yoonkim/" target="_blank" rel="external">Yoon Kim</a> <b><a href="https://github.com/yoonkim" target="_blank" rel="external">Github</a></b></p>
<p>10、<a href="http://cs.stanford.edu/~quocle/" target="_blank" rel="external">Quoc Le</a></p>
<p>11、<a href="http://www.cs.toronto.edu/~rkiros/" target="_blank" rel="external">Ryan Kiros</a>  <b><a href="https://github.com/ryankiros" target="_blank" rel="external">Github</a></b></p>
<p>12、<a href="http://www.cl.cam.ac.uk/~fh295/" target="_blank" rel="external">Felix Hill</a></p>
<p>13、<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a></p>
<p>14、[Dzmitry Bahdanau] <b><a href="https://github.com/rizar" target="_blank" rel="external">Github</a></b></p>
<p>15、<a href="http://rockt.github.io/" target="_blank" rel="external">TIM ROCKTÄSCHEL</a> <b><a href="https://github.com/rockt" target="_blank" rel="external">Github</a></b></p>
<p>16、<a href="http://www1.icsi.berkeley.edu/~vinyals/" target="_blank" rel="external">Oriol Vinyals</a> </p>
<p>17、<a href="https://mila.umontreal.ca/en/person/iulian-vlad-serban/" target="_blank" rel="external">Iulian Vlad Serban</a> <b><a href="https://github.com/julianser" target="_blank" rel="external">Github</a></b></p>
<p>18、<a href="http://web.stanford.edu/~jiweil/" target="_blank" rel="external">Jiwei Li</a> <b><a href="https://github.com/jiweil" target="_blank" rel="external">Github</a></b></p>
<p>19、<a href="http://homepages.inf.ed.ac.uk/s0681274/About_Me.html" target="_blank" rel="external">Andrew M. Dai</a></p>
<p>20、<a href="http://www.ccs.neu.edu/home/luwang/" target="_blank" rel="external">Lu Wang</a></p>
<p>21、<a href="http://www.karlmoritz.com/" target="_blank" rel="external">Karl Moritz Hermann</a> <b><a href="https://github.com/karlmoritz" target="_blank" rel="external">Github</a></b></p>
<p>22、<a href="http://cs.stanford.edu/people/danqi/" target="_blank" rel="external">Danqi Chen</a> <b><a href="https://github.com/danqi" target="_blank" rel="external">Github</a></b></p>
<p>23、<a href="http://www.sc.isc.tohoku.ac.jp/~koba/" target="_blank" rel="external">Sosuke Kobayashi</a></p>
<p>24、<a href="http://people.csail.mit.edu/karthikn/" target="_blank" rel="external">Karthik Narasimhan</a> <b><a href="https://github.com/karthikncode" target="_blank" rel="external">Github</a></b></p>
<p>25、<a href="http://tejask.com/" target="_blank" rel="external">Tejas Kulkarni</a> <b><a href="https://github.com/mrkulk" target="_blank" rel="external">Github</a></b></p>
<p>26、<a href="http://www.site.uottawa.ca/~hguo028/mainpage.htm" target="_blank" rel="external">Hongyu Guo</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-30T05:49:58.000Z"><a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">2016-05-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/">Learning Distributed Representations of Sentences from Unlabelled Data #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>sentence representation的文章已经分享了几篇，包括了supervise和unsupervise的方法，但并没有对各种model进行系统地对比和分析，今天分享的这篇文章对现有各种各样的distributed representations of sentences model进行了分类、对比和分析，为了增强对比效果，还提出了两种虚拟的模型。最后将所有的模型在supervised和unsupervised评价任务中进行对比，得出了一些有意义的结论。本文的题目是：<a href="http://arxiv.org/pdf/1602.03483v1.pdf" target="_blank" rel="external">Learning Distributed Representations of Sentences from Unlabelled Data</a>，作者是来自剑桥大学的<a href="https://www.cl.cam.ac.uk/~fh295/" target="_blank" rel="external">Felix Hill</a>博士。</p>
<p>首先对现有模型进行了分类描述。</p>
<ul>
<li><p>直接在纯文本上进行训练的模型，模型包括：<a href="http://rsarxiv.github.io/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vector</a>、<a href="http://rsarxiv.github.io/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Paragraph Vector</a>，两种模型都在之前分享过。</p>
</li>
<li><p>在结构化资源上进行训练的模型，这种模型借助了一些纯文本之外的资源进行辅助训练。模型包括：DictRep、CaptionRep、NMT。</p>
</li>
</ul>
<p><code>DictRep</code>是本文作者之前提出的一个模型，模型训练了一个从词典定义到预训练好的词向量之间的映射。</p>
<p><code>CaptionRep</code>模型架构与DictRep一样，采用的数据集不同而已，这里使用了COCO数据集，训练一个从图像vector representation到图像caption的映射。</p>
<p><code>NMT</code>是神经网络机器翻译，该模型架构与skip-thought vector模型相同，但训练数据换成了sentence-aligned翻译文本，WMT语料中的En-Fr和En-De。</p>
<ul>
<li>本文提出的一些新模型。为了解决当前存在模型的问题，本文设计了两种虚拟模型。包括：Sequential (Denoising) Autoencoders(SDAE、SAE)和FastSent。</li>
</ul>
<p><code>SDAE</code>模型是为了解决Skip-Thought Vector模型对语料中句子连贯性的依赖问题。传统的去噪自编码器（DAE）一般都是一个输入是固定尺寸图像数据的前馈神经网络，本文利用一个噪声函数将传统的DAE扩展到变长度句子中，噪声函数是N(S|p0,px)，S表示一个句子，p0,px都是一个[0,1]的数，表示概率。首先，对于每一个S中的word，N函数会以一个p0的概率来删除word，概率是相互对立的。然后，对于S中的每一对不重叠的bigram，w(i)w(i+1)，N函数会以一个px的概率来交换两个词的位置。最后用一个类似NMT的encoder-decoder模型进行训练，只不过不同的是目标函数变了，变成了使得噪声最小。这里，source是经过噪声函数处理过的sentence，target是原始的sentence。这个模型就是SDAE模型，相比于skip-thought vector，可以处理任意顺序的句子集。如果令px=0,p0=0，我们称为<code>SAE</code>模型。这里p0其实就是防止深度网络模型训练时过拟合的正则化方法<code>Dropout</code>。</p>
<p><code>FastSent</code>模型旨在解决Skip-Thought Vector模型计算速度慢的缺点，解决的思路与word2vec突破传统多层神经网络语言模型的思路类似，只用了一个简单的log-linear层。给定一个用词袋模型表示的句子，模型来预测该句子两边相邻的句子。该模型在训练时也会学习句中每个单词的词向量，并且将句子用句中所有词的词向量之和来表示。</p>
<p>下图给出了所有模型在性能上的比较：</p>
<img src="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig1.png" width="500" height="500">
<p>其中，OS是指是否需要保留句子在语料中的顺序；R表示需要结构化的训练资源；WO：对词序敏感；SD：句子向量维度；WD：词向量维度；TR：训练时间；TE：编码50w句子需要的时间。</p>
<p>任务评价一共分为两类，监督学习任务和无监督学习任务。通过大量实验的比较，得出了一下的结论：</p>
<ul>
<li><p>不同的任务适合不同的表示模型，这听起来像一句废话，也就是说没有哪种模型可以通吃所有的任务。比如：Skip-Thought Vector模型在TREC任务中最好，是因为句子和句子之间的衔接非常好，非常适合这个模型的特点。而Paraphrase detection任务更加适合于SDAE模型。</p>
</li>
<li><p>监督学习和无监督学习任务的表现存在差异，在监督学习任务中表现好的模型在无监督学习模型中表现的就会很一般，带有非线性网络结构的Skip Thought Vector、SDAE、NMT模型在监督学习中表现更好，而log-linear类的模型FastSent则在无监督学习任务中表现更好。</p>
</li>
<li><p>额外的资源会影响到训练处模型的通用性和实用性，比如一个在线demo需要很快的查询最近邻速度，用fastsent可能就没有问题，但用其他模型就达不到快速的要求。</p>
</li>
<li><p>词序的重要性并没有得到体现。本文的结果给出了一个与常识相左的结论，词序在决定句子意思表示时并没有想象中的那么重要。作者说到，可能是因为当前的评价方式并不能反映出词序的重要性，所以这个问题得不出一个明确的答案。（这点很有意思，在前面分享的一篇文章<a href="http://rsarxiv.github.io/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding</a>中，引用了一个结论，词序信息占了语义信息的20%，那么到底词序对于句子语义有多大的影响？需要好好研究一番）</p>
</li>
<li><p>评价指标存在缺陷，并不能绝对准确的对比出各个模型的差异。</p>
</li>
</ul>
<p>最后，展示一个各模型训练之后的应用效果。</p>
<img src="/2016/05/30/Learning-Distributed-Representations-of-Sentences-from-Unlabelled-Data-PaperWeekly/fig2.png" width="600" height="600">
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-29T06:09:18.000Z"><a href="/2016/05/29/我以为/">2016-05-29</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/29/我以为/">我以为</a></h1>
  

    </header>
    <div class="entry">
      
        <p>周末了，给自己放一个假，今天不分享paper了，分享一些别的东西。</p>
<p>一直认为人能够坚持并且努力做好一件事情的最大动力是热爱，是那种没有半点虚伪、没有半点功利的热爱。因为热爱，所以纯粹。于是，在经过几个月内心的煎熬和挣扎，我决定上路了，从此不再为那些不感兴趣、耗时耗力但却没有半点成就感的事情而纠结了，心中犹如放下一块巨石，终于可以静下心来做一些让自己内心感到充实的事情。</p>
<p>从小生活在一个小县城里，没有那些IT大牛传说一般的经历，小时候爸爸妈妈也并没有给我买电脑，让我成为一个电脑神童，只是希望我快乐地活在这个世界上。第一次接触电脑是小学毕业时的暑假，拿着妈妈给的零花钱在家附近的网吧打CS，那个时候第一次从电脑中获得了一些肤浅的快乐。对“编程”有一种魔性地冲动，是在初二看了一本盗版的比尔盖茨传，里面讲着盖茨各种传奇的故事，用一台电脑创造了一个帝国，当时对他的种种经历都非常崇拜，还跟着传记里的一些描述来模仿他，困了累了的时候开始前后摇晃自己的身体。那个时候讲盖茨视为自己的偶像，是那种真正的偶像，和现在很多女孩迷小鲜肉是一种感觉。那个时候经常会幻想自己有朝一日可以写代码，可以像偶像一样建立一个帝国。然而，梦想在不正确的时间可能只适合放在心里珍藏起来。</p>
<p>初中毕业之后，考上了省里最好高中之一，来到了省城读书，这个阶段为我打开了一扇很大很宽的门，让我走进了一个更大的世界。在这里接触到了visual basic，Pascal，机器人比赛，感受到了一个更加生动的世界，更加有趣的世界。原来学习可以不只是读书，不只是做题，还可以动手实现一个好玩的机器人，实现一个有趣的程序。三年的高中生活让我看到了一个更好玩的世界。没有辛苦地努力学习，只是因为高三参加的一次奥林匹克竞赛保送到了南方的一所985，因为一些不可控的原因，学习了一个一直想努力感兴趣却一直都没感兴趣的专业，四年除了一些竞赛成绩没有什么其他的亮点。大学的时候，抓住了一切业余时间来多写程序，也跟着刷一些学校自己online judge系统上的题目，但终究不是太系统。在大四下学期的时候，开始疯狂崇拜扎克伯格，尤其是看了《社交网络》之后，大概是因为当时自己的状态很低落，一方面考研成绩并不理想，一方面谈了三年的女朋友选择了更安稳的港湾，整个人的状态一落千丈，非常需要一些提气的东西来鼓励自己走出困境，于是用了一个开源的sns程序，php写的，在大学同学这个圈子里运营了一个社交网络，叫memory。那段时光，我开始自学一些php，每天给网站添加一些小的新功能，来满足同学们的种种好玩的需求，每天忙活到夜里一两点，却感觉到非常充实，非常有成就感。那段时光大概是大学四年里最快乐、最踏实的一段日子了。</p>
<p>后来，因为种种不可抗拒的原因没有在学校继续深造，去了一个自己也没法选择的单位工作。工作第一年在北京长期出差，有机会接触到了真正的IT圈，看到了真实的创业，真刀真枪，也看到了推荐系统从2011年开始在国内火起来，大大小小的网站都在搜罗推荐系统方面的人才，供不应求，关于推荐系统方面的交流活动也是一个接一个，媒体也在热炒，甚至都说推荐系统有望接管了搜索引擎的地位，现在想想真是可笑。而现在，人工智能处于一个非常火热的状态，有许多家专门报道人工智能相关资讯的媒体在社交网络上也非常活跃，还有大量的自媒体，每天也都在分享着各种各样和人工智能有关的信息。仿佛，真正的人工智能就快要实现了一样，尤其是阿尔法狗事件将人工智能推向了一个新的高度，人才市场上又开始吆喝着，严重缺乏人工智能、自然语言处理、深度学习、计算机视觉等方面的人才，媒体每天也在鼓吹着各个方面的论调，有的甚至在说AI威胁论，很多做其他研究的人也随之变成了人工智能专家，深度学习专家，自然语言处理专家。现在，这个世界上最不缺少的是专家，然后就是看热闹不嫌事大的吃瓜群众。网络上充斥各种形态的网红、大V，掌握着充分的话语权，许多不明真相的群众总是特别迷信他们说的话，形成了一种不健康的氛围。屌丝迷信小V，小V迷信大V，大V迷信大大V，一个人的title远比他的内容更能让人信服；如果学术中，总是这样迷信权威或者迷信title，学术该如何进步？长江后浪该如何推前浪？毕业之后的这些年，我看到了一个更加多元化的世界，一个兼容并包的世界，一个充满机会也充满挑战的世界。</p>
<p>我以为，一个人的胸怀和他的视野成正比，一个人的视野和他看到的世界有关系，一个人可以通过多旅行，到处看看来拓宽自己的视野，也可以通过多读书来丰富自己的内心世界。</p>
<p>我以为，人生短暂，不应浪费自己的时间在不感兴趣的事情上，时间宝贵，谁都不应该视别人的时间如粪土，大肆去浪费别人的时间。</p>
<p>我以为，生活的本质应该是生活本身，我们努力干活，努力拼搏，为的不就是生活中每一点一滴都过的很幸福嘛？荣誉也好、成就也罢终究是过眼云烟，敌不过内心持久的充实，充实并不是谁给你的，而是你自己对自己的一个肯定。</p>
<p>我以为，幸福就是不打扰到别人的快乐，可能是陪爱人一起看看大海，可能是一起拍拍照，可能是一起遛遛狗，也可能是一起发发呆。幸福也是将一个又一个的心愿完成，也是拥有一辆华丽的车子，尽管可能只是一辆淘宝购物车。</p>
<p>我以为，每天读一篇paper，写一篇博客，来丰富自己的内心和知识体系也是一件让我快乐和幸福的事情。每天可以有很多好玩的想法，并且可以通过自己的双手来实现这个想法也是一件让我快乐和幸福的事情。</p>
<p>我以为，编程让我感觉到自己像一个造物主，在程序世界里，每一行代码，每一个变量，每一个函数都是一个活生生的人，协同地一起工作着。</p>
<p>我以为，简单和纯粹才是这个世界上最真实的元素，纯真才是对人最大的夸奖，而不是什么帅气，漂亮，有才。</p>
<p>我以为，陪伴才是最长情的告白，感谢我家狗子hare童鞋每一个日日夜夜的陪伴，感谢我的爱人无条件地支持我做我喜欢做的事情，并且真的敢于放弃一些已有的、很好的待遇来跟着我去完成一个梦想。谢谢！</p>
<img src="/2016/05/29/我以为/1.jpg" width="600" height="600">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-28T00:51:24.000Z"><a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">2016-05-28</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/">Skip-Thought Vectors #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>已经分享过多种无监督的word level表示模型，和多种有监督的sentence level表示模型，以及与word2vec模型类似的paragraph vector模型。无监督模型比有监督模型带给大家更多的惊喜，本文将会分享一篇sentence level的无监督表示模型，模型中用到了当下流行的seq2seq框架。paper的题目是<a href="http://cn.arxiv.org/pdf/1506.06726v1.pdf" target="_blank" rel="external">Skip-Thought Vectors</a>，作者是来自多伦多大学的<a href="http://www.cs.toronto.edu/~rkiros/" target="_blank" rel="external">Ryan Kiros</a>博士。</p>
<p>word level表示已经有太多无监督模型，然而sentence level表示大多仍停留在监督模型的范畴内，比如之前分享过的RNN、CNN、RCNN等模型来表示一个句子，主要是针对具体的分类任务来构造句子向量，仅适用于本任务，不具有一般性。之前，Tomas Mikolov（word2vec的作者）提出了一种类似于Word2vec的paragraph vector，也是一种无监督模型，但并不能很好地扩展来用。</p>
<p>本文旨在提出一个通用的无监督句子表示模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。</p>
<p>当然，这里存在一个很大的问题是，如果测试数据中有未登录词，如何表示这个未登录词？针对这个问题，本文提出了一种词汇表扩展的方法来解决这个问题。</p>
<p>首先，介绍本文的模型，参考下图来理解：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig1.png" width="600" height="600">
<p>模型分为两个部分，一个是encoder，一个是两个decoder，分别decode出当前句子的上一句和下一句。</p>
<p>encoder-decoder框架已经介绍过太多次了，这里不再赘述。本文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hidden state作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。如下式：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig2.png" width="300" height="300">
<p>其次，介绍下本文的另一大亮点，词汇表扩展。</p>
<p>借鉴于Tomas Mikolov的一篇文章<a href="http://arxiv.org/pdf/1309.4168.pdf" target="_blank" rel="external">Exploiting Similarities among Languages for Machine Translation</a>中解决机器翻译missing words问题的思路，对本文训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，本文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。文中给出了一个例子，如下图：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig3.png" width="400" height="400">
<p>当然，词汇表扩展有很多方法，比如不同词，而用字符来作为基本元素，这种思路在语言模型中也常常被用到。</p>
<p>最后，作者在Semantic relateness、Paraphrase detection、Image-sentence ranking和classification任务中进行了测试和对比，验证了本文模型的效果。最后还给出了在多个数据集上对句子聚类的可视化结果，以及用decoder部分生成一段话。</p>
<p>关于未来的改进，作者有几点想法：</p>
<ul>
<li><p>用更深的encoder和decoder网络。</p>
</li>
<li><p>用更大的窗口，而不仅仅预测上一句和下一句。</p>
</li>
<li><p>试着将sentence替换成paragraph。</p>
</li>
<li><p>换一些别的encoder来做，比如用CNN。</p>
</li>
</ul>
<p>每个想法都可能会是未来另一篇牛paper的思路。</p>
<p><code>看过了很多的decoder，有char-level，word-level和sentence-level，我有一个小小的想法是，到底哪种level生成的paragraph更出色呢？速度方面，不必比较了，sentence-level一定要快一些，但是质量方面呢？</code>文中最后给出了一个本文模型生成的demo，如下：</p>
<img src="/2016/05/28/Skip-Thought-Vectors-PaperWeekly/fig4.png" width="600" height="600">
<p>本文作者还开源了该模型的实现<a href="https://github.com/ryankiros/skip-thoughts" target="_blank" rel="external">代码</a>。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-27T00:24:12.000Z"><a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">2016-05-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/">Recurrent Convolutional Neural Networks for Text Classification #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>介绍了CNN表示文本的模型之后，本篇将会分享一篇用CNN结合RNN的模型来表示文本。paper题目是<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="external">Recurrent Convolutional Neural Networks for Text Classification</a>，作者是来自中科院大学的来斯惟博士。</p>
<p>本文要解决的问题是文本分类，文本分类最关键的问题是特征表示，传统的方法经常会忽略上下文信息和词序，无法捕捉到词义。近几年随着深度学习的火热，研究者们通过借助神经网络模型来解决传统方法存在的问题。比如：Socher提出的Recursive Neural Network（递归神经网络）模型，通过一种树结构来捕捉句子语义，取得了不错的效果，但时间复杂度是O(n2)，并且无法用一棵树来表示两个句子之间的关系。再比如：Recurrent Neural Network（循环神经网络）模型，时间复杂度是O(n)，每个单词的表示都包含了之前所有单词的信息，有很强的捕捉上下文的能力，但该模型有偏，后面的单词比前面的单词更重要，但这与常识并不相符，因为句中关键的词不一定在最后面。为了解决RNN的有偏性问题，有的研究者提出了用CNN（卷积神经网络）来表示文本，并且时间复杂度也是O(n)，但是CNN存在一个缺陷，卷积窗口的大小是固定的，并且这个窗口大小如何设置是一个问题，如果设置小了，则会损失有效信息，如果设置大了，会增加很多的参数。</p>
<p>于是，针对上述模型存在的问题，本文提出了RCNN（循环卷积神经网络）模型，模型架构图如下：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png" width="600" height="600">
<p>首先，构造CNN的卷积层，卷积层的本质是一个BiRNN模型，通过正向和反向循环来构造一个单词的下文和上文，如下式：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig2.png" width="300" height="300">
<p>得到单词的上下文表示之后，用拼接的方式来表示这个单词，如下式：</p>
<img src="/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig3.png" width="300" height="300">
<p>将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层（max-pooling），即将刚刚得到的所有单词的潜语义向量中每个维度上最大的值选出组成一个新的向量，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。</p>
<p>为了验证模型的有效性，在四组包括中文、英文的分类任务中进行了对比实验，取得了满意的结果。</p>
<p>本文灵活地结合RNN和CNN构造了新的模型，利用了两种模型的优点，提升了文本分类的性能。这也提供了一种研究思路，因为每一种model都有其鲜明的优点和无法回避的缺点，如何利用别的model的优点来弥补自身model的缺点，是改进model的一种重要思路。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-25T23:17:24.000Z"><a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">2016-05-26</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/">How to Generate a Good Word Embedding #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>之前介绍过几种生成word embedding的方法，那么针对具体的任务该如何选择训练数据？如何选择采用哪个模型？如何选择模型参数？本篇将分享一篇paper来回答上述问题，paper的题目是<a href="http://cn.arxiv.org/pdf/1507.05523.pdf" target="_blank" rel="external">How to Generate a Good Word Embedding</a>，作者是来自中科院大学的来斯惟博士。</p>
<p>当前，word embedding的模型有很多，性能几乎都是各说纷纭，每个模型在自己选定的数据集和任务上都取得了state-of-the-art结果，导致学术研究和工程应用上难以做出选择。不仅仅在word embedding这个子方向上存在这样的问题，很多方向都有类似的问题，如何公平客观地评价不同的模型是一个很困难的任务。本文作者试着挑战了一下这个难题，并且给出了一些有意义的结果。</p>
<p>本文所做研究都是一个同一个假设，即：出现在相似上下文的单词具有相似的意思。</p>
<p>下面来看下不同模型的比较，不同word embedding模型之间主要的区别在于两点：</p>
<p>1、目标词和上下文的关系</p>
<p>2、上下文的表示方法</p>
<p>本文提供探讨了6种模型，并从这两个方面对模型进行了对比，如下图：</p>
<img src="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig1.png" width="500" height="500">
<p>c表示上下文，w表示目标词。首先看w和c的关系，前五种模型均是用c来预测w，只有C&amp;W模型是给w和c的组合来打分。再看c的表示方法，Order模型是本文为了对比增加的一个虚拟模型，考虑了词序信息，将c中每个单词拼接成一个大向量作为输入，而word2vec的两个模型skip-gram和cbow都是将上下文处理为一个相同维度的向量作为输入，其中skip-gram选择上下文中的一个词作为输入，cbow将上下文的几个词向量作了平均，LBL、NNLM和C&amp;W模型都是在Order模型的基础上加了一层隐藏层，将上下文向量做了一个语义组合。具体见下表：</p>
<img src="/2016/05/26/How-to-Generate-a-Good-Word-Embedding-PaperWeekly/fig2.png" width="500" height="500">
<p>据研究估计，文本含义信息的20%来自于词序，剩下的来自于词的选择。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了包括三种类型的八组对比实验，分别是：</p>
<ul>
<li><p>研究词向量的语义特性。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy task：semantic和syntactic。</p>
</li>
<li><p>将词向量作为特征。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。</p>
</li>
<li><p>用词向量来初始化神经网络模型。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford Sentiment Treebank；后者用Wall Street Journal数据集进行了POS tagging任务。</p>
</li>
</ul>
<p>经过大量的对比实验，作者回答了以下几个问题：</p>
<p>Q：哪个模型最好？如何选择c和w的关系以及c的表示方法？</p>
<p>A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：数据集的规模和所属领域对词向量的效果有哪些影响？</p>
<p>A：数据集的领域远比规模重要，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：在训练模型时迭代多少次可以有效地避免过拟合？</p>
<p>A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果，好的做法是用task data作为early stopping的数据。</p>
<p>Q：词向量的维度与效果之间的关系？</p>
<p>A：越大的维度就会有越好的效果，但在一般的任务中50就已经足够了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果，对今后研究和使用词向量的童鞋们搭建了一个非常坚实的平台。并且在github上开源了<a href="https://github.com/licstar/compare" target="_blank" rel="external">实验结果</a>。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-25T04:56:45.000Z"><a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">2016-05-25</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/">Convolutional Neural Networks for Sentence Classification #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇将分享一个有监督学习句子表示的方法，文章是<a href="http://cn.arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="external">Convolutional Neural Networks for Sentence Classification</a>，作者是Harvard NLP组的Yoon Kim，并且开源了代码 <a href="https://github.com/harvardnlp/sent-conv-torch" target="_blank" rel="external">sent-conv-torch</a>。</p>
<p>卷积神经网络（CNN）在计算机视觉中应用广泛，其捕捉局部feature的能力非常强，为分析和利用图像数据的研究者提供了极大额帮助。本文作者将CNN引用到了NLP的文本分类任务中。</p>
<p>本文模型架构图：</p>
<img src="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/fig1.png" width="600" height="600">
<p>熟悉CNN结构的童鞋们看这个图就会非常眼熟，单通道图像可以表示为一个矩阵，输入到CNN中，经过多组filter层和pooling层，得到图像的局部特征，然后进行相关任务。本文用拼接词向量的方法，将一个句子表示成为一个矩阵，这里矩阵的每一行表示一个word，后面的步骤仅采用一组filter、pooling层来得到句子的特征向量，然后进行分类。</p>
<p>这里，模型根据词向量的不同分为四种：</p>
<ul>
<li>CNN-rand，所有的词向量都随机初始化，并且作为模型参数进行训练。</li>
<li>CNN-static，即用word2vec预训练好的向量（Google News），在训练过程中不更新词向量，句中若有单词不在预训练好的词典中，则用随机数来代替。</li>
<li>CNN-non-static，根据不同的分类任务，进行相应的词向量预训练。</li>
<li>CNN-multichannel，两套词向量构造出的句子矩阵作为两个通道，在误差反向传播时，只更新一组词向量，保持另外一组不变。</li>
</ul>
<p>在七组数据集上进行了对比实验，证明了单层的CNN在文本分类任务中的有效性，同时也说明了用无监督学习来的词向量对于很多nlp任务都非常有意义。</p>
<p>这里需要注意的一点是，static模型中word2vec预训练出的词向量会把good和bad当做相似的词，在sentiment classification任务中将会导致错误的结果，而non-static模型因为用了当前task dataset作为训练数据，不会存在这样的问题。具体可参看下图：</p>
<img src="/2016/05/25/Convolutional-Neural-Networks-for-Sentence-Classification-PaperWeekly/fig2.png" width="600" height="600">
<p>CNN最初应用在图像领域，将文本进行一些处理之后，也可以应用在nlp中，同样的思路，attention mechanism最初也是应用在图像识别领域中，现在seq2seq+attention的模型横扫了很多nlp task。其实很多问题在某个维度上看，是相似的问题，是可以用类似的方法进行解决的。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-24T04:34:00.000Z"><a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">2016-05-24</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/">Distributed Representations of Sentences and Documents #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>继分享了一系列词向量相关的paper之后，今天分享一篇句子向量的文章，<a href="http://cn.arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="external">Distributed Representations of Sentences and Documents</a>，作者是来自Google的Quoc Le和Tomas Mikolov，后者也是Word2Vec的作者。</p>
<p>用低维向量表示了word之后，接下来要挑战地就是表示句子和段落了。传统的表示句子的方式是用词袋模型，每个句子都可以写成一个特别大维度的向量，绝大多数是0，不仅没有考虑词序的影响，而且还无法表达语义信息。本文沿用了Word2Vec的思想，提出了一种无监督模型，将变长的句子或段落表示成固定长度的向量。不仅在一定上下文范围内考虑了词序，而且非常好地表征了语义信息。</p>
<p>首先简单回顾下word2vec的cbow模型架构图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig1.png" width="600" height="600">
<p>给定上下文the cat sat三个词来预测单词on。</p>
<p>与cbow模型类似，本文提出了PV-DM（Distributed Memory Model of Paragraph Vectors），如下图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig2.png" width="600" height="600">
<p>不同的地方在于，输入中多了一个paragraph vector，可以看做是一个word vector，作用是用来记忆当前上下文所缺失的信息，或者说表征了该段落的主题。这里，所有的词向量在所有段落中都是共用的，而paragraph vector只在当前paragraph中做训练时才相同。后面的过程与word2vec无异。</p>
<p>topic也好，memory也罢，感觉更像是一种刻意的说辞，本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。</p>
<p>另外一种模型，叫做PV-DBOW（Distributed Bag of Words version of Paragraph Vector），如下图：</p>
<img src="/2016/05/24/Distributed-Representations-of-Sentences-and-Documents-PaperWeekly/fig3.png" width="600" height="600">
<p>看起来和word2vec的skip-gram模型很像。</p>
<p>用PV-DM训练出的向量有不错的效果，但在实验中采用了两种模型分别计算出的向量组合作为最终的paragraph vector，效果会更佳。在一些情感分类的问题上进行了测试，得到了不错的效果。</p>
<p>本文的意义在于提出了一个无监督的paragraph向量表示模型，无监督的意义非常重大。有了paragraph级别的高效表示模型之后，解决类似于句子分类，检索，问答系统，文本摘要等各种问题都会带来极大地帮助。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-22T23:10:32.000Z"><a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">2016-05-23</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/">Character-Aware Neural Language Models #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>本篇分享的文章是<a href="http://cn.arxiv.org/pdf/1508.06615.pdf" target="_blank" rel="external">Character-Aware Neural Language Models</a>，作者是Yoon Kim、Alexander M. Rush。两位是HarvardNLP组的学生和老师，前者贡献了一些有意义的torch代码，比如<a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">seq2seq+attn</a>，后者第一次将seq2seq的模型应用到了文本摘要。</p>
<p>卷积神经网络之前常常用在计算机视觉领域，用来在图像中寻找features，前几年被研究者应用到了nlp任务中，在文本分类等任务中取得了不错的效果。传统的word embedding对低频词并没有太好的效果，而本文将char embedding作为CNN的输入，用CNN的输出经过一层highway层处理表示word embedding，然后作为RNNLM的输入，避免了这个问题。而且之前的神经网络语言模型中绝大多数需要优化的参数是word embedding，而本文的模型则会将优化参数减少非常多。</p>
<p>本文模型的架构图如下：</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/arch.png" width="600" height="600">
<p>可以分为三层，一层是charCNN，通过构建一个char embedding矩阵，将word表示成matrix，和图像类似，输入到CNN模型中提取经过filter层和max pooling层得到一个输出表示，然后将该输出放到Highway Network中，得到一个处理后的效果更好的word embedding作为输出，在第三层中是一个典型的RNN模型，后面的处理与传统方法一样了。</p>
<p>这里需要学习的参数中char embedding规模非常小，相对比之前的模型有非常明显的优势。这里需要说明的一点是HighWay Network，在Rupesh Kumar Srivastava的paper <a href="http://cn.arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="external">Training Very Deep Networks</a>被提出，受lstm解决rnn梯度衰减问题的思路启发，用来解决训练very deep networks，因为模型越深效果越好，但越难训练。本文的HighWay层如下：</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/fig1.png" width="400" height="400">
<p>其中</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/fig2.png" width="200" height="200">
<p>t被称为transform gate，1-t被称为carry gate。</p>
<p>最终的实验证明，使用HighWay层效果比使用普通的MLP或者不使用该层效果更好。</p>
<img src="/2016/05/23/Character-Aware-Neural-Language-Models-PaperWeekly/result.png" width="500" height="500">
<p>本文通过将传统的word embedding降级到char level，避免了大规模的embedding计算和低频词的问题，通过Highway network技术构建更深的网络，得到了不错的结果。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-22T01:23:52.000Z"><a href="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">2016-05-22</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/">GloVe: Global Vectors for Word Representation #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>Word2Vec虽然取得了很好的效果，但模型上仍然存在明显的缺陷，比如没有考虑词序，再比如没有考虑全局的统计信息。本篇分享的是<a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="external">GloVe: Global Vectors for Word Representation</a>，作者是stanford的Jeffrey Pennington, Richard Socher(metamind CEO)和Christopher Manning。同时作者还开源了相应的工具GloVe和一些训练好的模型。</p>
<p>本文的思路是将全局词-词共现矩阵进行了分解，训练得到词向量。整体上的思路和推荐系统当年横扫Netflix百万美元比赛的LFM模型类似，也和信息检索中LSI的思路类似。不同的地方是，本文采用的词-词共现矩阵比起词-文档矩阵更加稠密，模型中对低频词和高频词的影响做了一定地弱化处理。</p>
<p>首先，构建词-词共现矩阵，共现是建立在一个固定窗口范围内，给定范围之后，可以得到一个V*V的矩阵，这里V是词汇表大小。（虽然矩阵的稠密程度比词-文档矩阵好一些，但大多数也都是0）</p>
<p>然后，本文的模型如下：</p>
<img src="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/fig1.png" width="300" height="300">
<p>通过使该目标函数最小来得到最终的词向量，在计算误差时只考虑共现矩阵中非0的项。因为不同频次的词对目标的贡献不同，所以设定了一个权重函数f(x)，具有以下特点：</p>
<p>1、f(0) = 0</p>
<p>2、f(x)是增函数，这样低频词不会被over weight。</p>
<p>3、当x很大时，f(x)相对小一些，这样高频词也不会被over weight。</p>
<p>根据以上特性，选择下面的函数来作为f(x)：</p>
<img src="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/fig2.png" width="400" height="400">
<img src="/2016/05/22/GloVe-Global-Vectors-for-Word-Representation-PaperWeekly/fig3.png" width="500" height="500">
<p>本文的模型在Word Analogy Task（Tomas Mikolov提出的测试集）中获得了75%的正确率，击败了Word2Vec。</p>
<p>虽然paper中GloVe有着指标上的领先，但在实际使用中Word2Vec的使用率相对来说更多一些，可能的原因是Word2Vec可以更快地提供一个相对来说不错的word embedding层的初始值。从中得到的启发是，指标上的胜利有些时候只是paper上的胜利，不一定能代表在工程中也是赢家，而只有更加好的model被提出，才会真正地既赢得指标上的胜利，也赢得工程上的胜利。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-21T06:43:54.000Z"><a href="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">2016-05-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/">Efficient Estimation of Word Representations in Vector Space #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<p>词向量是语言模型的一个副产品，但这个副产品在2013年随着一个叫做word2vec的工具包而火了起来，大家在各种场合中都在用，并且取得了不错的效果。</p>
<p>本篇分享的文章是<a href="http://cn.arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a>。作者是来自Google的Tomas Mikolov，也是Word2Vec和RNNLM开源软件的作者。本文的最大贡献有：</p>
<p>1、提出了两种新的“神经网络语言模型”，这里之所以打引号，是因为其实两个模型都没有隐藏层，只是看起来像是神经网络而已。两种模型具有很高的计算效率和准确率，可谓是真正的“又好又快”。</p>
<p>2、设计了一种验证词向量效果的测试数据，从semantic和syntactic两个维度上进行测试。</p>
<p>首先介绍下传统模型的复杂度。</p>
<ul>
<li>NNLM的模型复杂度：</li>
</ul>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f1.png" width="300" height="300">
<p>这里，N是输入的单词数量，D是词向量维度，H是隐藏层维度，V是词汇表维度。</p>
<ul>
<li>RNNLM的模型复杂度：</li>
</ul>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f3.png" width="200" height="200">
<p>两个模型的输出层都可以用<code>hierarchical softmax</code>来替换，V的复杂度可以降为log2(V)</p>
<p>NNLM模型的计算瓶颈在于N <em> D </em> H，而RNNLM的计算瓶颈在与H * H。</p>
<p>本文提出的两种模型架构图如下：</p>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/arch.png" width="600" height="600">
<p>从架构图中可以看出本文的模型并没有隐藏层，直接由输入层做一次映射，就进行分类。</p>
<p>左图是CBOW模型,输入是指定单词的context单词（前后各取几个单词），预测的是该单词。模型复杂度如下：</p>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f5.png" width="200" height="200">
<p>右图是Skip-gram（SG）模型，输入时某个单词，预测的是它的context。模型复杂度如下：</p>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f5.png" width="200" height="200">
<p>这里的C表示单词距离上限，用来限制预测context的窗口大小。</p>
<p>本文的模型和NNLM和RNNLM有着不同的使命，前者更加专注于解决词向量的问题，在保证准确率的情况下，尽量地提高计算效率；而后者更加专注于解决语言模型的问题，词向量只是其副产品，因此并没有过多地在这部分进行优化。最后的实验结果表明，sg和cbow模型在semantic和syntactic两个维度上进行相似度测试时表现远好于nnlm和rnnlm。并且，sg在semantic上表现更好，cbow更擅长做syntactic。</p>
<p>将word映射成某个空间内的向量之后，我们可以轻松地通过cos similarity来计算word之间的相似度，并且可以做一些简单的加减运算。</p>
<p>Paris - France + Italy = Rome</p>
<p>Small - Smaller + Large = Larger</p>
<p>可以将word映射到vector space中，那么是否可以将phrase，sentence，paragraph，document都映射到vector space中呢？进一步是否可以将topic也映射到vector space呢？是否任何东西都可以映射到vector space呢？</p>
<p>丰富的想象力给了人类更大的动力去探索未知的世界，将word2vec的想法拓展到各个level的问题上。在后续的很多nlp研究中，词向量都起到了关键的作用。不仅如此，word2vec在其他领域中也有了广泛的应用，比如：app推荐，将每个user下载过的app作为word，就可以得到给user推荐类似的app（相似的word）；我在做rsarxiv时，构建了一个简单的paper graph，将authors，subjects，keywords都映射到了同一个空间中，给定一个author，很容易找到与之相关的authors，subjects，keywords。比如喜欢了这个作者，</p>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f7.png" width="300" height="300">
<p>之后就会得到推荐：</p>
<img src="/2016/05/21/Efficient-Estimation-of-Word-Representations-in-Vector-Space-PaperWeekly/f8.png" width="600" height="600">
<p>Word2Vec一个很重要的意义在于，是无监督方法，不需要花额外的功夫去构建数据集来teach模型，只需要给入一个非常大的文本数据集，就可以得到非常好的效果。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-20T13:37:06.000Z"><a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">2016-05-20</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/">A Neural Probabilistic Language Model #PaperWeekly#</a></h1>
  

    </header>
    <div class="entry">
      
        <p>(欢迎大家订阅本博客，订阅地址是<a href="http://rsarxiv.github.io/atom.xml">RSS</a>)</p>
<blockquote>
<p><strong> 站得高，望得远 </strong></p>
</blockquote>
<p>今天分享一篇年代久远但却意义重大的paper，<a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a>。作者是来自蒙特利尔大学的Yoshua Bengio教授，deep learning技术奠基人之一。</p>
<p>本文于2003年第一次用神经网络来解决语言模型的问题，虽然在当时并没有得到太多的重视，却为后来深度学习在解决语言模型问题甚至很多别的nlp问题时奠定了坚实的基础，后人站在Yoshua Bengio的肩膀上，做出了更多的成就。包括Word2Vec的作者Tomas Mikolov在NNLM的基础上提出了RNNLM和后来的Word2Vec。文中也较早地提出将word表示一个低秩的向量，而不是one-hot。word embedding作为一个language model的副产品，在后面的研究中起到了关键作用，为研究者提供了更加宽广的思路。</p>
<p>本文最大的贡献在于用多层感知器（<code>MLP</code>）构造了语言模型，如下图：</p>
<img src="/2016/05/20/A-Neural-Probabilistic-Language-Model-PaperWeekly/arch.png" width="600" height="600">
<p>模型一共三层，第一层是映射层，将n个单词映射为对应word embeddings的拼接，其实这一层就是MLP的输入层；第二层是隐藏层，激活函数用tanh；第三层是输出层，因为是语言模型，需要根据前n个单词预测下一个单词，所以是一个多分类器，用softmax。整个模型最大的计算量集中在最后一层上，因为一般来说词汇表都很大，需要计算每个单词的条件概率，是整个模型的计算瓶颈。</p>
<p>这里，需要注意的是需要提前初始化一个word embedding矩阵，每一行表示一个单词的向量。词向量也是训练参数，在每次训练中进行更新。这里可以看出词向量是语言模型的一个附属品，因为语言模型本身的工作是为了估计给定的一句话有多像人类的话，但从后来的研究发现，语言模型成了一个非常好的工具。</p>
<p>softmax是一个非常低效的处理方式，需要先计算每个单词的概率，并且还要计算指数，指数在计算机中都是用级数来近似的，计算复杂度很高，最后再做归一化处理。此后很多研究都针对这个问题进行了优化，比如层级softmax，比如softmax tree。</p>
<p>当然NNLM的效果在现在看来并不算什么，但对于后面的相关研究具有非常重要的意义。文中的Future Work提到了用RNN来代替MLP作为模型可能会取得更好的效果，在后面Tomas Mikolov的博士论文中得到了验证，也就是后来的RNNLM。</p>
<p>所以说我们赶上了一个好的时代，可以站在巨人的肩膀上，看到更远的未来。</p>
<p><b>工具推荐</b></p>
<p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="350" height="350">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-18T09:56:53.000Z"><a href="/2016/05/18/自动文摘（十三）/">2016-05-18</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/18/自动文摘（十三）/">自动文摘（十三）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 天下武功，唯快不破 </strong></p>
</blockquote>
<p>今天分享的paper是<b>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</b>，作者来自香港大学和华为诺亚方舟实验室。</p>
<p><code>本文的模型通过借鉴人类在处理难理解的文字时采用的死记硬背的方法，提出了COPYNET。将拷贝模式融入到了Seq2Seq模型中，将传统的生成模式和拷贝模式混合起来构建了新的模型，非常好地解决了OOV问题。解决问题的思路与之前的一篇有关Pointer的文章十分类似。decoder部分不断地变复杂，考虑的因素越来越多，模型的效果也越来越好。如果结合上一篇Minimum Risk Training的训练方法，相信在评价指标上会更进一步。</code></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>Seq2Seq技术占据了nlp多个研究任务的评测榜首，包括最早提出该技术的机器翻译，句法分析，文本摘要，对话系统。Seq2Seq本质上是一个encoder-decoder的模型，encoder部分将输入的序列变换成某一种表示，然后decoder将这种表示变换成输出序列。在Seq2Seq的基础上，首次增加注意力机制来做机器翻译的自动对齐。注意力机制在很大程度上提升了Seq2Seq的性能。</p>
<p>本文研究了人类语言交流的另一个机制，“拷贝机制”（<code>copy mechanism</code>），定位到输入序列中的某个片段，然后将该片段拷贝到输出序列中。比如：</p>
<img src="/2016/05/18/自动文摘（十三）/fig1.png" width="400" height="400">
<p>但是注意力机制严重依赖于语义的表示，在系统需要获取到命名实体或者日期时难以准确地表示。相对之下，拷贝机制更加接近于人类处理语言问题的方式。本文提出了COPYNET系统，不仅具备传统Seq2Seq生成词的能力，而且可以从输入序列中拷贝合适的片段到输出序列中。在合成数据和真实数据中均取得了不错的结果。</p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>文章的这部分简单介绍了一下Seq2Seq+Attention Mechanism技术，前面的博客分享了很多这部分的内容，这里就不再赘述了。</p>
<h1 id="COPYNET"><a href="#COPYNET" class="headerlink" title="COPYNET"></a>COPYNET</h1><p>从神经学角度来讲，拷贝机制和人类的死记硬背类似，较少地理解到了意思但保留了字面的完整。从模型的角度来讲，拷贝机制相比于soft注意力模型更加死板，所以更难整合到神经网络模型中。</p>
<h2 id="模型综述"><a href="#模型综述" class="headerlink" title="模型综述"></a>模型综述</h2><p>COPYNET依然是一个encoder-decoder模型，如图1所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig2.png" width="600" height="600">
<p>encoder采用了一个双向RNN模型，输出一个隐藏层表示的矩阵M作为decoder的输入。decoder部分与传统的Seq2Seq不同之处在于以下三部分：</p>
<ul>
<li><b>预测</b>：在生成词时存在两种模式，一种是生成模式，一种是拷贝模式，生成模型是一个结合两种模式的概率模型。</li>
<li><b>状态更新</b>：用t-1时刻的预测出的词来更新t时刻的状态，COPYNET不仅仅词向量，而且使用M矩阵中特定位置的hidden state。</li>
<li><b>读取M</b>：COPYNET也会选择性地读取M矩阵，来获取混合了内容和位置的信息。</li>
</ul>
<h2 id="拷贝模式和生成模式"><a href="#拷贝模式和生成模式" class="headerlink" title="拷贝模式和生成模式"></a>拷贝模式和生成模式</h2><p>首先，构造了两个词汇表，一个是高频词词汇表，另一个是只在输入序列中出现过一次的词，这部分的词用来支持COPYNET，用UNK表示超纲词（OOV），最终输入序列的词汇表是三者的并集。</p>
<p>给定了decoder当前状态和M矩阵，生成目标单词的概率模型如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig3.png" width="300" height="300">
<p>模型中g表示生成模式，c表示拷贝模式。两种模式的概率由下式给定：</p>
<img src="/2016/05/18/自动文摘（十三）/fig4.png" width="300" height="300">
<p>共四种可能情况，下图会更形象一些：</p>
<img src="/2016/05/18/自动文摘（十三）/fig5.png" width="300" height="300">
<p>其中生成模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig6.png" width="300" height="300">
<p>拷贝模式的打分公式是：</p>
<img src="/2016/05/18/自动文摘（十三）/fig7.png" width="300" height="300">
<h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><p>decoder状态更新的公式是</p>
<img src="/2016/05/18/自动文摘（十三）/fig8.png" width="300" height="300">
<p>不同的是这里的t-1时刻的y由下式表示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig9.png" width="300" height="300">
<p>后面的部分是M矩阵中与t时刻y相关的状态权重之和，如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig10.png" width="300" height="300">
<h2 id="M矩阵"><a href="#M矩阵" class="headerlink" title="M矩阵"></a>M矩阵</h2><p>M矩阵中既包含了内容（语义）信息，又包含了位置信息。COPYNET在attentive read时由内容（语义）信息和语言模型来驱动，即生成模式；在拷贝模式时，由位置信息来控制。</p>
<p>位置信息的更新方式如下图所示：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="300" height="300">
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>一共分为三个实验：</p>
<ul>
<li>简单规则构造的合成数据。</li>
<li>文本摘要相关的真实数据。</li>
<li>简单对话系统的数据。</li>
</ul>
<p>这里只看文本摘要实验。</p>
<h2 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h2><p>数据采用LCSTS中文短文本摘要数据集，分为两个level来测试：word-level和char-level，并且以LCSTS的baseline作为对比，结果如下：</p>
<img src="/2016/05/18/自动文摘（十三）/fig11.png" width="400" height="400">
<p>本文的模型远远优于baseline，而且word-level的结果比char-level更好，这与当时LCSTS paper中的结论不同，一个可能的原因是，数据集中包含了大量的命名实体名词（entity name），LCSTS paper中的方法并不能很好地处理大量的UNK单词，因此baseline中的char-level效果比word-level更好，而本文的模型的优势在于处理OOV问题，所以word-level结果更好一些。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1603.06393v2.pdf" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning Training</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p><code>PaperWeekly</code>，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-17T08:24:26.000Z"><a href="/2016/05/17/自动文摘（十二）/">2016-05-17</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/17/自动文摘（十二）/">自动文摘（十二）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 心是自由的，世界就是自由的 </strong></p>
</blockquote>
<p>今天分享的paper是<b>Neural Headline Generation with Minimum Risk Training</b>。</p>
<p><code>本文通过将评价指标融入目标函数来训练模型，在中文和英文数据集上均取得了超过之前所有模型的结果。结果一点也不意外，因为传统的MLE并不是以ROUGE评价指标最大为目标函数，而本文的方法针对了评价指标来做文章，一定会得到不错的结果。反过来，我们需要思考一个问题，如果文本摘要领域中出现了一个更加科学和准确的评价指标，不仅仅简单的比共现n-gram，那么本文的模型会不会得到一个优于其他模型的结果呢？个人觉得本文的方法很好地利用了评价指标，但对于研究摘要问题的本质并无太多的帮助，只是获得了更好的指标。有一点投其所好的感觉。</code></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>本文研究<code>NHG</code>(Neural Headline Generation)模型。</p>
<p>用Neural的思路来解决HG问题有以下优势：</p>
<p>1、完全数据驱动，不依赖与人工标注和语言学特征。</p>
<p>2、完全端到端，引入注意力机制会得到更好的效果。</p>
<p>存在以下弊端：</p>
<p>1、当前的优化方法都是用最大似然估计（<code>MLE</code>）来训练数据，没有将评价指标考虑在内。</p>
<p>本文用Minimum Risk Training(<code>MRT</code>)来改善NHG模型，将评价指标考虑在优化目标内，在中文和英文两个真实数据集上取得了不错的结果。</p>
<h1 id="NHG模型"><a href="#NHG模型" class="headerlink" title="NHG模型"></a>NHG模型</h1><img src="/2016/05/17/自动文摘（十二）/fig1.png" width="400" height="400">
<p>模型采用encoder-decoder框架，encoder和decoder都采用rnn作为模型。</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder包括两种：<code>GRU</code>和<code>Bi-RNN</code>。</p>
<p>Bi-RNN克服了传统RNN的语义偏置最后一个词的缺点。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder采用GRU，在生成结果时引入了注意力模型。</p>
<h1 id="MRT-NHG"><a href="#MRT-NHG" class="headerlink" title="MRT+NHG"></a>MRT+NHG</h1><h2 id="MLE"><a href="#MLE" class="headerlink" title="MLE"></a>MLE</h2><p>传统的训练方法都是采用最大似然估计来做，目标函数如下：</p>
<img src="/2016/05/17/自动文摘（十二）/fig2.png" width="300" height="300">
<h2 id="MRT"><a href="#MRT" class="headerlink" title="MRT"></a>MRT</h2><p>本文采用了最小风险训练方法来训练模型，目的是减少期望的损失。目标函数如下：</p>
<img src="/2016/05/17/自动文摘（十二）/fig3.png" width="300" height="300">
<p>进一步可以推出：</p>
<img src="/2016/05/17/自动文摘（十二）/fig4.png" width="300" height="300">
<p>作进一步近似处理：</p>
<img src="/2016/05/17/自动文摘（十二）/fig5.png" width="300" height="300">
<p>公式中的<img src="/2016/05/17/自动文摘（十二）/fig6.png" width="100" height="100">用来计算误差，这样训练处的模型将会将评价指标考虑在内。ROUGE是最常见的评价方法，所以本文考虑将ROUGE评价方法融入到目标函数中。</p>
<h2 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h2><p>本文考虑两种ROUGE指标，ROUGE-N和ROUGE-L。本文为了将ROUGE评价指标融入到目标函数中，定义了</p>
<img src="/2016/05/17/自动文摘（十二）/fig7.png" width="200" height="300">
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>DUC2004评测对比结果：</p>
<img src="/2016/05/17/自动文摘（十二）/t1.png" width="400" height="400">
<p>英文数据集上，本文模型的结果明显优于其他模型，包括之前的ABS+模型。</p>
<p>中文LCSTS数据上平尺对比结果：</p>
<img src="/2016/05/17/自动文摘（十二）/t2.png" width="400" height="400">
<p>采用MRT目标函数的模型远优于MLE作为目标函数的模型。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1604.01904.pdf" target="_blank" rel="external">Neural Headline Generation with Minimum Risk Training</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p><code>PaperWeekly</code>，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-13T14:12:01.000Z"><a href="/2016/05/13/Paper翻译列表/">2016-05-13</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/13/Paper翻译列表/">Paper翻译列表</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 读万卷书，行万里路 </strong></p>
</blockquote>
<p>最近精读了多篇自动文摘方面的paper，并且写成了博客，感觉受益良多。接下来会读更多的好paper，并且以一种更加精炼的形式摘译和评价。如果你有兴趣可以扫码关注微信账号：</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">
<h1 id="Accomplised"><a href="#Accomplised" class="headerlink" title="Accomplised"></a>Accomplised</h1><ul>
<li><b>Generating News Headlines with Recurrent Neural Networks</b></li>
<li><b>A Neural Attention Model for Abstractive Sentence Summarization </b></li>
<li><b>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</b></li>
<li><b>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</b></li>
<li><b>AttSum-Joint Learning of Focusing and Summarization with Neural Attention</b></li>
<li><b>LCSTS: A Large Scale Chinese Short Text Summarization Dataset</b></li>
</ul>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ul>
<li><b>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</b></li>
<li><b>Neural Headline Generation with Minimum Risk Training</b></li>
<li><b>Toward Abstractive Summarization Using Semantic Representations</b></li>
<li><b>Better Summarization Evaluation with Word Embeddings for ROUGE</b></li>
</ul>
<h1 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h1><p>在完成这文章的翻译和评价之后，会继续读nlp各顶级会议每年的best paper，一方面拓宽自己的视野，另一方面分享给同样感兴趣的童鞋。如果你也有兴趣做这件事情，可以发邮件联系我。（rsarxiv@163.com）</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-12T07:38:09.000Z"><a href="/2016/05/12/自动文摘（十一）/">2016-05-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/12/自动文摘（十一）/">自动文摘（十一）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 除了诗和远方，还有眼前的评价方法 </strong></p>
</blockquote>
<p>之前的博客中提到过影响一个技术发展的重要因素有几个，其中一个就是评价方法。因为评价方法是牵引，是参考，是衡量各个模型孰优孰劣的尺子。评价指标是否科学可行直接影响着这个领域能否进入一个正常的研究方向，目前在文本摘要任务中最常用的评价方法是<code>ROUGE</code>（<b>Recall-Oriented Understudy for Gisting Evaluation</b>，最细节的内容可参见Lin,2003的paper <b>Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</b>）。</p>
<p>10多年前提出的方法至今都是自动文摘系统评价的主流方法，不是说它没有缺点，而是至今没有提出一个更加好的评价方法来取代它。</p>
<p>ROUGE收到了机器翻译自动评价方法BLEU的启发，不同之处在于，采用召回率来作为指标。基本思想是将模型生成的摘要与参考摘要的n元组贡献统计量作为评判依据。这里，不同具体任务的参考文献获取方法不同，比如：DUC2003文本摘要中的参考摘要是由人工给出的，而seq2seq模型在处理Gigaword数据集时，用news headline作为摘要，中文微博短文本摘要数据集（哈工大）的参考摘要都是包含在微博内容中。</p>
<h1 id="ROUGE评价指标"><a href="#ROUGE评价指标" class="headerlink" title="ROUGE评价指标"></a>ROUGE评价指标</h1><h2 id="ROUGE-N"><a href="#ROUGE-N" class="headerlink" title="ROUGE-N"></a>ROUGE-N</h2><p>这个指标计算生成的摘要与相应的参考摘要的n-gram召回率。</p>
<h2 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h2><p>这个指标匹配两个文本单元之间的最长公共序列（LCS，Longest Common Subsequence）。</p>
<h2 id="ROUGE-W"><a href="#ROUGE-W" class="headerlink" title="ROUGE-W"></a>ROUGE-W</h2><p>这个指标计算加权的LCS。</p>
<h2 id="ROUGE-S"><a href="#ROUGE-S" class="headerlink" title="ROUGE-S"></a>ROUGE-S</h2><p>计算跳二元组（skip-bigram）同现统计量。</p>
<p>ROUGE自动评测方法最大的优点是不依赖语言处理工具，缺点是死板，不够灵活，没有考虑语义层次上的匹配。可以考虑用word embedding来做语义层次上的评判，而不仅仅是n-gram的匹配。</p>
<h1 id="ROUGE使用方法"><a href="#ROUGE使用方法" class="headerlink" title="ROUGE使用方法"></a>ROUGE使用方法</h1><p>在实际使用时，用ROUGE的开源程序，perl写的脚本，根据提示操作即可。通常用ROUGE-1,ROUGE-2指标来评测文摘的效果。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://research.microsoft.com/en-us/people/cyl/naacl2003.pdf" target="_blank" rel="external">Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</a></p>
<p>[2] <a href="http://www.berouge.com/Pages/default.aspx" target="_blank" rel="external">ROUGE开源工具包</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-12T02:20:59.000Z"><a href="/2016/05/12/自动文摘（十）/">2016-05-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/12/自动文摘（十）/">自动文摘（十）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong> 生活不只是眼前的苟且，还有paper和远方 </strong></p>
</blockquote>
<p>不知不觉坚持写自动文摘系列的博客已经50天了，本篇是系列的第十篇。其实说是系列文章并不准确，只是每篇博客与自动文摘有关系，但相互之间并没有递进的关系，只是get到了一些点顺手写下来，又懒得起一些好听的名字，所以就简单地命名为系列博客。我不知道这个系列可以写到几，但探索自动文摘技术并不会停止下来。言归正传，最近读了些paper，觉得UNK问题是一个值得关注的问题，所以本文简单讨论一下UNK问题。</p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>UNK是Unknown Words的简称，在用seq2seq解决问题上经常出现，比如机器翻译任务，比如文本摘要任务。在decoder中生成某个单词的时候就会出现UNK问题。decoder本质上是一个语言模型，而语言模型本质上是一个多分类器，通过计算词汇表中的每个单词在当前条件下出现的概率，来生成该条件下的单词。为了提高计算效率，往往只选择出现频次最高的Top N个单词作为词汇表，其他的单词都用UNK来替换，这样导致decoder的时候会出现UNK。其中，很多UNK可能都是一些不常出现的但有意义的词，比如机构名、地名。</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>解决方法有以下几个：</p>
<h2 id="Char-based"><a href="#Char-based" class="headerlink" title="Char-based"></a>Char-based</h2><p>英文字符的种类远远少于词的种类，用char来代替word可以将decoder输出层的维度降低很多，而且覆盖了所有的可能，从根本上避免了UNK的问题。在文本摘要任务中，数据中往往包括很多的人名、地名，基于word来做词汇表的话，经常会在摘要中看到大量的UNK，用基于char的模型来做，会得到不错的效果。</p>
<p>char方法虽然缓解了output部分的计算压力，却将大量计算耗在了input部分，尤其是在处理英文问题时，会将input放大很多倍。而且处理中文问题也不太有优势，常用汉字也有3000左右的规模。</p>
<h2 id="Vocabulary-expansion"><a href="#Vocabulary-expansion" class="headerlink" title="Vocabulary expansion"></a>Vocabulary expansion</h2><p>词汇表扩展的方法，在高频词词汇表中添加一定数量的UNK，并且编号。通过word embedding计算出带编号UNK的第一层最邻近words，如果匹配的这些词在原来词汇表中，则为有效词。有效词越多，本质上词汇表虽然规模没有增加，但表达能力会越强，在decoder生成词时遇到UNK就可以用词汇表中的高频词来替换。</p>
<p>这个方法是一个辅助技巧，可以提升效果，但不会解决根本问题。</p>
<h2 id="Output-layer-boost"><a href="#Output-layer-boost" class="headerlink" title="Output layer boost"></a>Output layer boost</h2><p>这个方法的思路是想办法提升输出层的效率，原始的方法是softmax，这是消耗计算资源的根源。有比如</p>
<p>1、importance sampling : (Bengio et al., 2003)</p>
<p>2、uniform sampling of ranking criterion : (Collobert et al., 2011)</p>
<p>3、hierarchical softmax : (Morin et al., 2005) </p>
<p>4、hierarchical log-bilinear model : (Mnih et al., 2009) </p>
<p>5、structured output layer : (Le et al., 2011)</p>
<p>6、noise-constrastive estimation : (Mnih et al., 2012)</p>
<p>各种各样的方法来提升多分类问题的效率。效率高了，词汇表中就可以放入更多的单词，但治标不治本，只能说改善了效果。</p>
<h2 id="Pointing-Copy"><a href="#Pointing-Copy" class="headerlink" title="Pointing/Copy"></a>Pointing/Copy</h2><p>观察人工参考摘要时会发现，摘要中有很多词都是来自于输入部分，比如机构名、地名、人名。这些词出现很少有的甚至只出现一次，如果靠语言模型来生成是不可能的。基于这个现象，有几篇paper提出用Pointing/Copy机制来生成摘要，两种模型意思上茶太不多，在decoder中存在两种模型来生成单词，一种是常规的生成方式，另一种就是拷贝方式。拷贝模型在很大程度上解决了UNK的问题，rare words都直接用原文中的词放在摘要的相应位置上。</p>
<p>本方法从正面解决了UNK问题，而且计算效率上可能比char-based的方法更好一些，因为并没有引入太大规模的input数据，output部分规模也不大。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-10T23:14:52.000Z"><a href="/2016/05/11/自动文摘（九）/">2016-05-11</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/11/自动文摘（九）/">自动文摘（九）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>坚持下去就是胜利。</strong></p>
</blockquote>
<p>今天分享一篇关于构造自动文摘数据集的paper，数据集的质量、内容和规模都是直接影响deep learning效果的最直接因素，作用非常重要。题目是<b>LCSTS: A Large Scale Chinese Short Text Summarization Dataset</b>。</p>
<p><code>本文最大的贡献在于构建了一个大规模、高质量中文短文本摘要数据集，弥补了这个空缺。并且在数据集的基础上用了最简单seq2seq给出了一个baseline，为后人的研究提供了基础。从本文的模型中可以看出，unk问题在文本摘要任务中的重要性，如何解决unk问题是提升摘要系统性能的一个重要方向。本文给出了一个思路，用character-based model来绕过这个问题，看过的paper中还有其他的解决思路，后面的博客将会专门介绍unk这个问题。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>自动文本摘要是一个非常难的问题，部分原因是因为缺乏大规模的高质量数据集。本文将会介绍一个取自于新浪微博的大规模中文短文本摘要数据集，数据集中包含了200万真实的中文短文本数据和每个文本作者给出的摘要。同时我们也手动标注了10666份文本的摘要。基于本数据集，我们测试了用RNN来生成摘要得到了不错的效果，不仅仅亚验证了数据集的有效性，而且为今后的研究提供了基准。</p>
<p><code>数据集一直都是困扰deep learning技术更好地应用在各大领域的一大瓶颈，尤其是中文数据集的匮乏，本文工作的意义在于给研究中文自动文摘的学者提供了数据支持。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><img src="/2016/05/11/自动文摘（九）/weibo.png" width="500" height="500">
<p>本文中的数据类似于上图中的形式。</p>
<p>传统研究abstractive summarization的方法将摘要过程分类两步，第一步是使用无监督方法或者语言知识将关键文本提取出来；第二步是用语言规则或者文本生成技术将第一步的结果转述（<code>paraphrase</code>）。近期研究表明深度学习技术有很强地表示学习能力和语言生成能力，尤其是用GPU在大规模数据集上进行计算，许多学者将该技术应用在abstractive summarization任务中。</p>
<p>然而，公开的高质量大规模文本摘要数据集还是少之又少，DUC、TAC、TREC的数据仅仅包括几百条英文人工摘要数据，这种情况在中文环境中更加糟糕。所以本文构建了一个大规模中文短文本摘要数据集。</p>
<p>以下是本文的贡献：</p>
<p>1、构建了目前最大的一个中文短文本摘要数据集。</p>
<p>2、提供了数据集分割的标准方法。</p>
<p>3、研究了数据集的特性，采样10666条样本，并进行数据集的质量评价。</p>
<p>4、利用了基于encoder-decoder rnn技术来生成摘要，作为该任务的基准。</p>
<p><code>大规模数据集的构建往往来自于网络爬虫，获取到了大量的raw data之后，如何处理得到高质量的内容是关键。本文通过抽样选出样本对数据集质量进行评价，并且用当下最流行的seq2seq技术给出了本数据集的benchmark，以供后人研究超越。</code></p>
<h1 id="Data-Collection"><a href="#Data-Collection" class="headerlink" title="Data Collection"></a>Data Collection</h1><p>为了保证质量，我们仅抓取通过认证组织的微博，这些微博更加清楚、规范和有信息。流程具体如下图：</p>
<img src="/2016/05/11/自动文摘（九）/arch.png" width="500" height="500">
<p>1、首先收集50个流行的官方组织用户作为种子。分别来自政治、经济、军事、电影、游戏等领域，比如人民日报。</p>
<p>2、然后从种子用户中抓取他们关注的用户，并且将不是大V，且粉丝少于100万的用户过滤掉。</p>
<p>3、然后抓取候选用户的微博内容。</p>
<p>4、最后通过过滤，清洗，提取等工作得到最后的数据集。</p>
<p><code>这个环节涉及到的技术是爬虫技术，整体的思路比较简单，先选择一些高质量的用户作为起点，从他们关注的用户中过滤出类似的大V用户，然后再爬取所有候选用户的微博内容，清洗、过滤和提取有效数据。</code></p>
<h1 id="Data-Properties"><a href="#Data-Properties" class="headerlink" title="Data Properties"></a>Data Properties</h1><p>数据集主要分为三个部分，如下表：</p>
<img src="/2016/05/11/自动文摘（九）/table.png" width="500" height="500">
<p>1、第一部分是本数据集的主要部分，包含了2400591对（短文本，摘要），这部分数据用来训练生成摘要的模型。</p>
<p>2、第二部分包括了10666对人工标注的（短文本，摘要），每个样本都打了1-5分，分数是用来评判短文本与摘要的相关程度，1代表最不相关，5代表最相关。这部分数据是从第一部分数据中随机采样出来的，用来分析第一部分数据的分布情况。其中，标注为3、4、5分的样本原文与摘要相关性更好一些，从中也可以看出很多摘要中会包含一些没有出现在原文中的词，这也说明与句子压缩任务不同。标注为1、2分的相关性差一些，更像是标题或者是评论而不是摘要。统计表明，1、2分的数据少于两成，可以用监督学习的方法过滤掉。</p>
<p>3、第三部分包括了1106对，三个人对2000对进行了评判，这里的数据独立于第一部分和第二部分。选择3分以上的数据作为短文本摘要任务的测试数据集。</p>
<p><code>数据集的构造是一个非常大的工作，因为涉及到大量的人工标注工作，如何保证所用的训练集、测试集都有很高的质量是一个问题。本文对第一部分的数据做了采样分析，用第二部分数据作为训练高质量数据的样本，提取出了更高质量的训练集，第三部分提供了测试集。到此，数据集比较完整了。</code></p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>本文用了当下流行的seq2seq技术来做验证实验，用第一部分的数据作为训练集，第三部分的3分以上数据作为测试集。一共用了两种方法来处理数据：</p>
<p>1、基于汉字的方法(character-based)，将词汇表降维到了4000。</p>
<p>2、基于词的方法（word-based），本文用jieba做分词，词汇表维度为50000。</p>
<p>两种网络架构：</p>
<p>1、RNN作为Encoder，用最后一个hidden state作为Decoder的输入。</p>
<img src="/2016/05/11/自动文摘（九）/fig1.png" width="500" height="500">
<p>2、Encoder中的所有hidden state的组合作为Decoder的输入。</p>
<img src="/2016/05/11/自动文摘（九）/fig2.png" width="500" height="500">
<p>RNN隐藏层用GRU，生成摘要时用beam search，beam size设置为10。对比结果如下：</p>
<img src="/2016/05/11/自动文摘（九）/result.png" width="500" height="500">
<p>评测方法采用ROUGE-1，ROUGE-2，ROUGE-L，由于标准的ROUGE包是用来评测英文的，所以这里将中文汉字转换成id。结果中基于汉字的RNN context模型有更好的效果。简单分析下原因，基于词的模型由于词汇表的限制，非常容易遇到unknown words，而基于字则不同，可以轻松解决unk的问题。</p>
<p><code>本文的两种模型搭配两种网络架构，涵盖了简单的seq2seq和seq2seq+attention，很明显地可以看到基于字的模型效果更加好，因为成功地避免了unk的问题。最近有的文章在解决unk的问题，比如用了Pointer/Copy Mechanism来解决。下一次要好好总结一下unk问题的解决方案和ROUGE评测方法。</code></p>
<h1 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h1><p>未来工作：</p>
<p>1、多层次RNN。</p>
<p>2、unk的改进。</p>
<p><code>本文的数据集中输入部分并非只有一句话，而是一段话，简单的rnn并不能准确捕捉其意思。unk是一个很棘手的问题，接下来的博客会单独介绍unk的问题。</code></p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-10T08:50:08.000Z"><a href="/2016/05/10/自动文摘（八）/">2016-05-10</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/10/自动文摘（八）/">自动文摘（八）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>最近读的几篇paper都是用seq2seq的思路来解决问题，有点读厌烦了，今天换个口味。分享一篇extractive式的paper，<b>AttSum: Joint Learning of Focusing and Summarization with Neural Attention</b>，AttSum是本文所阐述的摘要系统的名称。</p>
<p><code>本文用了CNN+word embedding来表示sentence，然后将sentence vector加权求和作为document vector，通过将sentence和document映射到同一空间中，更容易在语义层上计算相似度。CNN之前多用于CV领域，后来在NLP中也应用了起来，尤其是各种各样的sentence classification任务中。在这个层面上将deep learning应用到了extractive summarization中，与之前seq2seq的paper有着本质的区别。整体来看，本文并没有太出众的创新点和突出的结果，反倒是提到了Attention机制，但并没有从模型体现地很充分，所以有炒概念的嫌疑。将文本中表示文本的方法应用在seq2seq的encoder部分，是本文的一种扩展和未来要做的工作。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>基于查询的抽取式文本摘要系统通常需要解决好相关性和显著性两个方面的任务，但一般的系统通常将两者分开考虑。而本文将两个任务合并考虑。本文的算法可以自动学习sentence和document cluster的embedding，并且当查询给定之后，可以应用注意力机制来模拟人类阅读行为。算法的有效性在DUC基于查询的文本摘要任务中进行了验证，得到了有竞争力的结果。</p>
<p><code>本文是将最近比较火的注意力模型应用到了extractive文摘系统中，同时也用了sentence embedding来解决语义层面的相关性问题，并没有像之前的文章在改动seq2seq+attention的细节上做文章，而是切换到了另外一种思路上。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>基于查询的文本摘要系统一般应用于多文档摘要任务，既需要考虑摘要中句子的相关性，又要考虑句子的显著性，相关性反应了文档的主题思想，显著性避免了重复和冗余。很长一段时间，逻辑斯特回归是解决这类问题的热门方法，但是类似的大多数的监督学习方法都是将两个问题分开考虑，通过一些feature对相关性和显著性分开打分排序。但是人在写摘要的时候，往往会将相关性和显著性合并起来考虑。</p>
<p>另外，相关性打分的方法也存在弊端，用类似TF-IDF的指标来打分有时并不能得到非常相似的结果，尽管可能匹配到了核心词，但检索出的结果并不一定可以满足用户的要求。</p>
<p>用深度学习的技术来表示feature，会比用简单的feature去打分排序更加科学，将两个指标融合在一个模型中解决文本摘要问题将会是一个不错的方案。本文提出了一个名叫AttSum的文摘系统，联合了相关性和显著性两个指标。</p>
<p>注意力模型已经成功应用在学习多模态对齐问题中，也可以借鉴到本问题当中。人类总是会将注意力放在他们query的东西上面。</p>
<p>本文在DUC2005-2007基于查询的文摘任务中进行了验证测试，在没有使用任何人为feature的情况下，获得了有竞争力的结果。本文的贡献有两点：</p>
<p>1、应用了注意力机制在基于查询的文摘任务中。</p>
<p>2、提出了一种联合查询相关性和句子显著性的神经网络模型。</p>
<p><code>相关性的打分问题是搜索引擎的基本问题，传统的方案是用一些简单的feature，比如TF-IDF来打分排序，但经常会得不到满意的结果，原因在于feature太过肤浅，并没有考虑语义层面的东西，换句话说并没有真正理解用户需要查什么。当然，有一段时间推荐系统扮演着搜索引擎助手的角色，当一个用户通过留下一些蛛丝马迹给网站，网站就会给他做一些个性化的推荐来辅助搜索引擎，但并不能从根本上解决这个问题。于是本文用了sentence embedding，document embedding来解决这个问题，就像当初的word embedding 一样，将语义映射到一个空间中，然后计算距离来测量相关性。</code></p>
<h1 id="Query-Focused-Sentence-Ranking"><a href="#Query-Focused-Sentence-Ranking" class="headerlink" title="Query-Focused Sentence Ranking"></a>Query-Focused Sentence Ranking</h1><img src="/2016/05/10/自动文摘（八）/arch.png" width="600" height="600">
<p>AttSum系统一共分为三层：</p>
<p>1、CNN Layer，用卷积神经网络将句子和query映射到embedding上。</p>
<p>2、Pooling Layer，用注意力机制配合sentence embeddings构造document cluster embeddings。</p>
<p>3、Ranking Layer，计算sentence和document cluster之间的相似度，然后排序。</p>
<h2 id="CNN-Layer"><a href="#CNN-Layer" class="headerlink" title="CNN Layer"></a>CNN Layer</h2><p>这一层的输入是用word embeddings构造的sentence matrix，然后在该矩阵上用一个卷积filter，之后再应用一个max pooliing获取到features，得到输出。</p>
<p><code>这个方法非常简单，是一个典型的CNN的应用，需要注意的是filter的宽度和词向量的宽度一致，看起来和n-gram类似，但是用了卷积神经网络来捕捉sentence matrix中的最大特征。将变长的句子都统一映射到同一个空间中，为后续计算相似度提供了极大的方便。</code></p>
<h2 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h2><p>这一层用sentence embedding加权求和来得到document cluster embedding。首先计算句子和query的相关性：</p>
<img src="/2016/05/10/自动文摘（八）/formula1.png" width="300" height="300">
<p>这里的相关性计算和相似度是两回事，其中M矩阵是一个tensor function，可以用来计算sentence embedding和query embedding之间的相互影响，两个相同的句子会得到一个较低的分数。然后加权求和得到document cluster embedding：</p>
<img src="/2016/05/10/自动文摘（八）/formula2.png" width="300" height="300">
<p>这里，sentence embedding扮演两个角色，既是pooling项，又是pooling权重。一方面，如果一个句子与query高度相关，则权重会很大；另一方面，如果一个句子在文档中是显著的，该句子的embedding也应被表示在其中。</p>
<p>本文强调了attention机制在Rush等人的工作中依赖于手工feature，不是那么自然地模拟人类的注意力，而本文是真正地无干预地在模拟人类的注意力。</p>
<p><code>感觉这一层的模型中只有M比较神秘一些，但整体来说思路还是非常简单，sentence表示出来了，document用sentence加权求和的方式来表示。只是说权重的计算方法很玄乎，还鄙视了其他人在用attention机制时并不自然。</code></p>
<h2 id="Ranking-Layer"><a href="#Ranking-Layer" class="headerlink" title="Ranking Layer"></a>Ranking Layer</h2><p>打分排序层同样简单，用了余弦公式来计算sentence和document之间的相似度。在训练的过程中用了<code>pairwise ranking strategy</code>，选择样本的时候，用了ROUGE-2计算了所有句子的score，高分的作为正样本，低分的作为负样本。</p>
<p>根据pairwise ranking的标准，相比于负样本，AttSum应该给正样本打出更高的分数，因此损失函数定义如下：</p>
<img src="/2016/05/10/自动文摘（八）/formula3.png" width="300" height="300">
<p>训练方式采用mini-batch SGD。</p>
<p><code>排序层也没什么特别的地方，用了最简单的余弦公式来计算相似度，通过结对排序的方法，先用ROUGE-2指标将所有的句子进行了打分，高分的句子作为正样本，低分的作为负样本，构造损失函数，让正样本的分数尽可能高，负样本的分数尽可能低。</code></p>
<h1 id="Sentence-Selection"><a href="#Sentence-Selection" class="headerlink" title="Sentence Selection"></a>Sentence Selection</h1><p>本文在选择句子时采用了一种类似于MMR的简单贪婪算法（MMR在之前的博客中有介绍）。具体过程如下：</p>
<p>1、去掉少于8个词的句子。因为摘要不可能少于8个词。</p>
<p>2、用之前计算好的score对所有的句子进行降序排列。</p>
<p>3、迭代地将排名靠前的且不冗余的句子添加到队列中。这里的冗余定义为该句子相比进入队列的句子有更新的内容。</p>
<p>具体算法流程如下：</p>
<img src="/2016/05/10/自动文摘（八）/select.png" width="600" height="600">
<p><code>句子的选择算法几乎就是MMR，也是一种贪心的思路。不同的地方在于对冗余的定义不如MMR，MMR是用当前句子与已经在队列中的句子的相似度作为冗余判断，其实这样更加科学。</code></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>数据集用DUC2005-2007三年的数据，用两年的数据作为训练集，一年的数据作为测试集。</p>
<h2 id="Model-Setting"><a href="#Model-Setting" class="headerlink" title="Model Setting"></a>Model Setting</h2><p>CNN层：50维词向量，用gensim实现，训练过程中不更新词向量，窗口尺寸选择2，即2-gram，和ROUGE-2保持一致，句子向量维度用5-100进行试验，最终用50作为句子向量维度。</p>
<h2 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h2><p>评价指标用ROUGE-2。</p>
<h2 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h2><p>使用了之前成绩不错的MultiMR和SVR系统作为baselines，同时为了验证本文模型的有效性，构造了一个ISOLATION系统，单独考虑相关性和显著性。</p>
<h2 id="Summarization-Performance"><a href="#Summarization-Performance" class="headerlink" title="Summarization Performance"></a>Summarization Performance</h2><p>对比结果看下图：</p>
<img src="/2016/05/10/自动文摘（八）/result.png" width="600" height="600">
<p><code>整体来看本文的算法结果具有竞争性，但没有绝对竞争性。训练数据用ROUGE-2指标做了预处理分析，目标函数也是朝着ROUGE-2最大的方向，最后的评价指标也是ROUGE-2，在DUC2005和2006上很容易出现过拟合的情况，比其他结果表现好也是正常情况。整体感觉模型的效果很一般。</code></p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1604.00125v1.pdf" target="_blank" rel="external">AttSum: Joint Learning of Focusing and Summarization with Neural Attention</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-07T04:02:15.000Z"><a href="/2016/05/07/自动文摘（七）/">2016-05-07</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/07/自动文摘（七）/">自动文摘（七）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>再坚持一下，就会等到黎明破晓，重见光明。</strong></p>
</blockquote>
<p>今天继续分享一篇sentence level abstractive summarization相关的paper，出自于IBM Watson，<b>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</b></p>
<p><code>本文是一篇非常优秀的paper，在seq2seq+attention的基础上融合了很多的features、trick进来，提出了多组对比的模型，并且在多种不同类型的数据集上做了评测，都证明了本文模型更加出色。从本文中得到了很多的启发。</code></p>
<p><code>（1）单纯地data-driven模型并不能很好地解决文本摘要的问题，针对文本摘要问题的特点，融合一些feature到模型之中，对模型的效果有很大的帮助。</code></p>
<p><code>（2）他山之石可以攻玉。其他领域的研究成果，小的trick都可以尝试借鉴于文本摘要问题之中，比如seq2seq+attention的技术从机器翻译中借鉴过来应用于此，比如LVT技术等等。</code></p>
<p><code>（3）文本摘要问题的解决需要解决好方方面面的问题，不仅仅是模型方面，还有数据集，还有评价指标，每个方面的进步都会是一大进步。</code></p>
<p><code>（4）deep learning技术训练出的模型泛化能力和扩展能力还有很长的路要走，对训练数据的严重依赖，导致了泛化能力和扩展能力的不足。针对特定的问题，构建特定的训练数据集，这对corpus的建设提出了更高的要求。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p> 本文将自动文摘问题当作一个<code>Seq2Seq</code>的问题，并且应用<code>Attentional Encoder-Decoder Recurrent Neural Networks</code>框架来解决这个问题，并且在两个不同的数据集上取得了超越ABS（Rush,2015）模型的结果。同时，本文还提出多种模型来研究自动文摘中的重要问题，比如对关键词进行建模，并且得出词对于文档研究起关键作用的结论。研究结果表明本文的解决方案在性能上有很大的提升，另外，还贡献了一个包括多句子文摘的数据集和基准。</p>
<p><code>本文的贡献点有三个：（1）在两个新的数据集上应用seq2seq+attention模型，并且取得了state-of-the-art的结果；（2）研究了关键词对于自动文摘所起到的关键作用，并且提出了一种新的模型；（3）提出了一个新的数据集，供研究者使用。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文研究的文本摘要问题是根据一篇长文来生成一篇短摘要或者标题，可以等同于将输入文本序列映射为一个目标文本序列，也就是seq2seq的问题。目前，解决seq2seq的问题，都是借鉴machine translation的方法。</p>
<p>但文本摘要问题和机器翻译问题有着很大的不同。</p>
<p>（1）文本摘要的问题输出长度一般都很短，并且不依赖于输入的长度。</p>
<p>（2）文本摘要的问题一般都会损失大量的信息，只保留少量的核心概念作为输出，而机器翻译则要保证信息损失最低，在单词级别上保证对齐。</p>
<p>那么机器翻译的相关技术是否会在文本摘要问题上表现同样突出呢？本文将会回答这个问题。受文本摘要与机器翻译问题的不同特点所启发，本文将超越一般的架构而提出新的模型来解决摘要问题。</p>
<p><code>本文与之前seq2seq类的paper有着一个很明显的区别，就是将摘要问题和机器翻译问题严格区别开，而不是简单地套用MT的技术来解决摘要问题，根据摘要问题的特点提出了更加合适的模型，相比于之前的研究更进了一步。</code></p>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><p>之前大量的研究都是集中于extractive摘要方法，在DUC2003和2004比赛中取得了不俗的成绩。但人类在做摘要工作时，都是采用abstractive方法，理解一篇文档然后用自己的语言转述出来。随着深度学习技术在NLP任务中的广泛使用，研究者们开始更多地研究abstractive方法，比如Rush组的工作，比如哈工大Hu的工作。</p>
<p>本文的贡献：</p>
<p>（1）在两种不同数据集上应用seq2seq+attention的模型，得到了state-of-the-art结果。</p>
<p>（2）根据摘要问题的特点提出了针对性的模型，结果更优。</p>
<p>（3）提出了一个包括多句子摘要的数据集和基准。</p>
<p><code>绝大多数的extractive方法都是unsupervised learning方法，不需要数据集来做训练，更适合搭建实用的文本摘要系统；而abstractive方法一般都是supervised learning方法，虽然在评测任务中表现更佳，但需要大量的领域数据做训练，横向扩展性并不好，数据集的内容、质量和规模都直接影响着模型的效果，目前比较难应用在实际系统中。所以，在data-driven的模型中，往往都需要配合大规模、高质量、领域相关的数据集。相比之下deep learning的方法更加简单粗暴一下，并不需要什么领域知识和特征工程，只要给定输入输出，就能拟合出一个巨大的神经网络，并且取得优于传统解决方案的结果，但太过依赖于数据，因此丧失了一般性。</code></p>
<h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1><h2 id="Encoder-Decoder-with-Attention"><a href="#Encoder-Decoder-with-Attention" class="headerlink" title="Encoder-Decoder with Attention"></a>Encoder-Decoder with Attention</h2><p>这个模型本文的基准模型，Encoder是一个双向GRU-RNN，Decoder是一个单向GRU-RNN，两个RNN的隐藏层大小相同，注意力模型应用在Encoder的hidden state上，一个softmax分类器应用在Decoder的生成器上。</p>
<p><code>基准模型是套用Bahdanau,2014在机器翻译中的方法，解决方案的思路都与之前的paper类似，并无新颖之处。</code></p>
<h2 id="Large-Vocabulary-Trick"><a href="#Large-Vocabulary-Trick" class="headerlink" title="Large Vocabulary Trick"></a>Large Vocabulary Trick</h2><p>这个模型引入了<code>large vocabulary trick(LVT)</code>技术到文本摘要问题上。本文方法中，每个mini batch中decoder的词汇表受制于encoder的词汇表，decoder词汇表中的词由一定数量的高频词构成。这个模型的思路重点解决的是由于decoder词汇表过大而造成softmax层的计算瓶颈。本模型非常适合解决文本摘要问题，因为摘要中的很多词都是来自于原文之中。</p>
<p><code>LVT是一个针对文本摘要问题的有效方法，考虑到了摘要中的大部分词都是来源于原文之中，所以将decoder的词汇表做了约束，降低了decoder词汇表规模，加速了训练过程。</code></p>
<h2 id="Vocabulary-expansion"><a href="#Vocabulary-expansion" class="headerlink" title="Vocabulary expansion"></a>Vocabulary expansion</h2><p>LVT技术很好地解决了decoder在生成词时的计算瓶颈问题，但却不能够生成新颖的有意义的词。为了解决这个问题，本文提出了一种扩展LVT词汇表的技术，将原文中所有单词的一度最邻近单词扩充到词汇表中，最邻近的单词在词向量空间中cosine相似度来计算得出。</p>
<p><code>词汇表的扩展是一项非常重要的技术，word embedding在这里起到了关键作用。用原文中单词的最邻近单词来丰富词汇表，不仅仅利用LVT加速的优势，也弥补了LVT带来的问题。</code></p>
<h2 id="Feature-rich-Encoder"><a href="#Feature-rich-Encoder" class="headerlink" title="Feature-rich Encoder"></a>Feature-rich Encoder</h2><p>文本摘要面临一个很大的挑战在于确定摘要中应该包括哪些关键概念和关键实体。因此，本文使用了一些额外的features，比如：词性，命名实体标签，单词的TF和IDF。将features融入到了word embedding上，对于原文中的每个单词构建一个融合多features的word embedding，而decoder部分，仍采用原来的word embedding。</p>
<p><code>本文的一个创新点在于融入了word feature，构建了一组新的word embedding，分别考虑了单词的词性、TF、IDF和是否为命名实体，让单词具有多个维度的意义，而这些维度上的意义对于生成摘要至关重要。本文结果的优秀表现再一次印证了简单粗暴的纯data driven方法比不上同时考虑feature的方法。后面的研究可以根据本文的思路进行进一步地改进，相信会取得更大的突破。</code></p>
<h2 id="Switching-Generator-Pointer"><a href="#Switching-Generator-Pointer" class="headerlink" title="Switching Generator/Pointer"></a>Switching Generator/Pointer</h2><p>文本摘要中经常预见这样的问题，一些关键词出现很少但却很重要，由于模型基于word embedding，对低频词的处理并不友好，所以本文提出了一种decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中。pointer机制在解决低频词时鲁棒性比较强，因为使用了encoder中低频词的隐藏层表示作为输入，是一个上下文相关的表示，而仅仅是一个词向量。</p>
<img src="/2016/05/07/自动文摘（七）/Pointer.png" width="600" height="800">
<p><code>Pointer机制与某篇paper中的Copy机制有异曲同工之妙，都是用来解决OOV问题的，本文的最关键的是如何计算开关状态是generator的概率，这一层的计算关系到当前time step是采用generator模式还是pointer模式。</code></p>
<h2 id="Hierarchical-Encoder-with-Hieratchical-Attention"><a href="#Hierarchical-Encoder-with-Hieratchical-Attention" class="headerlink" title="Hierarchical Encoder with Hieratchical Attention"></a>Hierarchical Encoder with Hieratchical Attention</h2><p>数据集中的原文一般都会很长，原文中的关键词和关键句子对于形成摘要都很重要，这个模型使用两个双向RNN来捕捉这两个层次的重要性，一个是word-level，一个是sentence-level，为了区别与Li的工作，本文在两个层次上都使用注意力模型。注意力权重如下：</p>
<img src="/2016/05/07/自动文摘（七）/formula1.png" width="300" height="300">
<p>本文模型示意图如下：</p>
<img src="/2016/05/07/自动文摘（七）/Attention.png" width="600" height="800">
<p><code>注意力机制的本质是一组decoder与encoder之间相关联的权重，本文在两个层次上重新定义了attention weight，既考虑了某个encoder中每个word对于decoder的重要性，也考虑了该word所在sentence对于decoder的重要性。</code></p>
<h1 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h1><h2 id="Gigaword-Corpus"><a href="#Gigaword-Corpus" class="headerlink" title="Gigaword Corpus"></a>Gigaword Corpus</h2><p>为了作对比，本文采用了Rush文章中的Gigaword数据集和他的开源代码来处理数据，形成了380万训练样本和约40万验证样本和测试样本，本文随机选取2000组样本作为验证和测试集来测试本文模型的性能，为了更加公平地对比，本文使用了Rush采用的测试集来对比。</p>
<p>初始词向量的生成是用Word2Vec，但在训练的过程中会更新词向量。训练的参数设置也都采用一般的训练设置。</p>
<p>在Decoder阶段，采用大小为5的<code>beam search</code>来生成摘要，并且约束摘要长度不大于30个单词。</p>
<p>评价指标方面，采用full-length Rouge召回率，然而该指标更加青睐于长摘要，所以在比较两个生成不同长度摘要的系统时并不公平，用full-length F1来评价更加合理。</p>
<p>对比实验共有以下几组：</p>
<p>（1）words-1sent：baseline模型，对应之前的Encoder-Decoder with Attention模型。1sent表示模型的输入是原文的第一句话。</p>
<p>（2）words-lvt2k-1sent：对应之前的Large Vocabulary Trick模型。lvt2k表示decoder的词汇表上限是2000。</p>
<p>（3）words-lvt2k-(2|5)sent：与第二组实验采用相同的模型，只是输入采用了原文的前两句话和前五句话。</p>
<p>（4）words-lvt2k-2sent-exp：对应之前的Vocabulary expansion模型。</p>
<p>（5）words-lvt2k-2sent-hieratt：对应之前的Hierarchical Encoder with Hieratchical Attention模型。</p>
<p>（6）big-words-lvt2k-(1|2)sent：模型与第二组实验相同，但将embedding size和hidden state size增大到了200和400。</p>
<p>（7）big-feats-lvt2k-2sent：对应之前的Feature-rich Encoder模型。</p>
<p>（8）feats-lvt2k-2sent-ptr：对应之前的Switching Generator/Pointer模型。</p>
<p>实验结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result1.png" width="600" height="800">
<p>从表中清晰地看到switching generator/pointer模型在各个指标上都是最好的模型，本文的模型在Rush测试集中的结果都优于Rush的ABS+模型。</p>
<p><code>Gigaword由于其数据量大的特点，常被用于文本摘要任务中作为训练数据。本文的训练、生成参数都沿用了传统的方法，评价指标选择了更合适的F1，共设计了8大组实验，从方方面面对比了各个模型之间的优劣，从多个角度说明了本文模型比前人的模型更加优秀。</code></p>
<p><code>数据集对于deep learning是至关重要的，构建一个合适的数据集是一个非常有意义的工作。哈工大之前有一个工作就是构建了微博摘要的数据集，方便了研究中文文本摘要的研究者。</code></p>
<h2 id="DUC-Corpus"><a href="#DUC-Corpus" class="headerlink" title="DUC Corpus"></a>DUC Corpus</h2><p>DUC2003作为模型的验证集，DUC2004作为对比测试的数据集，模型的训练都是通过Gigaword来做，这里主要是为了对比本文模型和Rush的ABS和ABS+模型，结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result2.png" width="600" height="800">
<p>在DUC2003中big-words-lvt2k-1sent表现更加突出，所以用该模型来与其他系统进行对比，结果明显优于其他系统。</p>
<p><code>本文模型相比于其他系统的优势在DUC数据集中并不如Gigaword数据集上更加明显。因为大家的模型都是采用Gigaword来做的，模型都非常好地拟合了Gigaword数据集。从这个结论中也可以看出，deep learning技术对于数据集规模、质量和类型的依赖，并不能很好地泛化到其他数据内容中。</code></p>
<h2 id="CNN-DailyMail-Corpus"><a href="#CNN-DailyMail-Corpus" class="headerlink" title="CNN/DailyMail Corpus"></a>CNN/DailyMail Corpus</h2><p>现有的abstractive摘要系统都是单句摘要，本节实验将要证明本文的模型同样在多句摘要任务中会有更好的表现。</p>
<p>实验结果如下：</p>
<img src="/2016/05/07/自动文摘（七）/result3.png" width="600" height="800">
<p>从表中结果可以看出，switching generator/pointer模型更加优秀。</p>
<p><code>本文的一大贡献在于构建了CNN/DailyMail文本摘要数据集，用来评测多句摘要的任务，为今后大量的相关工作提供了数据保障。</code></p>
<h1 id="Qualitative-Analysis"><a href="#Qualitative-Analysis" class="headerlink" title="Qualitative Analysis"></a>Qualitative Analysis</h1><p>本文的模型在一些数据的处理会理解错原文的意思，生成一些“错误”的摘要。另外，Switching Generator/Pointer模型不仅仅在处理命名实体上有优势，而且在处理词组上表现也非常好。未来的工作中，将会对该模型进行更多的实验。效果见下图：</p>
<img src="/2016/05/07/自动文摘（七）/result4.png" width="600" height="800">
<p><code>本文模型相对于Rush的模型有了更进一步的效果，但对于文本摘要问题来说，并没有本质上的提升，也会经常出现这样或者那样的错误。指标上的领先是一种进步，但与评价指标太过死板也有关系，现有的评价指标很难从语义这个层次上来评价结果。所以，文本摘要问题的解决需要解决方方面面的问题，比如数据集，比如评价指标，比如模型，任何一个方面的突破都会带来文本摘要问题的突破。</code></p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>本文提出的模型相比于前人的模型有更好的效果，并且构建了一个新的摘要数据集，可以满足多句摘要的任务。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1602.06023" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-30T08:30:43.000Z"><a href="/2016/04/30/自动文摘（六）/">2016-04-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/30/自动文摘（六）/">自动文摘（六）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>万事开头难，其实之后的事情可能会更难，但开好了头，就会有充足的信心来面对后面的困难。</strong></p>
</blockquote>
<p>记得Andrew Ng在一个采访中曾经说过：“当我和研究人员，或是想创业的人交谈时，我告诉他们如果你不断地阅读论文，每周认真研究六篇论文，坚持两年。然后，你会学到很多东西。这是对你长期发展一个极好的投资。”，这个投资将会是一个回报非常高的投资。其实，不仅仅是坚持读论文，当你将任何一件不起眼但却有一些意义的事情坚持做很长的时间都会得到不错的回报，比如健身，比如跑步，比如写博客，比如摄影，每一个这样的习惯看起来都不会立刻产生很明显的效果，但坚持久了对我们的生活和工作都会带来一个质的飞跃。</p>
<p>上周写了篇paper的读书笔记，也是对自己读了那篇paper的一些思考的提炼，本周将会带来对两篇paper的思考。其中一篇是<b>A Neural Attention Model for Abstractive Sentence Summarization</b>，另一篇是<b>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</b>，两篇文章都出自于<a href="http://harvardnlp.github.io/" target="_blank" rel="external">Harvard NLP</a>组，两篇是姊妹篇，第二篇是第一篇的升级版，所以要结合着读，对比着分析。</p>
<p><code>世上没有什么所谓的银弹，每种方法存在都有其存在的意义。第一篇paper尝试将seq2seq+attention应用在summarization任务上，但并未取得比较令人满意的结果，反而增加了一些人工特征之后，才得到了很大的提升，虽然第二篇模型依旧是一个data-driven的模型，但我想如果给其添加上人工特征也会得到更好的效果。综合多种方法的优点来解决一个问题才是王道，而不是一味地、粗暴地套用某个范式，某个框架。</code></p>
<p><code>两篇文章从同一个角度入手，采用了不同难度的模型，非常好地解决了这个问题。联想到上周看的paper，他所采用的是多层lstm作为encoder和decoder，但数据集使用的并不相同，所以并不知道与本周的两篇paper哪个效果更好。但这也给出了一种发paper的思路，多去尝试一些encoder和decoder模型，不断地组合和对比，一定会有不错的发现。但这样的解决方案对于提升层次上没有太多溢出，因为大家都是照着模板去做，并没有真正地更深地理解到这个问题的本质。</code></p>
<p><code>一个系统的构建需要处理好方方面面的细节，比如数据的预处理，比如评测的实现，比如模型的参数调优，每个方面想要做好做精都是一门学问。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><h2 id="Paper-1"><a href="#Paper-1" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>本文提出了一种<code>data-driven</code>的方法来做句子摘要，使用了一种基于局部注意力模型（<code>local attention-based model</code>），在给定输入句子的情况下，生成摘要的每个词。模型结构非常简单，可以套用流行的<code>end2end</code>框架来训练，并且很容易扩展到大型训练数据集上。模型在DUC-2004任务中效果优于几个不错的baselines。</p>
<h2 id="Paper-2"><a href="#Paper-2" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文使用一种conditional RNN来生成摘要，条件是卷积注意力模型（<code>convolutional attention-based encoder</code>），用来确保每一步生成词的时候都可以聚焦到合适的输入上。模型仅仅依赖于学习到的features，并且很容易在大规模数据上进行end2end式地训练，并且在Gigaword语料上和DUC-2004任务中取得了更好的效果。</p>
<p><code>两篇paper的模型框架都是seq2seq+attention，最大的区别在于选择encoder和decoder的模型，第一篇的模型偏容易一些，第二篇用了rnn来做。seq2seq或者说end2end现在火的不得了，最初在机器翻译开始使用，后面推广到多模态学习，对话生成，自动问答，文本摘要等等诸多领域。</code></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>文本摘要有几种类型的任务，本文属于<code>headlines generation</code>，输入的是一段话，输出的是一句话或者一个标题。</p>
<h2 id="Paper-1-1"><a href="#Paper-1-1" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>受最近机器翻译中seq2seq技术的启发，本文将<code>Neural Language Model</code>和带有上下文的encoder结合起来，其中encoder与Bahdanau（<b>Neural machine translation by jointly learning to align and translate</b>）的attention-based encoder一样。encoder和decoder在句子摘要任务中共同训练。另外，decoder中也使用了beam search进行摘要生成。本文的方法简称ABS（<code>Attention-Based Summarization</code>），可以轻易扩展到大规模数据集进行训练，而且可以在任何document-summary对中进行使用。本文采用Gigaword语料集进行训练，包括大约400万篇新闻文章。为了检验模型的效果，与多种类型的文摘系统进行了对比，并且在DUC-2004任务上获得了最高分。</p>
<h2 id="Paper-2-1"><a href="#Paper-2-1" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文的decoder是一个<code>RNN LM</code>，生成摘要依赖的条件是encoder的输出，encoder会计算输入中每个词的分数，这个分数可以理解为对输入作软对齐（<code>soft alignment</code>），也就是说decoder在生成下一个单词时需要注意输入中的哪些单词。encoder和decoder要在一个sentence-summary数据集中进行共同训练。本文的模型可以看作第一篇ABS模型的扩展，ABS模型中decoder是用FNN LM，而本文使用RNN，encoder部分本文更加复杂，将输入单词的位置信息考虑在内，并且使用了卷积网络来编码输入单词。本文模型效果优于第一篇paper。</p>
<p><code>两篇paper都是seq2seq在sentence-level abstractive summarization任务中早期的尝试，给文本摘要方法带来了新鲜血液，第一篇paper中encoder和decoder都用了比较简单的模型，但已经得到了优于传统方法的结果，再一次地证明了deep learning在解决问题上的优势，第二篇paper升级了encoder和decoder，考虑了更复杂的细节，得到了更好的效果，相信后面会有大量的paper套用seq2seq+attention，再配合一些其他的技术来提升模型的效果，但整体的思路基本已固定下来，如果想要更大的突破，可能还需要提出另外一种框架来解决问题。</code></p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>这部分内容是初入门径的研究者最喜欢的工作，尤其是这个领域中最新研究的paper还没有出survey的情况下，大家想最快地了解这个领域中新技术的应用情况，读高水平paper中的相关工作是最有效的。</p>
<h2 id="Paper-1-2"><a href="#Paper-1-2" class="headerlink" title="Paper 1"></a>Paper 1</h2><p>文本摘要任务在sentence这个level可以等同于headlines generation，某种程度上与paraphrase相近。seq2seq于2014年在机器翻译领域中提出并流行开来，之前的研究大多都是基于extractive的思路，借助一些人工features来提升效果。seq2seq的意义在于完全基于数据本身，从数据中学习feature出来，并且得到了更好的效果。本文的方法比较简单，decoder也只用了NNLM（2003年由Bengio提出），而seq2seq在机器翻译中应用时都采用的是RNNLM，所以在Future Work中作者会用RNNLM，于是就有了第二篇paper。</p>
<h2 id="Paper-2-2"><a href="#Paper-2-2" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>由于都是一个组出的paper，还有共同的作者，这个部分写的差不多，只是多提了第一篇paper做的工作。</p>
<p><code>都说 读书破万卷，下笔如有神。在做一个领域的研究之前，免不了读大量相关的paper来做一些积累，related work这个部分就是大家写的小型survey，经常会提到一些该领域最经典的paper。感觉Rush他们组应该是比较新的NLP研究力量，将一个新的技术用在了自动文摘领域中，攒了两篇paper，也是数量上的一种积累。不过他们share了paper相关的code，用Torch来写模型部分，用python作数据处理。组里也包括那位将CNN用在sentence classification中的Yoon Kim，相信他们日后会有更多更好的成果。</code></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>本节是用数学语言定义句子摘要问题，两篇文章解决的问题相同。给定一个输入句子，目标是生成一个压缩版的摘要。句子级别的摘要问题可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula1.png" width="300" height="400">
<p>x表示输入句子，y表示生成的摘要句子集合，定义一个系统是abstractive的，就是从生成句子集合中找到score最大的那一个。而extractive摘要系统可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula2.png" width="300" height="400">
<p>sentence compression系统可以定义如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula3.png" width="300" height="400">
<p>然而abstractive summarization存在一个更加困难的生成过程。</p>
<p><code>用了一个简单的数学公式将问题描述地非常清楚，包括一些细节，比如输入长度大于输出长度，输出长度为固定值，输入输出拥有相同的词汇表等等。从数学公式来看score函数的定义很重要，考虑的参数类型不同会有不同的score，也就是不同的模型，明显看得出abstractive要远难于extractive和sentence compression。</code></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>模型部分是paper的重头戏，分为Objective，Encoder，Decoder，Generation，Training五个子部分来讨论。</p>
<h2 id="Paper-1-3"><a href="#Paper-1-3" class="headerlink" title="Paper 1"></a>Paper 1</h2><h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><p>目标函数是Negative Log-Likelihood（NLL），decoder中生成摘要单词的条件是encoder的输出和当前生成词的窗口词向量，具体如下：</p>
<img src="/2016/04/30/自动文摘（六）/formula4.png" width="300" height="400">
<p>这里当前生成词的窗口词向量由下式表示：</p>
<img src="/2016/04/30/自动文摘（六）/formula5.png" width="100" height="100">
<p>其实也就是NNLM中的N-gram，用来预测下一个词。目标函数表示为：</p>
<img src="/2016/04/30/自动文摘（六）/formula6.png" width="300" height="400">
<p>对于i&lt;1的情况，在句子前padding几个开始符号。接下来建模的部分就是研究如何表达条件概率。</p>
<p><code>目标函数用生成词的条件概率的对数来表示是NLP中非常常用的做法。不同的模型都在研究如何表示条件，比如encoder的表示，encoder输出的表示，decoder中当前词前序词的表示等等。</code></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>本文一共提出了三种encoder模型。</p>
<h4 id="Bag-of-Words-Encoder"><a href="#Bag-of-Words-Encoder" class="headerlink" title="Bag-of-Words Encoder"></a>Bag-of-Words Encoder</h4><p>词袋模型是最简单的一个模型，将输入的句子用词袋模型降维到H，生成一个word embedding层。模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/bow.png" width="300" height="400">
<p><code>词袋模型并不会考虑词序的关系，效果并不会太好，但是作为paper中的一个baseline模型会有很好的对比结果。</code></p>
<h4 id="Convolutional-Encoder"><a href="#Convolutional-Encoder" class="headerlink" title="Convolutional Encoder"></a>Convolutional Encoder</h4><p>卷积模型是一个深度网络模型，可以很好地捕捉输入的特征。模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/CNN.png" width="300" height="400">
<p>其中矩阵F是输入句子的word embedding矩阵，Q包括了一系列过滤层，并且采用了最大池化技术来处理。</p>
<p><code>CNN通过结合word embedding将句子表示成一个matrix，通过不同尺寸的卷积核来filter出句子中的feature，本质上和N-gram一样，N-gram的N就是卷积核的尺寸，构建出多种feature maps，然后max pooling，然后filter，然后pooling，最终采用一个MLP得出结果。</code></p>
<h4 id="Attention-Based-Encoder"><a href="#Attention-Based-Encoder" class="headerlink" title="Attention-Based Encoder"></a>Attention-Based Encoder</h4><p>虽然卷积模型比词袋模型更能捕捉句子的特征，却同样需要对整个句子做表示，机器翻译领域在解决相同问题时采用了注意力模型来构建context，然后基于生成的context来构建representation。本文采用一种类似于词袋模型的注意力模型，模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/attention.png" width="300" height="400">
<p>其中矩阵G是context的word embedding矩阵，P是一个权重矩阵，权重连接着输入word embedding和context embedding，Q是一个光滑窗口，流程如下图：</p>
<img src="/2016/04/30/自动文摘（六）/attention2.png" width="300" height="400">
<p><code>本文的注意力模型可以视作将词袋模型中的P向量用一个待学习的soft alignment来替换了。</code></p>
<p><code>三种encoder模型给出了input sentence的表示，第三种还给出了summary和input之间的关系，encoder的输出将作为decoder的输入，来生成summary。</code></p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder的本质就是一个神经网络语言模型，本文用了2003年Bengio提出的NNLM，模型如下：</p>
<img src="/2016/04/30/自动文摘（六）/NNLM.png" width="300" height="400">
<p>Bengio的模型是一个FNN（Feed-Forward Neural Network），通过上文（当前词的前N个词）来预测当前词，流程如下图：</p>
<img src="/2016/04/30/自动文摘（六）/NNLM2.png" width="300" height="400">
<p>待求的参数是word embedding矩阵E，输入层到隐藏层的权重矩阵U，隐藏层到decoder输出层的权重矩阵V，encoder输出层到decoder输出层的权重矩阵W。</p>
<p><code>NNLM是一个经典的语言模型，本质上就是一个神经网络多分类器，文中也提到可以考虑用RNNLM来作decoder，也就有了第二篇paper的模型。</code></p>
<h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>一般的语言模型都是基于上下文生成概率最高的一个词，但对于生成摘要句子来说还不够。通常的做法是用一种搜索算法在一定的可行域之内找到几组可行的解。本文采用<code>beam search</code>，也是之前机器翻译领域生成翻译结果时常用的算法，算法描述如下：</p>
<img src="/2016/04/30/自动文摘（六）/beamsearch.png" width="300" height="400">
<p><code>给定一个beam size K，在生成每一个summary word时，都保留概率最大的K个词，从生成第二个词开始，计算所有路径的概率，只保留概率最大的前K个分枝，裁剪掉剩余的分枝，继续生成第三个词，依次进行下去，直到生成的词是EOS或者达到最大句子长度限制。最后得到的结果是K个最好的sentence summary。</code></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>本文采用mini batch SGD算法对训练集进行训练，使得NLL最小。</p>
<p><code>因为在生成summary时并没有什么约束条件，所以本方法可以拓展到任意input-output pairs中使用。</code></p>
<h3 id="ABS"><a href="#ABS" class="headerlink" title="ABS+"></a>ABS+</h3><p>作者提出了一个纯数据驱动的模型之后，又提出了一个abstractive与extractive融合的模型，在ABS模型的基础上增加了feature function，修改了score function，也就是结果对比中的<code>ABS+</code>模型。</p>
<h2 id="Paper-2-3"><a href="#Paper-2-3" class="headerlink" title="Paper 2"></a>Paper 2</h2><p>本文模型简称为RAS（<code>Recurrent Attentive Summarizer</code>）</p>
<h3 id="Objective-1"><a href="#Objective-1" class="headerlink" title="Objective"></a>Objective</h3><p>目标函数如下：</p>
<img src="/2016/04/30/自动文摘（六）/NLL2.png" width="300" height="400">
<p>两篇paper都是采用NLL，但不同的是第二篇paper目标函数条件概率中的条件与第一篇不同，本文采用decoder的所有上文，而不是一个窗口内的上文。</p>
<h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><p>encoder的输出是decoder的输入，对于每一个time step，encoder都需要给出一个context vector，本文encoder的重点在于如何计算时间相关的context。</p>
<p>输入句子每个词最终的embedding是各词的embedding与各词位置的embedding之和，经过一层卷积处理得到aggregate vector：</p>
<img src="/2016/04/30/自动文摘（六）/formula21.png" width="300" height="400">
<p>根据<code>aggregate vector</code>计算context（encoder的输出）：</p>
<img src="/2016/04/30/自动文摘（六）/formula22.png" width="300" height="400">
<p>其中权重由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula23.png" width="300" height="400">
<p><code>Rush组的paper有一个特点，喜欢用CNN多一些，包括那位用CNN做句子分类的童鞋。可能的原因是，Rush是Facebook AI Research的研究人员，Lecun是Leader，所以他们对CNN的理解也更深一些，在model中使用的也就更多一些。</code></p>
<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder的部分是一个RNNLM，这里的RNN Hidden Layer使用的是LSTM单元。decoder的输出由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula24.png" width="300" height="400">
<p>其中c(t)是encoder的输出，h(t)是RNN隐藏层，由下式计算：</p>
<img src="/2016/04/30/自动文摘（六）/formula25.png" width="300" height="400">
<p>这里隐藏层的单元有两种思路，一种是常规的Elman RNN，一种是LSTM。</p>
<p><code>RNNLM的Hidden Unit可以不用LSTM或者GRU这么复杂，普通的隐藏层Elman RNN可以解决问题，采用Truncate-BPTT对RNN进行训练（详见Tomas Mikolov的PhD Thesis）。况且LSTM和GRU会带来更多的参数，造成overfit。</code></p>
<h3 id="Generation-1"><a href="#Generation-1" class="headerlink" title="Generation"></a>Generation</h3><p>生成过程中也采用<code>beam search</code>算法进行summary生成。</p>
<h3 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h3><p>给定一个训练集，包括大量的sentence-summary pairs，用SGD将NLL函数最小化得到最优的参数集，参数包含encoder和decoder两个部分的参数。</p>
<p><code>SGD是一种常用的优化算法，在解决NLP问题中非常有效，其中最常见的mini batch训练方法。</code></p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="Paper-1-4"><a href="#Paper-1-4" class="headerlink" title="Paper 1"></a>Paper 1</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>本文采用经过处理的约400万Gigaword数据集作为训练集和验证集，在DUC2004数据集上进行评测，评测使用ROUGE方法。</p>
<p><code>DUC的比赛经常会包括文本摘要，所以常常用来比较每个模型或系统的优劣。</code></p>
<h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>1、PREFIX，这个baseline是取输入的前75个字符作为headline。</p>
<p>2、TOPIARY。</p>
<p>3、COMPRESS。</p>
<p>4、IR。</p>
<p>5、W&amp;L。</p>
<p>6、MOSES+。</p>
<p><code>baselines选择了几组非常有代表性的系统。</code></p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>本文的程序用Torch实现，并且开源在Github上，处理1000个mini batch大概用时160s，最好的验证集参数出现在第15个epoch。</p>
<p><code>Torch是一个使用率非常高的开源工具，尤其是在研究领域。相比于Theano的难以调试，Torch具有非常简单、易用、灵活、易调试的特点。</code></p>
<h2 id="Paper-2-4"><a href="#Paper-2-4" class="headerlink" title="Paper 2"></a>Paper 2</h2><h3 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a>Dataset</h3><p>与第一篇相同的训练集和处理方法，同样使用DUC2004作为评测数据，ROUGE作为评测方法。</p>
<h3 id="Baselines-1"><a href="#Baselines-1" class="headerlink" title="Baselines"></a>Baselines</h3><p>1、ABS（第一篇paper中的方法）<br>2、ABS+（第一篇paper中的方法）</p>
<h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>同样使用Torch开发，在训练时用摘要的混乱度（<code>perplexity</code>）作为评价指标控制训练过程。</p>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="Paper-1-5"><a href="#Paper-1-5" class="headerlink" title="Paper 1"></a>Paper 1</h2><img src="/2016/04/30/自动文摘（六）/result1.png" width="300" height="400">
<p>分别在DUC2004和Gigaword数据集上进行了对比，本文的ABS模型在DUC2004上评测结果相比于最好的baseline MOSES+并不如意，MOSES+是一个基于短语的统计机器翻译系统（Koehn，2007），在Gigaword训练集上比MOSES+好一些。但增加了人工feature的ABS+模型比ABS模型和MOSES+系统表现好了非常多。</p>
<img src="/2016/04/30/自动文摘（六）/result11.png" width="300" height="400">
<p>5种不同的模型在混乱度这个指标上比较，ABS具有明显的优势。</p>
<p><code>ABS模型实际上的效果并不理想，所以本文作者又提出了一种所谓的ABS+模型，将人工feature融合到了ABS模型中，得到了不错的效果。如果只看这一篇paper，可能会觉得不理想的原因是seq2seq在自动文摘中的效果一般，但看过第二篇paper之后，就会明白是因为本文的模型太过简单，第二篇paper也就有了意义。从另一个角度来看，纯粹的data-driven方法如果配合上一些extractive的方法会得到更好的结果，这点对于实际系统的开发非常有意义。</code></p>
<h2 id="Paper-2-5"><a href="#Paper-2-5" class="headerlink" title="Paper 2"></a>Paper 2</h2><img src="/2016/04/30/自动文摘（六）/result21.png" width="300" height="400">
<p>在Gigaword数据集上对比各个模型，RAS-Elman模型表现最好，说明了seq2seq相比于传统的文摘系统和算法，可以更好地解决问题，又一次证明了deep learning的强大。同时也验证了普通的RNN不见得比LSTM活着GRU表现差，尤其是当序列长度不是特别长的情况。</p>
<img src="/2016/04/30/自动文摘（六）/result22.png" width="300" height="400">
<p>在DUC2004数据集上对比各个模型，得到了相同的结论。</p>
<img src="/2016/04/30/自动文摘（六）/result23.png" width="300" height="400">
<p>5种不同的模型在混乱度这个指标上比较，本文算法RAS-Elman具有明显的优势。</p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a> <b>Proceedings of EMNLP 2015</b></p>
<p>[2] <a href="http://harvardnlp.github.io/papers/naacl16_summary.pdf" target="_blank" rel="external">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</a> <b>Proceedings of NAACL 2016</b></p>
<p>[3] <a href="https://github.com/harvardnlp/NAMAS" target="_blank" rel="external">ABS Torch Code</a></p>
<p>[4] <a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="external">Seq2Seq Torch Code</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-24T03:14:12.000Z"><a href="/2016/04/24/自动文摘（五）/">2016-04-24</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/24/自动文摘（五）/">自动文摘（五）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>读万卷书 行万里路</strong></p>
</blockquote>
<p>最近读了几篇关于deep learning在summarization领域应用的paper，主要的方法是借鉴机器翻译中seq2seq的技术，然后加上attention model提升效果。今天来分享其中一篇paper，<b>Generating News Headlines with Recurrent Neural Networks</b></p>
<p><code>本篇文章是近期所读文章中最简单的一篇，没有太精彩的理论和创新，是一个工程性很强的paper，将实现过程中的思路和一些参数交代的很清楚，对于复现此paper提供了很大的帮助。</code></p>
<p><code>深度学习是一门研究表示学习的技术，用一张巨大的网来表征给入的数据，使得模型不依赖于领域的特征，是一种full data driven的模型，听起来像是一种银弹，尤其是近几年的在各大领域的都收获了state-of-the-art的结果，但模型的参数调优不没有太多的理论依据，之前的神经网络规模小调参数时间代价会小一些，但deep learning动不动就需要几天甚至几周的训练时间，调参数代价太大；中间层的表示如何解释，也是一个十分头疼的事情，对于cv领域来说还好，总可以将matrix显示成一幅图片来看效果，比较直观，但对于nlp领域，hidden state到底是什么，表示哪个词？表示哪种关系？词向量的每一个维度代表什么？具体真说不清楚，只有在输出的那一层才能看到真正的意义。</code></p>
<p><code>一个领域的发展需要很多种不同思路的试错，应该是一种百家争鸣的态势，而不是大家一股脑地都用一种技术，一种思路来解决问题，理论模型都趋于大同，这样对这个领域的发展不会有太积极的意义。</code></p>
<p><code>machine translation是最活跃的一个研究领域，seq2seq框架就是从该领域中提炼出来的，attention model也是借鉴于soft alignment，对于文本摘要这个问题来说，套用seq2seq只能解决headlines generation的问题，面对传统的single document summarization和multi document summarization任务便束手无策了，因为输入部分的规模远大于输出部分的话，seq2seq的效果不会很好，因此说abstractive summarization的研究还长路漫漫。不过这里可以将extractive和abstractive结合在一起来做，用extractive将一篇文档中最重要的一句话提取出来作为输入，套用seq2seq来做abstractive，本质上是一个paraphrase的任务，在工程中可以试一下这种思路。在后续的研究中也可以尝试将extractive和abstractive的思路结合在一起做文本摘要。</code></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文的思路是用LSTM RNN作为encoder-decoder框架的模型，并且使用了attention模型来生成新闻文章的标题，效果很好。并且提出了一种简化版的attention mechanism，相比于复杂版的注意力机制在解决headline generation问题上有更好的效果。</p>
<p><code>本文定义的文本摘要问题是给新闻文章命题，为了套用seq2seq技术，一般都会将source定义为新闻的第一句话，target定义为标题。本文的亮点在于提出了一种简化版的注意力机制，并且得到了不错的结果。</code></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2016/04/24/自动文摘（五）/model.png" width="600" height="800">
<p>encoder使用文章内容作为输入，一个时间点表示一个单词，每个单词先通过embedding层将词转换为一个分布式向量（<code>word embedding</code>）。每个词向量都由前一个词向量生成，第一个词定义为0向量。</p>
<p>decoder将encoder中最后一个词向量作为输入，decoder本质是一个rnnlm，使用softmax和attention mechanism来生成每个词。</p>
<p>损失函数：</p>
<img src="/2016/04/24/自动文摘（五）/lossfunction.png" width="600" height="650">
<p>这里y是输出的词，x是输入的词。</p>
<p>本文采用了4层LSTM，每层有600个单元，使用Dropout控制过拟合，所有参数的初始值都服从-0.1到0.1的平均分布，训练方法是RMSProp，学习速率0.01，动量项0.9，衰减项0.9，训练9个回合，在第5个回合之后，每个回合都将训练速率减半。batch训练，384组训练数据为一个batch。</p>
<p><code>模型的定义和训练方法都是借鉴于其他文章，模型参数的不同并不是什么创新，别人用gru或者birnn，你用lstm，或者别人用2层，你用3层、4层更多层，不同的模型参数可能会有不同的state-of-the-art结果，但并不会对大家认识abstractive summarization问题有什么实质性的帮助，也不会促进这个领域的发展，只是用着现有的方法在这个领域刷了一篇paper罢了。</code></p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>注意力机制可以用来帮助神经网络更好地理解输入数据，尤其是一些专有名词和数字。attention在decoder阶段起作用，通过将输出与所有输入的词建立一个权重关系来让decoder决定当前输出的词与哪个输入词的关系更大（即应该将注意力放到哪个词上）。</p>
<p>本文采用两种不同的注意力机制，第一种称作复杂注意力模型（<code>complex attention</code>），与Minh-Thang采用的点乘机制（<code>dot mechanism</code>）一样，看下图：</p>
<img src="/2016/04/24/自动文摘（五）/complex.png" width="400" height="650">
<p>第二种称作简单注意力模型（<code>simple attention</code>），是第一种模型的变种，该种模型使得分析神经网络学习注意力权重更加容易。看下图：</p>
<img src="/2016/04/24/自动文摘（五）/simple.png" width="400" height="650">
<p>对比两幅图可以看出区别在于隐藏层的最后一层的表示上，简单模型将encoder部分在该层的表示分为两块，一小块用来计算注意力权重（<code>attention weight</code>），另一大块用来作为上下文（<code>context vector</code>）；decoder部分在该层的表示也分为两块，一小块用来计算注意力权重，另一大块用来导入softmax，进行输出预测。</p>
<p><code>simple attention mechanism的提出可以算作本文的主要贡献，但是感觉贡献量并不大。修改所谓的理论模型，而不仅仅是对模型参数进行修改，本质上是对encoder的context vector进行了更换，用了一些技巧，比如文中的方法，将隐藏层最后一层的表示分为两部分，一部分用来表示context，一部分用来表示attention weight，就有了新的模型。</code></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><p>本文用English Gigaword数据集，该数据集包括了六大主流媒体机构的新闻文章，包括纽约时报和美联社，每篇文章都有清晰的内容和标题，并且内容被划分为段落。经过一些预处理之后，训练集包括5.5M篇新闻和236M单词。</p>
<h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>headlines作为target，news text的第一段内容作为source，预处理包括：小写化，分词，从词中提取标点符号，标题结尾和文本结尾都会加上一个自定义的结束标记<code>&lt;eos&gt;</code>，那些没有标题或者没有内容或者标题内容超过25个tokens或者文本内容超过50个tokens都会被过滤掉，按照token出现频率排序，取top 40000个tokens作为词典，低频词用符号<code>&lt;unk&gt;</code>进行替换。</p>
<p>数据集被划分为训练集和保留集，训练集将会被随机打乱。</p>
<p><code>数据的预处理是一件重要的事情，处理的好坏直接影响结果的好坏。本文的每一个处理细节都交代的很清楚，有希望做相同实验的童鞋可以借鉴他的处理方法</code></p>
<h2 id="Dataset-Issues"><a href="#Dataset-Issues" class="headerlink" title="Dataset Issues"></a>Dataset Issues</h2><p>训练集中会出现标题与所输入文本关系不大的情况，比如：标题包括以下字样For use by New York Times service clients，或者包括一些代码，biz-cover-1等等，本文对此不作处理，因为一个理想的模型可以处理这些问题。‘</p>
<p><code>数据集本身会有一些错误，但一个好的模型是可以处理好这些错误的数据，所以本文对此种数据并不做处理。</code></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>模型的优劣用两种方法进行评价。第一种，将训练集和保留集<code>损失值</code>作为评价指标；第二种，将<code>BLEU</code>作为评价指标，为了保证效率，保留集仅仅用了384个样本进行计算。</p>
<p><code>评价指标也是常规的两种，两种数据集上的loss值直观地反应了训练和测试效果，BLEU是机器翻译领域中常用的评价标准。</code></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>计算硬件是GTX 980 Ti GPU，每种模型的计算都会花费4.5天时间。效果直接看下图：</p>
<img src="/2016/04/24/自动文摘（五）/evaluation.png" width="600" height="800">
<p>在应用模型结果做保留集的预测时，不同新闻来源的文章预测效果不一样。比如：在BBC、华尔街日报、卫报的效果就非常好，但是在赫芬顿邮报和福布斯的效果就很差。</p>
<p><code>结果看上图也是一目了然，本文的simple attention mechanism更胜一筹。</code></p>
<h2 id="Understanding-information-stored-in-last-layer-of-the-neural-network"><a href="#Understanding-information-stored-in-last-layer-of-the-neural-network" class="headerlink" title="Understanding information stored in last layer of the neural network"></a>Understanding information stored in last layer of the neural network</h2><p>存在有许多思路来理解注意力机制函数，考虑下面的公式，从输入计算到softmax输出：</p>
<img src="/2016/04/24/自动文摘（五）/formula.png" width="600" height="650">
<p>第一个部分表示attention context vector对decoder输出的影响，由于context是从input计算得来的，可以理解为encoder的每个输入对decoder输出的影响；第二个部分表示decoder当前隐藏层最后一层对输出的影响；第三个部分表示偏置项。</p>
<h2 id="Understanding-how-the-attention-weight-vector-is-computed"><a href="#Understanding-how-the-attention-weight-vector-is-computed" class="headerlink" title="Understanding how the attention weight vector is computed"></a>Understanding how the attention weight vector is computed</h2><p><code>注意到这一点很重要，encoder部分的神经元对docoder部分的神经元起作用，也就是attention weight的本质。</code></p>
<h2 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h2><p>本文的模型中存在几种类型的错误，包括：</p>
<p>1、神经网络机制在填充细节时细节发生丢失。比如：target是 72 people died when a truck plunged into a gorge on Friday，而模型的预测是 72 killed in truck accident in Russia。这种错误经常出现在decoder beam很小的情况下。</p>
<p>2、生成的headline与输入的文本没有太大的关系，这些headline在训练集中出现太多次。这种错误常出现在decoder beam很大的情况下。</p>
<p>上述两种错误反映了本文的模型对decoder beam非常敏感。</p>
<p><code>个人感觉本文的重点在于动手实践seq2seq+attention在自动文摘中的应用，对很多模型层面上的研究很少，对效果分析上的研究也很浅。</code></p>
<h1 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h1><p>使用BiRNN来代替RNN配合attention model效果可能会更好一些。</p>
<p><code>将模型更换为Bi-RNN会得到一个新的结果，不知道会不会有人拿这个来刷paper，个人觉得好无趣。</code></p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>本文提出的simple attention mechanism效果很不错。</p>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1512.01712" target="_blank" rel="external">Generating News Headlines with Recurrent Neural Networks</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-17T11:06:18.000Z"><a href="/2016/04/17/自动文摘（四）/">2016-04-17</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/17/自动文摘（四）/">自动文摘（四）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>这篇博客是自动文摘系列的第四篇，重点介绍近期abstractive summarization的一些研究情况。abstractive是学术界研究的热点，尤其是Machine Translation中的encoder-decoder框架和attention mechanism十分火热，大家都试着将abstractive问题转换为sequence-2-sequence问题，套用上面两种技术，得到state-of-the-art结果，2015年来已经有许多篇paper都是这种套路，于是就有了下面的吐槽：</p>
<img src="http://ww4.sinaimg.cn/mw690/62caff97jw1f2qam97d3tj20sg0nd775.jpg" width="400" height="650">
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>Encoder-Decoder不是一种模型，而是一种框架，一种处理问题的思路，最早应用于机器翻译领域，输入一个序列，输出另外一个序列。机器翻译问题就是将一种语言序列转换成另外一种语言序列，将该技术扩展到其他领域，比如输入序列可以是文字，语音，图像，视频，输出序列可以是文字，图像，可以解决很多别的类型的问题。这一大类问题就是上图中的sequence-to-sequence问题。这里以输入为文本，输出也为文本作为例子进行介绍：</p>
<img src="/2016/04/17/自动文摘（四）/seq2seq.png" width="400" height="650">
<p>encoder部分是将输入序列表示成一个带有语义的向量，使用最广泛的表示技术是Recurrent Neural Network，RNN是一个基本模型，在训练的时候会遇到gradient explode或者gradient vanishing的问题，导致无法训练，所以在实际中经常使用的是经过改良的LSTM RNN或者GRU RNN对输入序列进行表示，更加复杂一点可以用BiRNN、BiRNN with LSTM、BiRNN with GRU、多层RNN等模型来表示，输入序列最终表示为最后一个word的hidden state vector。</p>
<p>decoder部分是以encoder生成的hidden state vector作为输入“解码”出目标文本序列，本质上是一个语言模型，最常见的是用Recurrent Neural Network Language Model(RNNLM)，只要涉及到RNN就会有训练的问题，也就需要用LSTM、GRU和一些高级的model来代替。目标序列的生成和LM做句子生成的过程类似，只是说计算条件概率时需要考虑encoder向量。</p>
<p>这里，每一种模型几乎都可以出一篇paper，尤其是在这个技术刚刚开始应用在各个领域中的时候，大家通过尝试不同的模型组合，得到state-of-the-art结果。</p>
<p>该框架最早被应用在Google Translation中，paper详情可以见[1]，2014年12月发在arxiv上。</p>
<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><p>注意力机制在NLP中的使用也就是2015年的事情，也是从机器翻译领域开始。我们仔细看decoder中生成目标文本序列这部分，第一个word的生成完全依赖于encoder的last hidden state vector，而这个vector更多的是表示输入序列的最后一个word的意思，也就是说rnn一般来说都是一个有偏的模型。</p>
<p>打个比方，rnn可以理解为一个人看完了一段话，他可能只记得最后几个词说明的意思，但是如果你问他前面的信息，他就不能准确地回答，attention可以理解为，提问的信息只与之前看完的那段话中一部分关系密切，而其他部分关系不大，这个人就会将自己的注意力锁定在这部分信息中。这个就是所谓attention mechanism的原理，每个hidden state vector对于decoder生成每个单词都有影响，但影响分布并不相同，请看下图：</p>
<img src="/2016/04/17/自动文摘（四）/attention.png" width="400" height="650">
<p>图中行文本代表输出，列文本代表输入，颜色越深表示两个词相关性越强，即生成该词时需要多注意对应的输入词。不同的paper在使用attention上会有不同的技巧，这里不一一赘述了。</p>
<h1 id="Neural-Summarization"><a href="#Neural-Summarization" class="headerlink" title="Neural Summarization"></a>Neural Summarization</h1><p>使用deep learning技术来做abstractive summarization的paper屈指可数，大体的思路也类似，大概如下：</p>
<p>0、首先将自动文摘的问题构造成一个seq2seq问题，通常的做法是将某段文本的first sentence作为输入，headlines作为输出，本质上变成了一个headlines generative问题。</p>
<p>1、选择一个big corpus作为训练、测试集。自动文摘的技术没有太成熟的一个重要原因在于没有一个成熟的大规模语料。一般来说都选择Gigawords作为训练、测试集，然后用DUC的数据集进行验证和对比。</p>
<p>2、选择一个合适的encoder，这里可以选simple rnn，lstm rnn，gru rnn，simple birnn，lstm birnn，gru birnn，deep rnn，cnn，以及各种各样的cnn。不同model之间的组合都是一种创新，只不过创新意义不太大。用encoder将输入文本表示成一个向量。</p>
<p>3、选择一个合适的decoder，decoder的作用是一个language model，用来生成summary words。</p>
<p>4、设计一个合适的attention model。不仅仅基于encoder last hidden state vector和上文来预测输出文本序列，更要基于输入中“注意力”更高的词来预测相应的词。</p>
<p>5、设计一个copy net。只要是语言模型都会存在相同的问题，比如out-of-vocabulary词的处理，尤其是做新闻类摘要的生成时，很多词都是人名、机构名等专有名词，所以这里需要用copy net 将输入中的词copy过来生成输出。在生成中文摘要问题上，将words降维到characters可以避免oov的问题，并且取得不错的结果。</p>
<p>接下来想做的事情是将neural summarization相关的paper精读之后写成blog。</p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p>[1] <a href="http://cn.arxiv.org/pdf/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></p>
<p>[2] <a href="http://cn.arxiv.org/pdf/1509.00685" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a></p>
<p>[3] <a href="http://cn.arxiv.org/pdf/1506.05865" target="_blank" rel="external">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</a></p>
<p>[4] <a href="http://cn.arxiv.org/pdf/1603.06393" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-06T03:31:55.000Z"><a href="/2016/04/06/自动文摘（三）/">2016-04-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/06/自动文摘（三）/">自动文摘（三）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><blockquote>
<p><strong>蜀道之难 难于上青天</strong></p>
</blockquote>
<p>虽然有很多SaaS提供Summarization的服务，虽然有很多App尤其是新闻类App标榜自己拥有多么牛的技术做Summarization，我们还是不得不承认自动文摘的技术离一个高水平的AI还有一段距离，很长的一段距离。都说自动文摘很难，到底难在哪里？</p>
<h1 id="Abstractive"><a href="#Abstractive" class="headerlink" title="Abstractive"></a>Abstractive</h1><p>上一篇博客分享了Extraction方法的一些思路，本篇简单聊一点Abstractive的想法。<br>Abstractive是一个True AI的方法，要求系统理解文档所表达的意思，然后用可读性强的人类语言将其简练地总结出来。这里包含这么几个难点：</p>
<p>1、理解文档。所谓理解，和人类阅读一篇文章一样，可以说明白文档的中心思想，涉及到的话题等等。</p>
<p>2、可读性强。可读性是指生成的摘要要能够连贯（Coherence）与衔接（Cohesion），通俗地讲就是人类读起来几乎感觉不出来是AI生成的（通过图灵测试）。</p>
<p>3、简练总结。在理解了文档意思的基础上，提炼出最核心的部分，用最短的话讲明白全文的意思。</p>
<p>上述三个难点对于人类来说都不是一件容易的事情，何况是发展没太多年的自然语言处理技术。人工智能领域中AI能够领先人类的例子很多，包括前不久很火的Alpha狗，图片识别，主要是利用计算机远强于人类的计算能力，但也有很多的领域，AI离人类的水平还有很远，比如paper的survey，summarization，机器翻译等等。</p>
<p>近几年随着Deep Learning的火爆，研究者们利用一些最新的研究成果来做summarization，比如attention model，比如rnn encoder-decoder框架，在一定程度上实现了abstractive，但还是处于研究初期，效果还不算很好。</p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>自动文摘最大的一个难点是评价问题，如何有效地、合理地评价一篇文摘的效果是一个很难的问题。</p>
<h2 id="人工评价"><a href="#人工评价" class="headerlink" title="人工评价"></a>人工评价</h2><blockquote>
<p><strong>一千个读者，有一千个哈姆雷特</strong></p>
</blockquote>
<p>不同的人理解一篇文档会有很大的不同，基于人工评价的方法有类似于评价开放的文科辨析题目答案一样，需要从答案中寻找一些所谓的要点，计算要点覆盖率，打分。人工评价结果在很大程度上都是可信的，因为人可以推理、复述并使用世界知识将具有类似意思但形式不同的文本单元关联起来，更加灵活一些，但时间成本太高，效率太低。</p>
<h2 id="自动评价"><a href="#自动评价" class="headerlink" title="自动评价"></a>自动评价</h2><p>计算机评价效果，需要给定参考摘要作为标准答案，通过制定一些规则来给生成的摘要打分。目前，使用最广泛的是ROUGH系统（Recall-Oriented Understudy for Gisting Evaluation），基本思想是将待审摘要和参考摘要的n元组共现统计量作为评价依据，然后通过一系列标准进行打分。包括：ROUGH-N、ROUGH-L、ROUGH-W、ROUGH-S和ROUGH-SU几个类型。通俗地将就是通过一些定量化的指标来描述待审摘要和参考文摘之间的相似性，维度考虑比较多，在一定程度上可以很好地评价Extracive产生的摘要。</p>
<p>这里涉及到一个重要的问题，就是标注语料问题。自动评价需要给定一系列文档已经他们的参考文摘，用来测试不同的算法效果。TAC（Text Analysis Conference）和TREC（Text REtrieval Conference）两个会议提供了相关的评测数据集，自动文摘领域的paper都是以这些数据集为baseline，与其他paper的算法进行对比。会议的数据集毕竟有限，新的领域中做自动文摘需要建立自己的数据集作为标准。</p>
<p>现有的评价标准存在的一个重要问题在于没有考虑语义层面上的相似，评价extractive还好，但评价abstractive就会效果不好了。Deep Learning其实就是一个representation learning，将世界万物表示成数字，然后作分析。在词、句子甚至段落这个层面上的表示学习研究的非常多，也有很多的state-of-the-art的结果，所以做语义层面上的评价并不难。</p>
<h2 id="重要性"><a href="#重要性" class="headerlink" title="重要性"></a>重要性</h2><p>评价对于一个研究领域非常重要，是牵引这个领域前进的首要因素，评价需要制定标准，标准的好坏关系到这个领域的研究质量，尤其是研究者们的paper质量，因为大家相互比较算法的优劣就十分依赖这样的标准。标准数据集的建立以及baseline的提出，是最首要的任务。</p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-30T11:39:58.000Z"><a href="/2016/03/30/自动文摘（二）/">2016-03-30</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/30/自动文摘（二）/">自动文摘（二）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>自动文摘的方法主要分为两大类，extractive和abstractive。前者是目前最主流、应用最多、最容易的方法，后者相对来说更有一种真正人工智能的味道。还有另外一种分类方法是，单文档摘要和多文档摘要，前者是后者的基础，但后者不只是前者结果简单叠加那么简单。本文只介绍单文档的extractive方法。</p>
<h1 id="Extractive-Summarization"><a href="#Extractive-Summarization" class="headerlink" title="Extractive Summarization"></a>Extractive Summarization</h1><p>抽取式的方法基于一个假设，一篇文档的核心思想可以用文档中的某一句或几句话来概括。那么摘要的任务就变成了找到文档中最重要的几句话，也就是一个排序的问题。</p>
<p>排序是一个非常经典的问题，也是一个非常多解决方案的问题。比如：Google根据用户的query生成的网页列表，就是一个排序之后的结果；再比如Amazon的推荐系统推荐给用户的N个可能感兴趣的产品，也都是通过算法做了排序输出的。</p>
<p>排序针对不同的问题，需要提出不同的指标，比如有的应用关心的是相关性，有的关心的是时效性，有的关心的是新颖性等等，在这个层面上来讨论排序，会有不同的模型。</p>
<p>一般的抽取式摘要问题，会考虑相关性和新颖性两个指标。相关性是指摘要所用的句子最能够代表本文档的意思，而新颖性是指候选句子包含的冗余信息要少，尽可能每句话都可以独立地表达出一种独立的意思。</p>
<p>下面简单介绍一些思路。</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>NLP任务的标准流程中第一步都是预处理，将拿到的文本做分句，这里有两种可能性，一是用句点或者其他可以表达一句话结尾的符号作为分隔，另外一种是用逗号作为分隔符获取句子。</p>
<h2 id="词、句表示"><a href="#词、句表示" class="headerlink" title="词、句表示"></a>词、句表示</h2><p>这一步的思路是：将词、句子表示成计算机能理解的量，然后计算一些指标进行排序。这个地方也是各种算法、模型最大的不同之处：</p>
<p>1、Bag Of Words。词袋模型将词定义为一个维度，一句话表示成在所有词张成的空间中的一个高维稀疏向量。<br>2、TFIDF。可以理解为带权重的词袋模型，计算出每个词的TFIDF值，作为该词的权重。<br>3、LDA/LSI。将整篇文档利用TFIDF模型表示成一个矩阵，做SVD降维分解，生成两个矩阵，一个是文档-话题矩阵、另一个是词-话题矩阵。得到词-话题矩阵之后，可以得到句子-话题矩阵。<br>4、Word Embedding。Tomas Mikolov提出的Word2Vec，用了很多技巧和近似的思路让word很容易地表示成一个低维稠密向量，在很多情况下都可以达到不错的效果。词成为了一个向量，句子也可有很多种方法表示成一个向量。</p>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>这里介绍两种常见的方法。</p>
<p>1、基于图排序</p>
<p>将文档的每句话作为节点，句子之间的相似度作为边权值构建图模型，用pagerank算法进行求解，得到每个句子的得分。</p>
<p>代表算法有TextRank和LexRank。</p>
<p>2、基于特征</p>
<p>特征工程在深度学习火之前是解决特定领域问题的良药，这里用到的特征包括：</p>
<p>1）句子长度，长度为某个长度的句子为最理想的长度，依照距离这个长度的远近来打分。</p>
<p>2）句子位置，根据句子在全文中的位置，给出分数。（比如每段的第一句是核心句的比例大概是70%）</p>
<p>3）句子是否包含标题词，根据句子中包含标题词的多少来打分。</p>
<p>4）句子关键词打分，文本进行预处理之后，按照词频统计出排名前10的关键词，通过比较句子中包含关键词的情况，以及关键词分布的情况来打分。</p>
<p>代表算法是TextTeaser。</p>
<h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>排序之后的结果只考虑了相关性并没有考虑新颖性，非常有可能出现排名靠前的几句话表达的都是相似的意思。所以需要引入一个惩罚因子，将新颖性考虑进去。对所有的句子重新打分，如下公式：</p>
<p>a x score(i) + (1-a) x similarity(i,i-1), i = 2,3,….N</p>
<p>序号i表示排序后的顺序，从第二句开始，排第一的句子不需要重新计算，后面的句子必须被和前一句的相似度进行惩罚。</p>
<p>这个算法就是所谓的MMR（Maximum Margin Relevance）</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>输出的结果一般是取排序后的前N句话，这里涉及到一个非常重要的问题，也是一直自动文摘质量被诟病的问题，可读性。因为各个句子都是从不同的段落中选择出来的，如果只是生硬地连起来生成摘要的话，很难保证句子之间的衔接和连贯。保证可读性是一件很难的事情。</p>
<p>这里有一个取巧的方法，就是将排序之后的句子按照原文中的顺序输出，可以在一定程度下保证一点点连贯性。</p>
<blockquote>
<p><strong>路漫漫其修远兮，吾将上下而求索</strong></p>
</blockquote>
<h1 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h1><p>[1] <a href="https://gist.github.com/rsarxiv/11470a8d763b2845f671061c21230435" target="_blank" rel="external">TextRank源码阅读笔记</a><br>[2] <a href="https://gist.github.com/rsarxiv/4e949264b3bda98828b84cf2991e57e4" target="_blank" rel="external">TextTeaser源码阅读笔记</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-26T09:20:02.000Z"><a href="/2016/03/26/RSarXiv-Web版/">2016-03-26</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/26/RSarXiv-Web版/">RSarXiv Web版</a></h1>
  

    </header>
    <div class="entry">
      
        <p>一直感觉app很难承载太多的功能，所以在app后台基础上，开发了这个web版，包括以下几个功能：</p>
<h1 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h1><p>检索一直是查论文最常用的方式，web版提供了检索功能，查询结果只有一些简要信息，详细的信息需要进入detail页面进行查看。</p>
<h1 id="反馈"><a href="#反馈" class="headerlink" title="反馈"></a>反馈</h1><p>如果用户希望用到一些更深层次的功能，比如推荐功能，可以在paper详细信息页中进行显式地反馈，包括对某篇paper点个赞，某个作者点个赞，某个研究领域点个赞，某个关键词点个赞；当然也包括一些隐式反馈，对某篇paper进行review。</p>
<h1 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h1><p>这个名词在电商中出现的多一些，用来形容一个用户的各个维度的特征，然后根据用户的特点进行商品推荐。web版沿用了app版本中的用户画像，是一张人脸标签云，当你的tag越多，人脸就越明显。</p>
<h1 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h1><p>推荐主要来自三个维度：</p>
<p>[1]TopN推荐基于用户感兴趣的tag，包括subject、authors、keywords、paper等信息。</p>
<p>[2]单篇paper的相似paper由elasticsearch的more like this功能实现。</p>
<p>[3]对某个tag点赞之后，会对用户推荐与该tag相关的tag，这个结果由word2vec实现。</p>
<h1 id="与App比较"><a href="#与App比较" class="headerlink" title="与App比较"></a>与App比较</h1><p>Web版与App最大的不同在于用户的输入，App提供了一个类似于tinder的滑动卡片进行初始化的功能，用户不需要输入一个字，通过tag推荐进行初始tag的选择，生成初始的用户画像和推荐结果，再通过不断地反馈来丰富画像和更新推荐结果；而Web的输入是来自于检索，通过用户主动地查询所感兴趣的tag来生成用户画像和推荐结果。</p>
<h1 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h1><p>目前的应用，不管是web端也好，还是app，甚至是之前测试用的微信公众号，都没有涉及到语义这个层面，包括检索，也包括推荐。</p>
<p>paper的质量没有一个很好地保证，arxiv上的paper质量是这个平台的一个短板，下一步有时间把paper之间的引用关系爬下来，跑个pagerank，给每个领域中的paper排个序，尽量给用户多多推荐高质量的paper。</p>
<p>因为最近在关注自动文摘，所以想做这么一个事情，就是给query结果生成多个paper的综述（survey），但是这并不是一件很容易的事情，因为自动文摘有着很多的难点（下一期自动文摘blog写这个主题）。</p>
<p>应用没有太多的利用到所谓的群体智慧，比如各个平台上对各篇paper的review，比如协同过滤之类的东西，前者需要花费太多的精力而且效果应该不会太好，后者需要积累一些用户数据。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p>[1] App下载可以在app store中搜索rsarxiv</p>
<p>[2] Web版网址是<a href="http://rsarxiv.science/web" target="_blank" rel="external">http://rsarxiv.science/web</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-19T23:44:46.000Z"><a href="/2016/03/20/自动文摘（一）/">2016-03-20</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/20/自动文摘（一）/">自动文摘（一）</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="引"><a href="#引" class="headerlink" title="引"></a>引</h1><p>最近人工智能随着AlphaGo战胜李世乭这一事件的高关注度，重新掀起了一波新的关注高潮，有的说人工智能将会如何超越人类，有的说将会威胁到人类的生存和发展，种种声音都在表明人工智能的又一个春天即将到来，但很多学者认为媒体的过度炒作，会引发民众对人工智能不切实际地期待，从而导致人工智能寒冬的又一次到来。Yann Lecun作为上一个人工智能寒冬时期还在坚持做冷门的神经网络研究的人，他对AI有一个非常理性的认知。<br><img src="http://ww2.sinaimg.cn/mw690/5396ee05jw1f21a1ncxl4j20do0b376e.jpg" width="400" height="650"></p>
<p>最近几年在人工智能领域中大热的工程技术deep learning，将机器对图像，语音，人类语言的认知能力都提升了不少，前前后后也涌现出不少不仅仅是很cool而且是非常实用的应用，比如人脸识别，猫脸识别，无人车，语义搜索等等。其中，深度学习技术对图像和语音的影响最大，但对人类语言的理解（NLP）做的没有那么那么好。所以，不必太过鼓吹人工智能将会如何如何，民众的期待不应太过接近科幻电影，不然只能换来无尽的失望，从而导致寒冬的来临。</p>
<p>NLP是一个非常难的task，至今有很多的子task都没有得到太好的解决。虽然每天我们在arxiv上都可以看到update的paper，但大多数都是一些model上的小trick，在个别数据集上跑一些example，和baseline做一些对比，得到所谓的state-of-the-art结果，并没有真正深刻理解要解决的问题，所谓的唯model论。不久前，Christopher D. Manning在文章中写了这么一句话：</p>
<blockquote>
<p><strong><br>However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.</strong></p>
</blockquote>
<p>以上是一些简单的背景介绍，下面进入正题。</p>
<p>自动文摘（auto text summarization）是NLP中较难的技术，难点很多，至今并没有一个非常让人满意的、成熟的技术来解决这个问题。</p>
<h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><p>大家在查文献的时候，输入一个关键词之后，会返回一个paper列表，如果你只看paper的title可能会被一些标题党蒙骗，如果每篇paper都看abstract，时间会花太久，看着很烦。所以我在想，给rsarxiv添加一个功能，基于query的research survey生成。当你输入一个keyword之后，返回的结果不仅仅是paper列表，还有一个非常精炼的survey，你可以通过阅读survey了解到每篇paper的最核心工作，如果你感兴趣的话，可以进一步查看paper的具体内容。</p>
<p>基于这个idea，开始逐步地了解自动文摘技术，所以这一系列blog的目的是为了记录我在学习自动文摘过程中的一些点滴心得。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>第一篇blog对自动文摘做一个简单的介绍。</p>
<p>自动文摘技术应用最广的领域在于新闻，由于新闻信息的过载，人们迫切地希望有这么一个工具可以帮助自己用最短的时间了解最多的最有用的新闻（为什么不直接看标题呢？因为很多新闻为了哗众取宠，故意将标题起的特别吸引人眼球，但却名不副实），因此就有了Yahoo 3000w$$ 收购summly的交易。另外，搜索引擎也是应用之一，基于query的自动文摘会帮助用户尽快地找到感兴趣的内容。前者是单文档摘要技术，后者是多文档摘要技术，后者较于前者会更加复杂一些。</p>
<p>自动文摘出现的重要原因之一是信息过载问题的困扰，（当然个性化推荐系统是解决信息过载的另外一个好的办法）另外一个重要原因是人工文摘的成本较高。可以想象，如果计算机有能力写出一个topic下的综述paper，也就不需要survey作者去花大量的时间来读和写了。</p>
<p>自动文摘要解决的问题描述很简单，就是用一些精炼的话来概括整篇文章的大意，用户通过阅读文摘就可以了解到原文要表达的意思。问题包括两种解决思路，一种是extractive，抽取式的，从原文中找到一些关键的句子，组合成一篇摘要；另外一种是abstractive，摘要式的，这需要计算机可以读懂原文的内容，并且用自己的意思将其表达出来。现阶段，相对成熟的是抽取式的方案，有很多很多的算法，也有一些baseline的测试，但得到的摘要效果差强人意，对后者的研究并不是很多，人类语言包括字、词、短语、句子、段落、文档这几个level，研究难度依次递增，理解句子、段落尚且困难，何况是文档，这是自动文摘最大的难点。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p>[1] <a href="https://www.quora.com/profile/Zhang-Jun-3/Auto-text-summarization/What-are-the-challenges-of-automatic-text-summarization?srid=3gtv&share=6c9f8e3a" target="_blank" rel="external">Quora上的问答</a><br>[2] <a href="https://www.zhihu.com/question/41465328" target="_blank" rel="external">知乎上的问答</a></p>
<h1 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h1><p><code>RSarXiv</code> <b>一个好用的arxiv cs paper推荐系统</b> <a href="http://rsarxiv.science/web" target="_blank" rel="external">网站地址</a> <b>ios App下载：App Store 搜索rsarxiv即可获得 </b></p>
<p>PaperWeekly，每周会分享1-2篇人工智能领域的热门paper，内容包括摘译和评价，欢迎大家扫码关注。</p>
<img src="/2016/05/13/Paper翻译列表/qrcode.jpg" width="650" height="650">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-03-06T07:40:40.000Z"><a href="/2016/03/06/RSarXiv前世今生/">2016-03-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/03/06/RSarXiv前世今生/">RSarXiv前世今生</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="RSarXiv是什么？"><a href="#RSarXiv是什么？" class="headerlink" title="RSarXiv是什么？"></a>RSarXiv是什么？</h1><p>RSarXiv是我的一个side project，甚至有一段时间几乎做成了main project，从开始利用课余时间做设计，写代码到后面的每天全天时间都在写代码。</p>
<p>RSarXiv是一个让人用起来很愉快的论文推荐App，在其简单的操作背后，蕴藏着复杂的、有趣的智能算法作为支撑。</p>
<p>RSarXiv的目的是让用户可以轻松、快乐地获得感兴趣的论文。</p>
<h1 id="为什么要做RSarXiv？"><a href="#为什么要做RSarXiv？" class="headerlink" title="为什么要做RSarXiv？"></a>为什么要做RSarXiv？</h1><blockquote>
<p><strong>博观而约取 厚积而薄发</strong></p>
</blockquote>
<p>11年开始，断断续续地关注过推荐系统算法及相关的应用，也亲眼见证了从11年开始国内互联网界如雨后春笋般地涌现出大量的推荐系统应用，最早的是豆瓣猜，猜你喜欢的电影、书籍、歌曲，亚马逊、淘宝等一些电商的猜你可能喜欢的商品，无觅网、指阅到今日头条猜你感兴趣的新闻事件，以及12年一位该领域的大神为了写书而推出的paperlens项目，也是推荐用户论文等等。</p>
<p>大约两三年的时间，推荐系统几乎成了各大网站的标配系统，甚至一度出现了一种声音，推荐系统可能会将搜索引擎替代了。实践证明，将推荐系统作为搜索引擎的一种补充和增强是一种更加理智的选择。那个时候，也可以看得出概念的炒作，很多的人写博客、写书来鼓吹某一项技术可以打遍天下无敌手。现在，回头看看都是一个笑话。</p>
<p>世界上没有完美的技术，只有完美的产品。千万不要有一招武功吃遍天下鲜的想法，只有产品真真正正、踏踏实实地好用，给用户提供了方便才是真理！给自己身上贴满了热门标签的产品，只能表现出其浮夸的内心。</p>
<p>当时，做RSarXiv这个产品的想法其实早几年就在心里种下了种子，可能没有做出来那么具体，只是想着能够做一个好用的推荐系统，让自己真正地享受一把人工智能带来的乐趣。</p>
<p>时间定格在15年9月底，在烟台出差的无聊日子中，终于忍不住想做一些实实在在的事情了，于是写下了一个很粗糙的计划，包括搭建爬虫系统，学习swift ios app开发，API服务器的开发等等相关的东西。要推荐什么内容？这个问题想了一段时间，后来决定是论文，因为那段时间需要多读一些论文准备开题的工作，真正地感觉找论文，尤其是真正感兴趣的论文和质量高的论文是一件很难的事情，所以这个事情就这样开始了。</p>
<h1 id="如何做RSarXiv？"><a href="#如何做RSarXiv？" class="headerlink" title="如何做RSarXiv？"></a>如何做RSarXiv？</h1><h2 id="Version-1-0"><a href="#Version-1-0" class="headerlink" title="Version 1.0"></a>Version 1.0</h2><p>第一个版本的RSarXiv其实主要是做了一个前端的APP，功能非常简单，输入自己感兴趣的tag，提交给服务器，返回与这几个tag相关的paper，如果对哪篇paper感兴趣，通过点赞的方式反馈给系统，系统将这篇paper的tag分解出来，推荐给用户，用户通过管理这些tag，删除不感兴趣的tag，留下感兴趣的tag，重新提交给服务器，获取感兴趣的paper，就这样循环迭代地使用这个APP。</p>
<p>当时主要的目的是练习写APP，检验一下前一段时间学习swift的成果，最后还提交了一个版本到App Store，因为各种不规范的原因，没有能够发布出来。</p>
<p>这个版本非常简单，因为只需要做前端的APP，后端的算法使用的 <a href="http://lateral.io" target="_blank" rel="external">http://lateral.io</a> 这个网站的API，因为人家服务器在欧洲，所以每次返回的结果都需要花费3s以上，根本无法忍受。</p>
<h2 id="Version-2-0"><a href="#Version-2-0" class="headerlink" title="Version 2.0"></a>Version 2.0</h2><p>通过1.0的版本学习到了如何写一个简单的APP，但离真正地实现自己想做的事情还有好大一步。</p>
<p>于是，酝酿了几天的时间，终于下了狠心说，这次要做就做一个大的，决不草草了事。从11月初开始，从构建一个高可靠的爬虫系统开始，开始了一段main project生涯。那个时候开始，用teambition来管理任务和添加每天的感受，每天做完前一天晚上定下的task，然后晚上再定出第二天的task，day by day，task by task，隔三差五地写一篇心得，记录一下最近的想法和进步。</p>
<p>回过头看，那段时间打下了很扎实的基础，动手使用了很多以前望而却步的技术，攻克了许多以前想都不敢想的难题。感觉自己每天都在进步，很大的进步，人生简直充满了力量，当然体重也跟着增长了起来。</p>
<p>当然，现在看来，那段时间的很多实现方法还是有些幼稚，但正说明现在还在进步。</p>
<p>2.0版本最终的产品形式是微信公众号，提供了查询服务和推荐服务，还有一些其他好玩的东西。在后端技术上有了一个非常大的飞跃，打下了坚实的基础，极大地提升了自信心。</p>
<p>做完这个版本之后，我深深地感受到了一点，自信是做好事情最重要的一个因素。</p>
<h2 id="Version-2-3"><a href="#Version-2-3" class="headerlink" title="Version 2.3"></a>Version 2.3</h2><p>之所以没有将版本改为3.0，是因为后端的技术大多数沿用了2.0的基础，并没有做出太多的新的内容。这个版本最大的特点是技术性，这里说的技术不是开发的技术，而是对算法的学习、理解和应用，是一个将应用推向了一个新高度的技术。</p>
<p>这段时间里，我读了大量的paper、博客和代码，很多也很杂，有语言模型相关的，关键词提取相关的，词表示相关的，句子表示相关的，深度学习框架相关的，情感分析相关的等等等等。因为，这个阶段，我想让自己的APP真正的具有智能，让APP中用的信息都是高质量的，比如一篇paper中的关键词提取，用了很多方法；比如paper之间按照title来计算相似度，不仅仅是基于词袋模型来做，而是用了一些语义上的表示，比如用了word2vec将keywords，authors，subjects映射到了同一个空间中，输入一个tag，可以得到多个与之相关的tag，这里的tag是keywords，authors，subjects。</p>
<p>真正地接触到了这些知识之后，才慢慢感觉到了人工智能这个神秘的词汇在我们生活中的每一个角落里都发着光，每一次使用Google的时候，看新闻的时候，回复邮件的时候，都在使用着人工智能带给我们的方便。以前觉得CNN，RNN，LSTM这些词多么牛逼，多么难懂，看过paper，看过code，看过别人的分享之后，尤其是自己动手用过之后，觉得这些神奇就在不远处。用w2v训练出paper graph的时候，让我感到疯狂，那是我一直想做的事情，原来做起来就是那么地简单。</p>
<p>这个版本，开创了一个新纪元，不仅仅具备上述的技术含量，而且成功地在App Store中发布了，并且报名参加了一个App比赛。</p>
<h1 id="RSarXiv的未来如何？"><a href="#RSarXiv的未来如何？" class="headerlink" title="RSarXiv的未来如何？"></a>RSarXiv的未来如何？</h1><p>万事开头难，开了一个好头，现在说一说RSarXiv的未来如何。</p>
<ul>
<li><p>丰富数据源。目前RSarXiv只包括arXiv上cs领域的paper，因为之前的2.0版本包括了所有领域，感觉很多功能无法融合进去，比如code。所以，我考虑只做cs方向，但是不仅仅是arXiv的数据，还要包括ACM、citeseer等其他一些数据源。而且，如果只有paper，服务还是不够，因为在看paper的时候，很希望知道这个算法如何实现的，github或者一些其他代码收录平台的数据就会很重要，paper的评论和讨论，相关的news和blog也是一些非常好的数据。</p>
</li>
<li><p>提升技术性。因为刚刚迈入NLP的大门，刚刚找到一点门道，很多问题的理解和解决方法并不是最好的，所以这里我仍然需要利用大量的业余时间来学习相关的理论和解决方法，丰富自己的认知。比如，关键词的提取怎么做到更加高质量；title的表示可以不仅仅是word表示的加权线性组合，而是通过借助CNN来提取特征来表示title，来找相似paper；如何将N篇paper做一个自动摘要，就像专家写的survey一样，将用户的查询结果，以一个概括性的总结呈现给用户等等。一个东西做到很宽其实并不难，真正挖掘很深很细致才是最难的。</p>
</li>
<li><p>前端多样性。现在的产品版本其实是参加比赛的一个简化版本，很多有用的功能都没做进去，而且只有ios版本。当然手机操作有自己的好处，简单方便，但却真的不如电脑上可以做的那么丰富，所以之后会添加一个网站版本，提供更多更强大的功能。</p>
</li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-02-27T04:27:40.000Z"><a href="/2016/02/27/intro/">2016-02-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/02/27/intro/">RSarXiv</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>用户第一次使用的时候需要进行初始化，初始化的过程十分简单，向左滑动或者向右滑动卡片，卡片上标记有三类标签，<br>（1）某一个子研究领域，比如cs.CL表示计算语言学；<br>（2）某一位作者，比如Yoshua Bengio；<br>（3）某一个关键词，比如long short-term memory。<br>如果用户喜欢某一个卡片上的标签的话，只需要向左滑动，否则向右滑动。如果用户向左滑动，系统会将与该标签相关的标签展示在后面的卡片上，比如某位用户喜欢了long short-term memory这个关键词，系统在后面的卡片上可能会显示recurrent neural network,machine learning,data mining等相关关键词。<br>用户可以一直选择下去，直到喜欢的标签达到20条或者没有候选的标签为止。</p>
<img src="/2016/02/27/intro/1.png" width="400" height="650">
<h3 id="用户肖像"><a href="#用户肖像" class="headerlink" title="用户肖像"></a>用户肖像</h3><p>用户进行完初始化之后，会进入产品的首页，可以看到形象的用户肖像，即用户感兴趣的标签，标签越大，所占权重也越大，系统推荐的文章都是基于这些标签。</p>
<img src="/2016/02/27/intro/2.png" width="400" height="650">
<h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><p>推荐的结果来自于arxiv所有数据中与用户肖像最相关的10篇文章。推荐结果是一系列文章标题的列表，用户感兴趣可以点击进入文章详情页。下拉推荐列表可以获取实时推荐结果。</p>
<img src="/2016/02/27/intro/3.png" width="400" height="650">
<h3 id="用户反馈"><a href="#用户反馈" class="headerlink" title="用户反馈"></a>用户反馈</h3><p>进入文章详情页，可以看到文章标题、所属领域、作者、摘要、关键词、10篇相似文章、原文PDF链接。<br>（1）用户如果对整篇文章感兴趣的话，可以在标题处点赞；<br>（2）如果用户对某个领域感兴趣，可以点赞，系统将推荐与该领域相关的领域、作者、关键词<br>（3）如果用户对某位作者感兴趣的话，可以点赞，系统将推荐与该作者相关的领域、作者、关键词；<br>（4）如果用户对某个关键词感兴趣的话，可以点赞，系统将推荐与该关键词相关的领域、作者、关键词；<br>（5）如果用户想查看与该篇文章相似的文章，可以点Similar Paper中的文章进行查看。</p>
<img src="/2016/02/27/intro/4.png" width="400" height="650">
<p>用户反馈行为将被记录在日志中，所占权重与时间相关，越新的行为对用户肖像的结果影响越大。用户使用APP越多，产生的反馈行为也就越多，系统推荐给用户的文章就越准确。</p>
<img src="/2016/02/27/intro/5.png" width="400" height="650">
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><a href="http://www.tudou.com/programs/view/JZINheteVq4/" target="_blank" rel="external">使用方法视频</a>
<h2 id="下载链接"><a href="#下载链接" class="headerlink" title="下载链接"></a>下载链接</h2><img src="/2016/02/27/intro/qcode.png" width="100" height="100">

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/page/2/" class="alignleft prev">上一页</a>
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:rsarxiv.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Autoencoder/">Autoencoder</a><small>1</small></li>
  
    <li><a href="/tags/CNN/">CNN</a><small>2</small></li>
  
    <li><a href="/tags/DQN/">DQN</a><small>4</small></li>
  
    <li><a href="/tags/Memory-Network/">Memory Network</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>2</small></li>
  
    <li><a href="/tags/PaperWeekly/">PaperWeekly</a><small>110</small></li>
  
    <li><a href="/tags/RNN/">RNN</a><small>1</small></li>
  
    <li><a href="/tags/RNNLM/">RNNLM</a><small>1</small></li>
  
    <li><a href="/tags/ROUGE/">ROUGE</a><small>1</small></li>
  
    <li><a href="/tags/RSarXiv/">RSarXiv</a><small>1</small></li>
  
    <li><a href="/tags/Reading-Comprehension/">Reading Comprehension</a><small>6</small></li>
  
    <li><a href="/tags/Representation/">Representation</a><small>1</small></li>
  
    <li><a href="/tags/Text-Comprehension/">Text Comprehension</a><small>1</small></li>
  
    <li><a href="/tags/api-ai/">api.ai</a><small>1</small></li>
  
    <li><a href="/tags/arXiv/">arXiv</a><small>2</small></li>
  
    <li><a href="/tags/arxiv/">arxiv</a><small>2</small></li>
  
    <li><a href="/tags/attention/">attention</a><small>3</small></li>
  
    <li><a href="/tags/bot/">bot</a><small>21</small></li>
  
    <li><a href="/tags/chatbot/">chatbot</a><small>2</small></li>
  
    <li><a href="/tags/dataset/">dataset</a><small>1</small></li>
  
    <li><a href="/tags/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/tags/deeplearning/">deeplearning</a><small>1</small></li>
  
    <li><a href="/tags/language-model/">language model</a><small>1</small></li>
  
    <li><a href="/tags/nlp/">nlp</a><small>128</small></li>
  
    <li><a href="/tags/open-source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/paper/">paper</a><small>7</small></li>
  
    <li><a href="/tags/paperweekly/">paperweekly</a><small>2</small></li>
  
    <li><a href="/tags/reading-comprehension/">reading comprehension</a><small>1</small></li>
  
    <li><a href="/tags/reinforcement-learning/">reinforcement learning</a><small>1</small></li>
  
    <li><a href="/tags/sentence-representations/">sentence representations</a><small>1</small></li>
  
    <li><a href="/tags/seq2seq/">seq2seq</a><small>17</small></li>
  
    <li><a href="/tags/text-comprehension/">text comprehension</a><small>1</small></li>
  
    <li><a href="/tags/torch/">torch</a><small>1</small></li>
  
    <li><a href="/tags/word-embedding/">word embedding</a><small>2</small></li>
  
    <li><a href="/tags/word-embeddings/">word embeddings</a><small>1</small></li>
  
    <li><a href="/tags/word2vec/">word2vec</a><small>1</small></li>
  
    <li><a href="/tags/创业/">创业</a><small>1</small></li>
  
    <li><a href="/tags/招聘/">招聘</a><small>1</small></li>
  
    <li><a href="/tags/推荐系统/">推荐系统</a><small>2</small></li>
  
    <li><a href="/tags/综述/">综述</a><small>1</small></li>
  
    <li><a href="/tags/自动文摘/">自动文摘</a><small>16</small></li>
  
    <li><a href="/tags/随笔/">随笔</a><small>4</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- rsarxiv -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7417238904018690"
     data-ad-slot="4681057960"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 PaperWeekly
  
</div>
<div class="clearfix"></div>
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_xiaoyou"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END --></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>